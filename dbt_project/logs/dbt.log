[0m09:51:39.857485 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B28E437670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B28F6B7700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B28F6B7910>]}


============================== 09:51:39.858337 | ca4dcbb1-d285-4e82-851d-c8cd40cd5144 ==============================
[0m09:51:39.858337 [info ] [MainThread]: Running with dbt=1.11.6
[0m09:51:39.858337 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'None', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'profiles_dir': 'D:\\snowflake-incremental-pipeline\\dbt_project', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'indirect_selection': 'eager', 'invocation_command': 'dbt debug', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'D:\\snowflake-incremental-pipeline\\dbt_project\\logs'}
[0m09:51:39.889326 [info ] [MainThread]: dbt version: 1.11.6
[0m09:51:39.891422 [info ] [MainThread]: python version: 3.10.11
[0m09:51:39.891422 [info ] [MainThread]: python path: C:\Users\Alien\AppData\Local\Programs\Python\Python310\python.exe
[0m09:51:39.891422 [info ] [MainThread]: os info: Windows-10-10.0.26200-SP0
[0m09:51:39.897062 [info ] [MainThread]: Using profiles dir at D:\snowflake-incremental-pipeline\dbt_project
[0m09:51:39.897062 [info ] [MainThread]: Using profiles.yml file at D:\snowflake-incremental-pipeline\dbt_project\profiles.yml
[0m09:51:39.897062 [info ] [MainThread]: Using dbt_project.yml file at D:\snowflake-incremental-pipeline\dbt_project\dbt_project.yml
[0m09:51:40.009748 [info ] [MainThread]: Configuration:
[0m09:51:40.009748 [info ] [MainThread]:   profiles.yml file [[31mERROR invalid[0m]
[0m09:51:40.009748 [info ] [MainThread]:   dbt_project.yml file [[31mERROR invalid[0m]
[0m09:51:40.009748 [info ] [MainThread]: Required dependencies:
[0m09:51:40.024109 [debug] [MainThread]: Executing "git --help"
[0m09:51:40.085107 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--no-lazy-fetch]\n           [--no-optional-locks] [--no-advice] [--bare] [--git-dir=<path>]\n           [--work-tree=<path>] [--namespace=<name>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone      Clone a repository into a new directory\n   init       Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add        Add file contents to the index\n   mv         Move or rename a file, a directory, or a symlink\n   restore    Restore working tree files\n   rm         Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect     Use binary search to find the commit that introduced a bug\n   diff       Show changes between commits, commit and working tree, etc\n   grep       Print lines matching a pattern\n   log        Show commit logs\n   show       Show various types of objects\n   status     Show the working tree status\n\ngrow, mark and tweak your common history\n   backfill   Download missing objects in a partial clone\n   branch     List, create, or delete branches\n   commit     Record changes to the repository\n   merge      Join two or more development histories together\n   rebase     Reapply commits on top of another base tip\n   reset      Reset current HEAD to the specified state\n   switch     Switch branches\n   tag        Create, list, delete or verify tags\n\ncollaborate (see also: git help workflows)\n   fetch      Download objects and refs from another repository\n   pull       Fetch from and integrate with another repository or a local branch\n   push       Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m09:51:40.086857 [debug] [MainThread]: STDERR: "b''"
[0m09:51:40.086857 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m09:51:40.088676 [info ] [MainThread]: Connection test skipped since no profile was found
[0m09:51:40.088676 [info ] [MainThread]: [31m2 checks failed:[0m
[0m09:51:40.088676 [info ] [MainThread]: Profile loading failed for the following reason:
Runtime Error
  Could not find profile named 'sales_pipelines'


[0m09:51:40.088676 [info ] [MainThread]: Project loading failed for the following reason:
Runtime Error
  at path []: Additional properties are not allowed ('macro-path', 'model-path', 'test-path' were unexpected)

Error encountered in D:\snowflake-incremental-pipeline\dbt_project\dbt_project.yml


[0m09:51:40.093567 [debug] [MainThread]: Command `dbt debug` failed at 09:51:40.093567 after 0.46 seconds
[0m09:51:40.095361 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B28E437670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B28F6B7A00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B28F740AF0>]}
[0m09:51:40.095361 [debug] [MainThread]: Flushing usage events
[0m09:51:41.195547 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m09:52:57.752114 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002124D4C7640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002124E7866B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002124E784A90>]}


============================== 09:52:57.752114 | 931b5c7d-649f-49c7-adf0-22f49484d699 ==============================
[0m09:52:57.752114 [info ] [MainThread]: Running with dbt=1.11.6
[0m09:52:57.752114 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'None', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'profiles_dir': 'D:\\snowflake-incremental-pipeline\\dbt_project', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'indirect_selection': 'eager', 'invocation_command': 'dbt debug', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'D:\\snowflake-incremental-pipeline\\dbt_project\\logs'}
[0m09:52:57.768212 [info ] [MainThread]: dbt version: 1.11.6
[0m09:52:57.768212 [info ] [MainThread]: python version: 3.10.11
[0m09:52:57.768212 [info ] [MainThread]: python path: C:\Users\Alien\AppData\Local\Programs\Python\Python310\python.exe
[0m09:52:57.784321 [info ] [MainThread]: os info: Windows-10-10.0.26200-SP0
[0m09:52:57.784321 [error] [MainThread]: Encountered an error:
Parsing Error
  Env var required but not provided: 'SNOWFLAKE_ACCOUNT'
[0m09:52:57.800140 [debug] [MainThread]: Command `dbt debug` failed at 09:52:57.800140 after 0.23 seconds
[0m09:52:57.800140 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002124D4C7640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002124E5C9810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002124DF37700>]}
[0m09:52:57.800140 [debug] [MainThread]: Flushing usage events
[0m09:53:00.144878 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m09:54:10.051976 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AA9A1376D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AA9B3F4370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AA9B3F5B70>]}


============================== 09:54:10.055155 | 8c7b6de8-9ec2-4769-9c8a-4840f1d4f7d4 ==============================
[0m09:54:10.055155 [info ] [MainThread]: Running with dbt=1.11.6
[0m09:54:10.055155 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'empty': 'None', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'profiles_dir': 'D:\\snowflake-incremental-pipeline\\dbt_project', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'use_colors': 'True', 'indirect_selection': 'eager', 'invocation_command': 'dbt debug', 'use_experimental_parser': 'False', 'log_path': 'D:\\snowflake-incremental-pipeline\\dbt_project\\logs'}
[0m09:54:10.071985 [info ] [MainThread]: dbt version: 1.11.6
[0m09:54:10.071985 [info ] [MainThread]: python version: 3.10.11
[0m09:54:10.071985 [info ] [MainThread]: python path: C:\Users\Alien\AppData\Local\Programs\Python\Python310\python.exe
[0m09:54:10.071985 [info ] [MainThread]: os info: Windows-10-10.0.26200-SP0
[0m09:54:11.076726 [debug] [MainThread]: Snowflake adapter: Setting snowflake.connector to ERROR (file logging only)
[0m09:54:11.085061 [debug] [MainThread]: Snowflake adapter: Setting botocore to ERROR (file logging only)
[0m09:54:11.085061 [debug] [MainThread]: Snowflake adapter: Setting boto3 to ERROR (file logging only)
[0m09:54:11.174054 [info ] [MainThread]: Using profiles dir at D:\snowflake-incremental-pipeline\dbt_project
[0m09:54:11.174054 [info ] [MainThread]: Using profiles.yml file at D:\snowflake-incremental-pipeline\dbt_project\profiles.yml
[0m09:54:11.174054 [info ] [MainThread]: Using dbt_project.yml file at D:\snowflake-incremental-pipeline\dbt_project\dbt_project.yml
[0m09:54:11.174054 [info ] [MainThread]: adapter type: snowflake
[0m09:54:11.180460 [info ] [MainThread]: adapter version: 1.11.2
[0m09:54:11.280476 [info ] [MainThread]: Configuration:
[0m09:54:11.280476 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m09:54:11.280476 [info ] [MainThread]:   dbt_project.yml file [[31mERROR invalid[0m]
[0m09:54:11.280476 [info ] [MainThread]: Required dependencies:
[0m09:54:11.280476 [debug] [MainThread]: Executing "git --help"
[0m09:54:11.348021 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--no-lazy-fetch]\n           [--no-optional-locks] [--no-advice] [--bare] [--git-dir=<path>]\n           [--work-tree=<path>] [--namespace=<name>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone      Clone a repository into a new directory\n   init       Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add        Add file contents to the index\n   mv         Move or rename a file, a directory, or a symlink\n   restore    Restore working tree files\n   rm         Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect     Use binary search to find the commit that introduced a bug\n   diff       Show changes between commits, commit and working tree, etc\n   grep       Print lines matching a pattern\n   log        Show commit logs\n   show       Show various types of objects\n   status     Show the working tree status\n\ngrow, mark and tweak your common history\n   backfill   Download missing objects in a partial clone\n   branch     List, create, or delete branches\n   commit     Record changes to the repository\n   merge      Join two or more development histories together\n   rebase     Reapply commits on top of another base tip\n   reset      Reset current HEAD to the specified state\n   switch     Switch branches\n   tag        Create, list, delete or verify tags\n\ncollaborate (see also: git help workflows)\n   fetch      Download objects and refs from another repository\n   pull       Fetch from and integrate with another repository or a local branch\n   push       Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m09:54:11.348021 [debug] [MainThread]: STDERR: "b''"
[0m09:54:11.348021 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m09:54:11.348021 [info ] [MainThread]: Connection:
[0m09:54:11.348021 [info ] [MainThread]:   account: qsnwhfd-ts32426
[0m09:54:11.353903 [info ] [MainThread]:   user: musmankkh456
[0m09:54:11.355330 [info ] [MainThread]:   database: INCREMENTALETL
[0m09:54:11.355330 [info ] [MainThread]:   warehouse: MY_WH
[0m09:54:11.355330 [info ] [MainThread]:   role: ACCOUNTADMIN
[0m09:54:11.358305 [info ] [MainThread]:   schema: LANDINGZONE
[0m09:54:11.358305 [info ] [MainThread]:   authenticator: None
[0m09:54:11.358305 [info ] [MainThread]:   oauth_client_id: None
[0m09:54:11.360557 [info ] [MainThread]:   query_tag: None
[0m09:54:11.360557 [info ] [MainThread]:   client_session_keep_alive: False
[0m09:54:11.360557 [info ] [MainThread]:   host: None
[0m09:54:11.360557 [info ] [MainThread]:   port: None
[0m09:54:11.360557 [info ] [MainThread]:   proxy_host: None
[0m09:54:11.366215 [info ] [MainThread]:   proxy_port: None
[0m09:54:11.367091 [info ] [MainThread]:   protocol: None
[0m09:54:11.367091 [info ] [MainThread]:   connect_retries: 1
[0m09:54:11.368900 [info ] [MainThread]:   connect_timeout: None
[0m09:54:11.369913 [info ] [MainThread]:   retry_on_database_errors: False
[0m09:54:11.371944 [info ] [MainThread]:   retry_all: False
[0m09:54:11.372371 [info ] [MainThread]:   insecure_mode: False
[0m09:54:11.372371 [info ] [MainThread]:   reuse_connections: True
[0m09:54:11.372371 [info ] [MainThread]:   s3_stage_vpce_dns_name: None
[0m09:54:11.372371 [info ] [MainThread]:   platform_detection_timeout_seconds: 0.0
[0m09:54:11.378145 [info ] [MainThread]: Registered adapter: snowflake=1.11.2
[0m09:54:11.916797 [debug] [MainThread]: Acquiring new snowflake connection 'debug'
[0m09:54:12.020624 [debug] [MainThread]: Using snowflake connection "debug"
[0m09:54:12.022678 [debug] [MainThread]: On debug: select 1 as id
[0m09:54:12.024492 [debug] [MainThread]: Opening a new connection, currently in state init
[0m09:54:13.653644 [debug] [MainThread]: SQL status: SUCCESS 1 in 1.629 seconds
[0m09:54:13.656183 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m09:54:13.656183 [info ] [MainThread]: [31m1 check failed:[0m
[0m09:54:13.664890 [info ] [MainThread]: Project loading failed for the following reason:
Runtime Error
  at path []: Additional properties are not allowed ('macro-path', 'model-path', 'test-path' were unexpected)

Error encountered in D:\snowflake-incremental-pipeline\dbt_project\dbt_project.yml


[0m09:54:13.671895 [debug] [MainThread]: Command `dbt debug` failed at 09:54:13.664890 after 3.78 seconds
[0m09:54:13.672307 [debug] [MainThread]: Connection 'debug' was left open.
[0m09:54:13.672307 [debug] [MainThread]: On debug: Close
[0m09:54:13.963254 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AA9A1376D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AAAC9EBF70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AAAC9EBB20>]}
[0m09:54:13.963254 [debug] [MainThread]: Flushing usage events
[0m09:54:15.086977 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m09:54:55.451800 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028CC4167670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028CC5424220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028CC5425B40>]}


============================== 09:54:55.456238 | deaccf6e-1741-4eb3-bf3f-cc0f91e94b85 ==============================
[0m09:54:55.456238 [info ] [MainThread]: Running with dbt=1.11.6
[0m09:54:55.463650 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'None', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'profiles_dir': 'D:\\snowflake-incremental-pipeline\\dbt_project', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'indirect_selection': 'eager', 'invocation_command': 'dbt debug', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'D:\\snowflake-incremental-pipeline\\dbt_project\\logs'}
[0m09:54:55.489029 [info ] [MainThread]: dbt version: 1.11.6
[0m09:54:55.489029 [info ] [MainThread]: python version: 3.10.11
[0m09:54:55.496222 [info ] [MainThread]: python path: C:\Users\Alien\AppData\Local\Programs\Python\Python310\python.exe
[0m09:54:55.497199 [info ] [MainThread]: os info: Windows-10-10.0.26200-SP0
[0m09:54:56.468483 [debug] [MainThread]: Snowflake adapter: Setting snowflake.connector to ERROR (file logging only)
[0m09:54:56.470495 [debug] [MainThread]: Snowflake adapter: Setting botocore to ERROR (file logging only)
[0m09:54:56.470495 [debug] [MainThread]: Snowflake adapter: Setting boto3 to ERROR (file logging only)
[0m09:54:56.560284 [info ] [MainThread]: Using profiles dir at D:\snowflake-incremental-pipeline\dbt_project
[0m09:54:56.560284 [info ] [MainThread]: Using profiles.yml file at D:\snowflake-incremental-pipeline\dbt_project\profiles.yml
[0m09:54:56.560284 [info ] [MainThread]: Using dbt_project.yml file at D:\snowflake-incremental-pipeline\dbt_project\dbt_project.yml
[0m09:54:56.560284 [info ] [MainThread]: adapter type: snowflake
[0m09:54:56.560284 [info ] [MainThread]: adapter version: 1.11.2
[0m09:54:56.713728 [info ] [MainThread]: Configuration:
[0m09:54:56.713728 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m09:54:56.713728 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m09:54:56.729528 [info ] [MainThread]: Required dependencies:
[0m09:54:56.730588 [debug] [MainThread]: Executing "git --help"
[0m09:54:56.784273 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--no-lazy-fetch]\n           [--no-optional-locks] [--no-advice] [--bare] [--git-dir=<path>]\n           [--work-tree=<path>] [--namespace=<name>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone      Clone a repository into a new directory\n   init       Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add        Add file contents to the index\n   mv         Move or rename a file, a directory, or a symlink\n   restore    Restore working tree files\n   rm         Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect     Use binary search to find the commit that introduced a bug\n   diff       Show changes between commits, commit and working tree, etc\n   grep       Print lines matching a pattern\n   log        Show commit logs\n   show       Show various types of objects\n   status     Show the working tree status\n\ngrow, mark and tweak your common history\n   backfill   Download missing objects in a partial clone\n   branch     List, create, or delete branches\n   commit     Record changes to the repository\n   merge      Join two or more development histories together\n   rebase     Reapply commits on top of another base tip\n   reset      Reset current HEAD to the specified state\n   switch     Switch branches\n   tag        Create, list, delete or verify tags\n\ncollaborate (see also: git help workflows)\n   fetch      Download objects and refs from another repository\n   pull       Fetch from and integrate with another repository or a local branch\n   push       Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m09:54:56.784273 [debug] [MainThread]: STDERR: "b''"
[0m09:54:56.784273 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m09:54:56.784273 [info ] [MainThread]: Connection:
[0m09:54:56.784273 [info ] [MainThread]:   account: qsnwhfd-ts32426
[0m09:54:56.784273 [info ] [MainThread]:   user: musmankkh456
[0m09:54:56.784273 [info ] [MainThread]:   database: INCREMENTALETL
[0m09:54:56.784273 [info ] [MainThread]:   warehouse: MY_WH
[0m09:54:56.784273 [info ] [MainThread]:   role: ACCOUNTADMIN
[0m09:54:56.793134 [info ] [MainThread]:   schema: LANDINGZONE
[0m09:54:56.794457 [info ] [MainThread]:   authenticator: None
[0m09:54:56.794457 [info ] [MainThread]:   oauth_client_id: None
[0m09:54:56.794457 [info ] [MainThread]:   query_tag: None
[0m09:54:56.794457 [info ] [MainThread]:   client_session_keep_alive: False
[0m09:54:56.794457 [info ] [MainThread]:   host: None
[0m09:54:56.794457 [info ] [MainThread]:   port: None
[0m09:54:56.794457 [info ] [MainThread]:   proxy_host: None
[0m09:54:56.794457 [info ] [MainThread]:   proxy_port: None
[0m09:54:56.794457 [info ] [MainThread]:   protocol: None
[0m09:54:56.794457 [info ] [MainThread]:   connect_retries: 1
[0m09:54:56.794457 [info ] [MainThread]:   connect_timeout: None
[0m09:54:56.794457 [info ] [MainThread]:   retry_on_database_errors: False
[0m09:54:56.794457 [info ] [MainThread]:   retry_all: False
[0m09:54:56.794457 [info ] [MainThread]:   insecure_mode: False
[0m09:54:56.794457 [info ] [MainThread]:   reuse_connections: True
[0m09:54:56.809238 [info ] [MainThread]:   s3_stage_vpce_dns_name: None
[0m09:54:56.810404 [info ] [MainThread]:   platform_detection_timeout_seconds: 0.0
[0m09:54:56.810404 [info ] [MainThread]: Registered adapter: snowflake=1.11.2
[0m09:54:57.349717 [debug] [MainThread]: Acquiring new snowflake connection 'debug'
[0m09:54:57.405190 [debug] [MainThread]: Using snowflake connection "debug"
[0m09:54:57.421024 [debug] [MainThread]: On debug: select 1 as id
[0m09:54:57.421024 [debug] [MainThread]: Opening a new connection, currently in state init
[0m09:54:58.816175 [debug] [MainThread]: SQL status: SUCCESS 1 in 1.396 seconds
[0m09:54:58.816175 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m09:54:58.816175 [info ] [MainThread]: [32mAll checks passed![0m
[0m09:54:58.816175 [debug] [MainThread]: Command `dbt debug` succeeded at 09:54:58.816175 after 3.56 seconds
[0m09:54:58.832191 [debug] [MainThread]: Connection 'debug' was left open.
[0m09:54:58.832191 [debug] [MainThread]: On debug: Close
[0m09:54:59.226940 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028CC4167670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028CC44F9600>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028CD7DE7FD0>]}
[0m09:54:59.226940 [debug] [MainThread]: Flushing usage events
[0m09:55:00.455480 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m09:55:08.298675 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AB3E1176A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AB3F3C1000>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AB3F3C23B0>]}


============================== 09:55:08.301795 | cf264312-ac92-4197-ba55-e15dd02858b2 ==============================
[0m09:55:08.301795 [info ] [MainThread]: Running with dbt=1.11.6
[0m09:55:08.301795 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'D:\\snowflake-incremental-pipeline\\dbt_project', 'invocation_command': 'dbt run', 'use_colors': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'D:\\snowflake-incremental-pipeline\\dbt_project\\logs'}
[0m09:55:09.310172 [debug] [MainThread]: Snowflake adapter: Setting snowflake.connector to ERROR (file logging only)
[0m09:55:09.318233 [debug] [MainThread]: Snowflake adapter: Setting botocore to ERROR (file logging only)
[0m09:55:09.318233 [debug] [MainThread]: Snowflake adapter: Setting boto3 to ERROR (file logging only)
[0m09:55:09.655169 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'cf264312-ac92-4197-ba55-e15dd02858b2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AB3EBB52A0>]}
[0m09:55:09.742693 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'cf264312-ac92-4197-ba55-e15dd02858b2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AB3F458DF0>]}
[0m09:55:09.742693 [info ] [MainThread]: Registered adapter: snowflake=1.11.2
[0m09:55:10.275622 [debug] [MainThread]: checksum: ec27062d18352e9cfca161c166fcc73d2201661a23a3cf3c89196a27f32cfcb8, vars: {}, profile: , target: , version: 1.11.6
[0m09:55:10.275622 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m09:55:10.275622 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'cf264312-ac92-4197-ba55-e15dd02858b2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AB3EC17160>]}
[0m09:55:10.312391 [error] [MainThread]: Encountered an error:
Parsing Error
  Error reading sales_pipelines: processed\schema.yml - Runtime Error
    Syntax error near line 22
    ------------------------------
    19 |     description: "final model for processed sales data
    20 |     columns:
    21 |     - name: invoice_no
    22 |       description: "Invoice number"
    23 |       tests:
    24 |         - not_null
    25 |         - unique
    
    Raw Error:
    ------------------------------
    while parsing a block mapping
      in "<unicode string>", line 18, column 5
    did not find expected key
      in "<unicode string>", line 22, column 21
[0m09:55:10.312391 [debug] [MainThread]: Command `dbt run` failed at 09:55:10.312391 after 2.16 seconds
[0m09:55:10.312391 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AB3E1176A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AB505805E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AB50582890>]}
[0m09:55:10.312391 [debug] [MainThread]: Flushing usage events
[0m09:55:11.221632 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m09:56:00.649365 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002123FF37640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000212411B6EF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000212411B77F0>]}


============================== 09:56:00.650993 | 41b000eb-c3cf-4d28-8da7-630da4e7bf83 ==============================
[0m09:56:00.650993 [info ] [MainThread]: Running with dbt=1.11.6
[0m09:56:00.650993 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'empty': 'None', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'profiles_dir': 'D:\\snowflake-incremental-pipeline\\dbt_project', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'use_colors': 'True', 'invocation_command': 'dbt clean', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'D:\\snowflake-incremental-pipeline\\dbt_project\\logs'}
[0m09:56:00.869159 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '41b000eb-c3cf-4d28-8da7-630da4e7bf83', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000212411B7A00>]}
[0m09:56:00.900998 [debug] [MainThread]: Command `dbt clean` succeeded at 09:56:00.900998 after 0.38 seconds
[0m09:56:00.904978 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002123FF37640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000212411B6EF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002124124B370>]}
[0m09:56:00.904978 [debug] [MainThread]: Flushing usage events
[0m09:56:02.524654 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m09:56:09.647119 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002185FD57700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021861002230>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021861000AF0>]}


============================== 09:56:09.659804 | 91db5f1a-a708-4591-9a00-3bb6982ed371 ==============================
[0m09:56:09.659804 [info ] [MainThread]: Running with dbt=1.11.6
[0m09:56:09.659804 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'D:\\snowflake-incremental-pipeline\\dbt_project', 'indirect_selection': 'eager', 'use_colors': 'True', 'invocation_command': 'dbt run', 'use_experimental_parser': 'False', 'log_path': 'D:\\snowflake-incremental-pipeline\\dbt_project\\logs'}
[0m09:56:10.611816 [debug] [MainThread]: Snowflake adapter: Setting snowflake.connector to ERROR (file logging only)
[0m09:56:10.611816 [debug] [MainThread]: Snowflake adapter: Setting botocore to ERROR (file logging only)
[0m09:56:10.616851 [debug] [MainThread]: Snowflake adapter: Setting boto3 to ERROR (file logging only)
[0m09:56:10.932286 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '91db5f1a-a708-4591-9a00-3bb6982ed371', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021860FD3370>]}
[0m09:56:11.004028 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '91db5f1a-a708-4591-9a00-3bb6982ed371', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000218607CB9D0>]}
[0m09:56:11.004028 [info ] [MainThread]: Registered adapter: snowflake=1.11.2
[0m09:56:11.571967 [debug] [MainThread]: checksum: ec27062d18352e9cfca161c166fcc73d2201661a23a3cf3c89196a27f32cfcb8, vars: {}, profile: , target: , version: 1.11.6
[0m09:56:11.571967 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m09:56:11.571967 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '91db5f1a-a708-4591-9a00-3bb6982ed371', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000218720B57B0>]}
[0m09:56:13.869438 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '91db5f1a-a708-4591-9a00-3bb6982ed371', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002187267E260>]}
[0m09:56:14.023557 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m09:56:14.023557 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m09:56:14.052880 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '91db5f1a-a708-4591-9a00-3bb6982ed371', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000218726378B0>]}
[0m09:56:14.052880 [info ] [MainThread]: Found 2 models, 6 data tests, 1 source, 522 macros
[0m09:56:14.054579 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '91db5f1a-a708-4591-9a00-3bb6982ed371', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000218721F5300>]}
[0m09:56:14.054579 [info ] [MainThread]: 
[0m09:56:14.054579 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m09:56:14.054579 [info ] [MainThread]: 
[0m09:56:14.054579 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m09:56:14.063893 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m09:56:14.079916 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m09:56:14.168637 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m09:56:14.168637 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m09:56:14.168637 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m09:56:14.168637 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m09:56:14.168637 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:56:14.168637 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:56:15.456651 [debug] [ThreadPool]: SQL status: SUCCESS 3 in 1.285 seconds
[0m09:56:15.609031 [debug] [ThreadPool]: SQL status: SUCCESS 3 in 1.453 seconds
[0m09:56:15.625385 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_INCREMENTALETL, now create_INCREMENTALETL_LANDINGZONE_processed)
[0m09:56:15.625385 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_INCREMENTALETL, now create_INCREMENTALETL_LANDINGZONE_staging)
[0m09:56:15.625385 [debug] [ThreadPool]: Creating schema "database: "INCREMENTALETL"
schema: "LANDINGZONE_processed"
"
[0m09:56:15.633343 [debug] [ThreadPool]: Creating schema "database: "INCREMENTALETL"
schema: "LANDINGZONE_staging"
"
[0m09:56:15.649456 [debug] [ThreadPool]: Using snowflake connection "create_INCREMENTALETL_LANDINGZONE_processed"
[0m09:56:15.649456 [debug] [ThreadPool]: Using snowflake connection "create_INCREMENTALETL_LANDINGZONE_staging"
[0m09:56:15.657938 [debug] [ThreadPool]: On create_INCREMENTALETL_LANDINGZONE_processed: create schema if not exists INCREMENTALETL.LANDINGZONE_processed
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "create_INCREMENTALETL_LANDINGZONE_processed"} */
[0m09:56:15.657938 [debug] [ThreadPool]: On create_INCREMENTALETL_LANDINGZONE_staging: create schema if not exists INCREMENTALETL.LANDINGZONE_staging
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "create_INCREMENTALETL_LANDINGZONE_staging"} */
[0m09:56:15.805599 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.153 seconds
[0m09:56:15.821420 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.171 seconds
[0m09:56:15.843908 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL_LANDINGZONE_processed'
[0m09:56:15.845677 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL_LANDINGZONE_staging'
[0m09:56:15.877669 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL_LANDINGZONE_processed"
[0m09:56:15.895911 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL_LANDINGZONE_staging"
[0m09:56:15.895911 [debug] [ThreadPool]: On list_INCREMENTALETL_LANDINGZONE_processed: show objects in INCREMENTALETL.LANDINGZONE_processed
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL_LANDINGZONE_processed"} */;
[0m09:56:15.895911 [debug] [ThreadPool]: On list_INCREMENTALETL_LANDINGZONE_staging: show objects in INCREMENTALETL.LANDINGZONE_staging
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL_LANDINGZONE_staging"} */;
[0m09:56:15.895911 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:56:15.895911 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:56:17.567718 [debug] [ThreadPool]: SQL status: SUCCESS 0 in 1.665 seconds
[0m09:56:17.584117 [debug] [ThreadPool]: SQL status: SUCCESS 0 in 1.690 seconds
[0m09:56:17.600516 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '91db5f1a-a708-4591-9a00-3bb6982ed371', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021871FFD690>]}
[0m09:56:17.616492 [debug] [Thread-2 (]: Began running node model.sales_pipelines.stg_sales
[0m09:56:17.616492 [info ] [Thread-2 (]: 1 of 2 START sql incremental model LANDINGZONE_staging.stg_sales ............... [RUN]
[0m09:56:17.634786 [debug] [Thread-2 (]: Acquiring new snowflake connection 'model.sales_pipelines.stg_sales'
[0m09:56:17.637159 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.stg_sales
[0m09:56:17.665322 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.stg_sales"
[0m09:56:17.665322 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.stg_sales
[0m09:56:17.763241 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.stg_sales"
[0m09:56:17.779935 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m09:56:17.781949 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: create or replace transient table INCREMENTALETL.LANDINGZONE_staging.stg_sales
    
    
    
    as (

WITH raw_data AS (
    SELECT * FROM INCREMENTALETL.LANDINGZONE.RAW_SALES
),

staged AS (
    SELECT
        -- â”€â”€ IDs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        CAST(INVOICENO       AS VARCHAR)    AS invoice_no,
        CAST(STOCKCODE       AS VARCHAR)    AS stock_code,
        CAST(DESCRIPTION     AS VARCHAR)    AS description,

        -- â”€â”€ Quantities & Prices â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        TRY_CAST(QUANTITY    AS INT)        AS quantity,
        TRY_CAST(UNITPRICE   AS FLOAT)      AS unit_price,
        TRY_CAST(DISCOUNT    AS FLOAT)      AS discount,
        TRY_CAST(SHIPPINGCOST AS FLOAT)     AS shipping_cost,
        TRY_CAST(CUSTOMERID  AS FLOAT)      AS customer_id,

        -- â”€â”€ Date Fix: VARCHAR â†’ proper TIMESTAMP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        -- Handles format: '2020-01-01 00:00:00'
        TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS') AS invoice_date,
        YEAR(TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS'))  AS invoice_year,
        MONTH(TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS')) AS invoice_month,

        -- â”€â”€ Categoricals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        TRIM(UPPER(COUNTRY))         AS country,
        TRIM(UPPER(PAYMENTMETHOD))   AS payment_method,
        TRIM(UPPER(CATEGORY))        AS category,
        TRIM(UPPER(SALESCHANNEL))    AS sales_channel,
        TRIM(UPPER(RETURNSTATUS))    AS return_status,
        TRIM(UPPER(SHIPMENTPROVIDER)) AS shipment_provider,
        TRIM(UPPER(WAREHOUSELOCATION)) AS warehouse_location,
        TRIM(UPPER(ORDERPRIORITY))   AS order_priority,

        -- â”€â”€ Audit columns â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        CURRENT_TIMESTAMP()          AS stg_loaded_at

    FROM raw_data
),

-- â”€â”€ Data Quality Flags â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
validated AS (
    SELECT
        *,
        CASE
            WHEN invoice_date IS NULL           THEN 'INVALID_DATE'
            WHEN quantity IS NULL               THEN 'INVALID_QUANTITY'
            WHEN unit_price IS NULL             THEN 'INVALID_PRICE'
            WHEN shipping_cost < 0              THEN 'NEGATIVE_SHIPPING'
            WHEN discount < 0 OR discount > 1   THEN 'INVALID_DISCOUNT'
            WHEN invoice_no IS NULL             THEN 'NULL_INVOICE'
            ELSE 'VALID'
        END AS data_quality_flag

    FROM staged
)

SELECT * FROM validated


    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m09:56:17.783963 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m09:56:20.212170 [debug] [Thread-2 (]: SQL status: SUCCESS 744 in 2.435 seconds
[0m09:56:20.244034 [debug] [Thread-2 (]: Applying DROP to: INCREMENTALETL.LANDINGZONE_staging.stg_sales__dbt_tmp
[0m09:56:20.259787 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m09:56:20.259787 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: drop view if exists INCREMENTALETL.LANDINGZONE_staging.stg_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m09:56:20.424531 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.157 seconds
[0m09:56:20.444895 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '91db5f1a-a708-4591-9a00-3bb6982ed371', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000218741C60B0>]}
[0m09:56:20.444895 [info ] [Thread-2 (]: 1 of 2 OK created sql incremental model LANDINGZONE_staging.stg_sales .......... [[32mSUCCESS 744[0m in 2.81s]
[0m09:56:20.461340 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.stg_sales
[0m09:56:20.462508 [debug] [Thread-4 (]: Began running node model.sales_pipelines.processed_sales
[0m09:56:20.465075 [info ] [Thread-4 (]: 2 of 2 START sql incremental model LANDINGZONE_processed.processed_sales ....... [RUN]
[0m09:56:20.466655 [debug] [Thread-4 (]: Acquiring new snowflake connection 'model.sales_pipelines.processed_sales'
[0m09:56:20.466655 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.processed_sales
[0m09:56:20.469269 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.processed_sales"
[0m09:56:20.478262 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.processed_sales
[0m09:56:20.481860 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.processed_sales"
[0m09:56:20.493181 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m09:56:20.493181 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: create or replace transient table INCREMENTALETL.LANDINGZONE_processed.processed_sales
    
    
    
    as (

WITH staging AS (
    SELECT * FROM INCREMENTALETL.LANDINGZONE_staging.stg_sales
),

-- â”€â”€ Step 1: Remove duplicates (keep latest loaded record) â”€â”€â”€â”€â”€â”€â”€â”€
deduped AS (
    SELECT *,
        ROW_NUMBER() OVER (
            PARTITION BY invoice_no
            ORDER BY stg_loaded_at DESC
        ) AS row_num
    FROM staging
),

-- â”€â”€ Step 2: Keep only VALID records, remove bad data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cleaned AS (
    SELECT
        invoice_no,
        stock_code,
        description,
        quantity,
        unit_price,
        discount,
        shipping_cost,
        customer_id,
        invoice_date,
        invoice_year,
        invoice_month,
        country,
        payment_method,
        category,
        sales_channel,
        return_status,
        shipment_provider,
        warehouse_location,
        order_priority,

        -- â”€â”€ Derived / Enriched columns â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        ROUND(quantity * unit_price, 2)                        AS gross_amount,
        ROUND(quantity * unit_price * (1 - discount), 2)       AS net_amount,
        ROUND(quantity * unit_price * (1 - discount)
              + COALESCE(shipping_cost, 0), 2)                 AS total_amount,

        CASE
            WHEN return_status = 'RETURNED' THEN TRUE
            ELSE FALSE
        END AS is_returned,

        CASE
            WHEN customer_id IS NULL THEN TRUE
            ELSE FALSE
        END AS is_guest_customer,

        -- â”€â”€ Audit â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        stg_loaded_at,
        CURRENT_TIMESTAMP() AS processed_at

    FROM deduped
    WHERE
        row_num = 1                     -- remove duplicates
        AND data_quality_flag = 'VALID' -- only clean records
        AND quantity > 0                -- remove returns/negatives
        AND unit_price > 0              -- remove invalid prices
        AND invoice_date IS NOT NULL    -- must have valid date
),

-- â”€â”€ Step 3: Final output â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
final AS (
    SELECT * FROM cleaned
)

SELECT * FROM final


    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m09:56:20.493181 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m09:56:21.845557 [debug] [Thread-4 (]: SQL status: SUCCESS 714 in 1.358 seconds
[0m09:56:21.878624 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL.LANDINGZONE_processed.processed_sales__dbt_tmp
[0m09:56:21.878624 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m09:56:21.886457 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: drop view if exists INCREMENTALETL.LANDINGZONE_processed.processed_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m09:56:22.031917 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.152 seconds
[0m09:56:22.042292 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '91db5f1a-a708-4591-9a00-3bb6982ed371', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002185EBD9810>]}
[0m09:56:22.042292 [info ] [Thread-4 (]: 2 of 2 OK created sql incremental model LANDINGZONE_processed.processed_sales .. [[32mSUCCESS 714[0m in 1.58s]
[0m09:56:22.050561 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.processed_sales
[0m09:56:22.050561 [debug] [MainThread]: Connection 'master' was properly closed.
[0m09:56:22.058796 [debug] [MainThread]: Connection 'create_INCREMENTALETL_LANDINGZONE_staging' was left open.
[0m09:56:22.058796 [debug] [MainThread]: On create_INCREMENTALETL_LANDINGZONE_staging: Close
[0m09:56:22.303621 [debug] [MainThread]: Connection 'create_INCREMENTALETL_LANDINGZONE_processed' was left open.
[0m09:56:22.320779 [debug] [MainThread]: On create_INCREMENTALETL_LANDINGZONE_processed: Close
[0m09:56:22.557203 [debug] [MainThread]: Connection 'list_INCREMENTALETL_LANDINGZONE_processed' was left open.
[0m09:56:22.557203 [debug] [MainThread]: On list_INCREMENTALETL_LANDINGZONE_processed: Close
[0m09:56:22.871240 [debug] [MainThread]: Connection 'list_INCREMENTALETL_LANDINGZONE_staging' was left open.
[0m09:56:22.871240 [debug] [MainThread]: On list_INCREMENTALETL_LANDINGZONE_staging: Close
[0m09:56:23.130280 [debug] [MainThread]: Connection 'model.sales_pipelines.stg_sales' was left open.
[0m09:56:23.138044 [debug] [MainThread]: On model.sales_pipelines.stg_sales: Close
[0m09:56:23.399746 [debug] [MainThread]: Connection 'model.sales_pipelines.processed_sales' was left open.
[0m09:56:23.399746 [debug] [MainThread]: On model.sales_pipelines.processed_sales: Close
[0m09:56:23.689823 [info ] [MainThread]: 
[0m09:56:23.705212 [info ] [MainThread]: Finished running 2 incremental models in 0 hours 0 minutes and 9.64 seconds (9.64s).
[0m09:56:23.705212 [debug] [MainThread]: Command end result
[0m09:56:23.776652 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m09:56:23.776652 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m09:56:23.793311 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\snowflake-incremental-pipeline\dbt_project\target\run_results.json
[0m09:56:23.795325 [info ] [MainThread]: 
[0m09:56:23.796233 [info ] [MainThread]: [32mCompleted successfully[0m
[0m09:56:23.796233 [info ] [MainThread]: 
[0m09:56:23.796233 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=2
[0m09:56:23.801728 [debug] [MainThread]: Command `dbt run` succeeded at 09:56:23.801728 after 14.28 seconds
[0m09:56:23.801728 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002185FD57700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000218726C2080>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002185E77B730>]}
[0m09:56:23.801728 [debug] [MainThread]: Flushing usage events
[0m09:56:24.848948 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:03:40.906666 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DC90A776D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DC91D20190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DC91D21B70>]}


============================== 10:03:40.914667 | 3077fbe5-720c-4849-82b0-e23f685deb33 ==============================
[0m10:03:40.914667 [info ] [MainThread]: Running with dbt=1.11.6
[0m10:03:40.916682 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'None', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'profiles_dir': 'D:\\snowflake-incremental-pipeline\\dbt_project', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'invocation_command': 'dbt clean', 'indirect_selection': 'eager', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'D:\\snowflake-incremental-pipeline\\dbt_project\\logs'}
[0m10:03:41.155181 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3077fbe5-720c-4849-82b0-e23f685deb33', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DC90E3E290>]}
[0m10:03:41.180592 [debug] [MainThread]: Command `dbt clean` succeeded at 10:03:41.180592 after 0.46 seconds
[0m10:03:41.180592 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DC90A776D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DC8E01EB00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DC91D8E200>]}
[0m10:03:41.186443 [debug] [MainThread]: Flushing usage events
[0m10:03:42.301857 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:03:48.668464 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AC549476D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AC55BF1000>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AC55BF3520>]}


============================== 10:03:48.669937 | bcf06440-7bec-4498-84c8-f7b6f7226223 ==============================
[0m10:03:48.669937 [info ] [MainThread]: Running with dbt=1.11.6
[0m10:03:48.675452 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'profiles_dir': 'D:\\snowflake-incremental-pipeline\\dbt_project', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'use_colors': 'True', 'invocation_command': 'dbt run', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'D:\\snowflake-incremental-pipeline\\dbt_project\\logs'}
[0m10:03:49.628360 [debug] [MainThread]: Snowflake adapter: Setting snowflake.connector to ERROR (file logging only)
[0m10:03:49.628360 [debug] [MainThread]: Snowflake adapter: Setting botocore to ERROR (file logging only)
[0m10:03:49.628360 [debug] [MainThread]: Snowflake adapter: Setting boto3 to ERROR (file logging only)
[0m10:03:49.957204 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'bcf06440-7bec-4498-84c8-f7b6f7226223', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AC55C312A0>]}
[0m10:03:50.061109 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'bcf06440-7bec-4498-84c8-f7b6f7226223', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AC55C8A860>]}
[0m10:03:50.061109 [info ] [MainThread]: Registered adapter: snowflake=1.11.2
[0m10:03:50.704449 [debug] [MainThread]: checksum: ec27062d18352e9cfca161c166fcc73d2201661a23a3cf3c89196a27f32cfcb8, vars: {}, profile: , target: , version: 1.11.6
[0m10:03:50.704449 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m10:03:50.714838 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'bcf06440-7bec-4498-84c8-f7b6f7226223', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AC66E2DC30>]}
[0m10:03:53.141784 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'bcf06440-7bec-4498-84c8-f7b6f7226223', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AC66D51BD0>]}
[0m10:03:53.318595 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m10:03:53.318595 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m10:03:53.343063 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'bcf06440-7bec-4498-84c8-f7b6f7226223', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AC672113F0>]}
[0m10:03:53.343063 [info ] [MainThread]: Found 2 models, 6 data tests, 1 source, 522 macros
[0m10:03:53.343063 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bcf06440-7bec-4498-84c8-f7b6f7226223', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AC672103A0>]}
[0m10:03:53.348397 [info ] [MainThread]: 
[0m10:03:53.348397 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m10:03:53.351171 [info ] [MainThread]: 
[0m10:03:53.351171 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m10:03:53.361126 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m10:03:53.370829 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m10:03:53.448609 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m10:03:53.448609 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m10:03:53.448609 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m10:03:53.448609 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m10:03:53.448609 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:03:53.448609 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:03:54.920152 [debug] [ThreadPool]: SQL status: SUCCESS 5 in 1.460 seconds
[0m10:03:54.940183 [debug] [ThreadPool]: SQL status: SUCCESS 5 in 1.480 seconds
[0m10:03:54.948461 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_INCREMENTALETL, now create_INCREMENTALETL__processed)
[0m10:03:54.950429 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_INCREMENTALETL, now create_INCREMENTALETL__staging)
[0m10:03:54.953595 [debug] [ThreadPool]: Creating schema "database: "INCREMENTALETL"
schema: "_processed"
"
[0m10:03:54.958871 [debug] [ThreadPool]: Creating schema "database: "INCREMENTALETL"
schema: "_staging"
"
[0m10:03:54.984349 [debug] [ThreadPool]: Using snowflake connection "create_INCREMENTALETL__processed"
[0m10:03:54.997325 [debug] [ThreadPool]: Using snowflake connection "create_INCREMENTALETL__staging"
[0m10:03:55.000679 [debug] [ThreadPool]: On create_INCREMENTALETL__processed: create schema if not exists INCREMENTALETL._processed
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "create_INCREMENTALETL__processed"} */
[0m10:03:55.000679 [debug] [ThreadPool]: On create_INCREMENTALETL__staging: create schema if not exists INCREMENTALETL._staging
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "create_INCREMENTALETL__staging"} */
[0m10:03:55.161832 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.156 seconds
[0m10:03:55.185383 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.175 seconds
[0m10:03:55.187718 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__processed'
[0m10:03:55.208355 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__staging'
[0m10:03:55.208355 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__processed"
[0m10:03:55.208355 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__staging"
[0m10:03:55.208355 [debug] [ThreadPool]: On list_INCREMENTALETL__processed: show objects in INCREMENTALETL._processed
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__processed"} */;
[0m10:03:55.216787 [debug] [ThreadPool]: On list_INCREMENTALETL__staging: show objects in INCREMENTALETL._staging
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__staging"} */;
[0m10:03:55.216787 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:03:55.216787 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:03:56.069196 [debug] [ThreadPool]: SQL status: SUCCESS 0 in 0.851 seconds
[0m10:03:56.593800 [debug] [ThreadPool]: SQL status: SUCCESS 0 in 1.376 seconds
[0m10:03:56.597331 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bcf06440-7bec-4498-84c8-f7b6f7226223', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AC68BF2950>]}
[0m10:03:56.606319 [debug] [Thread-2 (]: Began running node model.sales_pipelines.stg_sales
[0m10:03:56.606319 [info ] [Thread-2 (]: 1 of 2 START sql incremental model _staging.stg_sales .......................... [RUN]
[0m10:03:56.610423 [debug] [Thread-2 (]: Acquiring new snowflake connection 'model.sales_pipelines.stg_sales'
[0m10:03:56.611944 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.stg_sales
[0m10:03:56.636609 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.stg_sales"
[0m10:03:56.639914 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.stg_sales
[0m10:03:56.739705 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.stg_sales"
[0m10:03:56.747840 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:03:56.747840 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: create or replace transient table INCREMENTALETL._staging.stg_sales
    
    
    
    as (

WITH raw_data AS (
    SELECT * FROM INCREMENTALETL.LANDINGZONE.RAW_SALES
),

staged AS (
    SELECT
        -- â”€â”€ IDs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        CAST(INVOICENO       AS VARCHAR)    AS invoice_no,
        CAST(STOCKCODE       AS VARCHAR)    AS stock_code,
        CAST(DESCRIPTION     AS VARCHAR)    AS description,

        -- â”€â”€ Quantities & Prices â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        TRY_CAST(QUANTITY    AS INT)        AS quantity,
        TRY_CAST(UNITPRICE   AS FLOAT)      AS unit_price,
        TRY_CAST(DISCOUNT    AS FLOAT)      AS discount,
        TRY_CAST(SHIPPINGCOST AS FLOAT)     AS shipping_cost,
        TRY_CAST(CUSTOMERID  AS FLOAT)      AS customer_id,

        -- â”€â”€ Date Fix: VARCHAR â†’ proper TIMESTAMP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        -- Handles format: '2020-01-01 00:00:00'
        TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS') AS invoice_date,
        YEAR(TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS'))  AS invoice_year,
        MONTH(TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS')) AS invoice_month,

        -- â”€â”€ Categoricals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        TRIM(UPPER(COUNTRY))         AS country,
        TRIM(UPPER(PAYMENTMETHOD))   AS payment_method,
        TRIM(UPPER(CATEGORY))        AS category,
        TRIM(UPPER(SALESCHANNEL))    AS sales_channel,
        TRIM(UPPER(RETURNSTATUS))    AS return_status,
        TRIM(UPPER(SHIPMENTPROVIDER)) AS shipment_provider,
        TRIM(UPPER(WAREHOUSELOCATION)) AS warehouse_location,
        TRIM(UPPER(ORDERPRIORITY))   AS order_priority,

        -- â”€â”€ Audit columns â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        CURRENT_TIMESTAMP()          AS stg_loaded_at

    FROM raw_data
),

-- â”€â”€ Data Quality Flags â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
validated AS (
    SELECT
        *,
        CASE
            WHEN invoice_date IS NULL           THEN 'INVALID_DATE'
            WHEN quantity IS NULL               THEN 'INVALID_QUANTITY'
            WHEN unit_price IS NULL             THEN 'INVALID_PRICE'
            WHEN shipping_cost < 0              THEN 'NEGATIVE_SHIPPING'
            WHEN discount < 0 OR discount > 1   THEN 'INVALID_DISCOUNT'
            WHEN invoice_no IS NULL             THEN 'NULL_INVOICE'
            ELSE 'VALID'
        END AS data_quality_flag

    FROM staged
)

SELECT * FROM validated


    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:03:56.747840 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m10:03:58.128694 [debug] [Thread-2 (]: SQL status: SUCCESS 744 in 1.378 seconds
[0m10:03:58.144900 [debug] [Thread-2 (]: Applying DROP to: INCREMENTALETL._staging.stg_sales__dbt_tmp
[0m10:03:58.153332 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:03:58.153332 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: drop view if exists INCREMENTALETL._staging.stg_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:03:58.343759 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.184 seconds
[0m10:03:58.392271 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bcf06440-7bec-4498-84c8-f7b6f7226223', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AC68DC3820>]}
[0m10:03:58.394287 [info ] [Thread-2 (]: 1 of 2 OK created sql incremental model _staging.stg_sales ..................... [[32mSUCCESS 744[0m in 1.78s]
[0m10:03:58.394287 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.stg_sales
[0m10:03:58.394287 [debug] [Thread-4 (]: Began running node model.sales_pipelines.processed_sales
[0m10:03:58.401589 [info ] [Thread-4 (]: 2 of 2 START sql incremental model _processed.processed_sales .................. [RUN]
[0m10:03:58.407645 [debug] [Thread-4 (]: Acquiring new snowflake connection 'model.sales_pipelines.processed_sales'
[0m10:03:58.410268 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.processed_sales
[0m10:03:58.422339 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.processed_sales"
[0m10:03:58.426103 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.processed_sales
[0m10:03:58.437228 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.processed_sales"
[0m10:03:58.445610 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:03:58.447631 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: create or replace transient table INCREMENTALETL._processed.processed_sales
    
    
    
    as (

WITH staging AS (
    SELECT * FROM INCREMENTALETL._staging.stg_sales
),

-- â”€â”€ Step 1: Remove duplicates (keep latest loaded record) â”€â”€â”€â”€â”€â”€â”€â”€
deduped AS (
    SELECT *,
        ROW_NUMBER() OVER (
            PARTITION BY invoice_no
            ORDER BY stg_loaded_at DESC
        ) AS row_num
    FROM staging
),

-- â”€â”€ Step 2: Keep only VALID records, remove bad data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cleaned AS (
    SELECT
        invoice_no,
        stock_code,
        description,
        quantity,
        unit_price,
        discount,
        shipping_cost,
        customer_id,
        invoice_date,
        invoice_year,
        invoice_month,
        country,
        payment_method,
        category,
        sales_channel,
        return_status,
        shipment_provider,
        warehouse_location,
        order_priority,

        -- â”€â”€ Derived / Enriched columns â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        ROUND(quantity * unit_price, 2)                        AS gross_amount,
        ROUND(quantity * unit_price * (1 - discount), 2)       AS net_amount,
        ROUND(quantity * unit_price * (1 - discount)
              + COALESCE(shipping_cost, 0), 2)                 AS total_amount,

        CASE
            WHEN return_status = 'RETURNED' THEN TRUE
            ELSE FALSE
        END AS is_returned,

        CASE
            WHEN customer_id IS NULL THEN TRUE
            ELSE FALSE
        END AS is_guest_customer,

        -- â”€â”€ Audit â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        stg_loaded_at,
        CURRENT_TIMESTAMP() AS processed_at

    FROM deduped
    WHERE
        row_num = 1                     -- remove duplicates
        AND data_quality_flag = 'VALID' -- only clean records
        AND quantity > 0                -- remove returns/negatives
        AND unit_price > 0              -- remove invalid prices
        AND invoice_date IS NOT NULL    -- must have valid date
),

-- â”€â”€ Step 3: Final output â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
final AS (
    SELECT * FROM cleaned
)

SELECT * FROM final


    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:03:58.447631 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m10:03:59.976942 [debug] [Thread-4 (]: SQL status: SUCCESS 714 in 1.526 seconds
[0m10:03:59.989050 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._processed.processed_sales__dbt_tmp
[0m10:03:59.993079 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:03:59.993079 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: drop view if exists INCREMENTALETL._processed.processed_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:04:00.163424 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.166 seconds
[0m10:04:00.169192 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bcf06440-7bec-4498-84c8-f7b6f7226223', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AC68D7D960>]}
[0m10:04:00.171199 [info ] [Thread-4 (]: 2 of 2 OK created sql incremental model _processed.processed_sales ............. [[32mSUCCESS 714[0m in 1.76s]
[0m10:04:00.173207 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.processed_sales
[0m10:04:00.178518 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:04:00.180531 [debug] [MainThread]: Connection 'create_INCREMENTALETL__staging' was left open.
[0m10:04:00.180531 [debug] [MainThread]: On create_INCREMENTALETL__staging: Close
[0m10:04:00.488581 [debug] [MainThread]: Connection 'create_INCREMENTALETL__processed' was left open.
[0m10:04:00.490591 [debug] [MainThread]: On create_INCREMENTALETL__processed: Close
[0m10:04:00.730013 [debug] [MainThread]: Connection 'list_INCREMENTALETL__processed' was left open.
[0m10:04:00.732024 [debug] [MainThread]: On list_INCREMENTALETL__processed: Close
[0m10:04:01.101549 [debug] [MainThread]: Connection 'list_INCREMENTALETL__staging' was left open.
[0m10:04:01.103563 [debug] [MainThread]: On list_INCREMENTALETL__staging: Close
[0m10:04:01.383362 [debug] [MainThread]: Connection 'model.sales_pipelines.stg_sales' was left open.
[0m10:04:01.385377 [debug] [MainThread]: On model.sales_pipelines.stg_sales: Close
[0m10:04:01.643452 [debug] [MainThread]: Connection 'model.sales_pipelines.processed_sales' was left open.
[0m10:04:01.645853 [debug] [MainThread]: On model.sales_pipelines.processed_sales: Close
[0m10:04:02.021858 [info ] [MainThread]: 
[0m10:04:02.023878 [info ] [MainThread]: Finished running 2 incremental models in 0 hours 0 minutes and 8.67 seconds (8.67s).
[0m10:04:02.027909 [debug] [MainThread]: Command end result
[0m10:04:02.071805 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m10:04:02.085588 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m10:04:02.101378 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\snowflake-incremental-pipeline\dbt_project\target\run_results.json
[0m10:04:02.103392 [info ] [MainThread]: 
[0m10:04:02.105406 [info ] [MainThread]: [32mCompleted successfully[0m
[0m10:04:02.107421 [info ] [MainThread]: 
[0m10:04:02.109439 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=2
[0m10:04:02.111449 [debug] [MainThread]: Command `dbt run` succeeded at 10:04:02.111449 after 13.57 seconds
[0m10:04:02.113460 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AC549476D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AC672113F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AC51D4FA60>]}
[0m10:04:02.113460 [debug] [MainThread]: Flushing usage events
[0m10:04:03.012411 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:17:07.891131 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001737A0376D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001737B2E2620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001737B2E30D0>]}


============================== 10:17:07.902134 | 08552267-c37e-463b-85c5-ce4da7140f22 ==============================
[0m10:17:07.902134 [info ] [MainThread]: Running with dbt=1.11.6
[0m10:17:07.902134 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'profiles_dir': 'D:\\snowflake-incremental-pipeline\\dbt_project', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'use_colors': 'True', 'invocation_command': 'dbt run', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'D:\\snowflake-incremental-pipeline\\dbt_project\\logs'}
[0m10:17:08.853547 [debug] [MainThread]: Snowflake adapter: Setting snowflake.connector to ERROR (file logging only)
[0m10:17:08.853547 [debug] [MainThread]: Snowflake adapter: Setting botocore to ERROR (file logging only)
[0m10:17:08.853547 [debug] [MainThread]: Snowflake adapter: Setting boto3 to ERROR (file logging only)
[0m10:17:09.157834 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '08552267-c37e-463b-85c5-ce4da7140f22', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001730CA31AE0>]}
[0m10:17:09.235630 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '08552267-c37e-463b-85c5-ce4da7140f22', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001737B37BD60>]}
[0m10:17:09.235630 [info ] [MainThread]: Registered adapter: snowflake=1.11.2
[0m10:17:09.745253 [debug] [MainThread]: checksum: ec27062d18352e9cfca161c166fcc73d2201661a23a3cf3c89196a27f32cfcb8, vars: {}, profile: , target: , version: 1.11.6
[0m10:17:10.126420 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m10:17:10.126420 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m10:17:10.126420 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m10:17:10.205790 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '08552267-c37e-463b-85c5-ce4da7140f22', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001730E11C130>]}
[0m10:17:10.349166 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m10:17:10.349166 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m10:17:10.365031 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '08552267-c37e-463b-85c5-ce4da7140f22', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001730E0BC700>]}
[0m10:17:10.365031 [info ] [MainThread]: Found 2 models, 6 data tests, 1 source, 522 macros
[0m10:17:10.365031 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '08552267-c37e-463b-85c5-ce4da7140f22', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001730E0BC730>]}
[0m10:17:10.381760 [info ] [MainThread]: 
[0m10:17:10.381760 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m10:17:10.381760 [info ] [MainThread]: 
[0m10:17:10.381760 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m10:17:10.381760 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m10:17:10.396421 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m10:17:10.492117 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m10:17:10.492117 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m10:17:10.492117 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m10:17:10.492117 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m10:17:10.492117 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:17:10.492117 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:17:12.227193 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 1.726 seconds
[0m10:17:12.212712 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 1.721 seconds
[0m10:17:12.245233 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__processed'
[0m10:17:12.245233 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__staging'
[0m10:17:12.316313 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__processed"
[0m10:17:12.323346 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__staging"
[0m10:17:12.323346 [debug] [ThreadPool]: On list_INCREMENTALETL__processed: show objects in INCREMENTALETL._processed
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__processed"} */;
[0m10:17:12.323346 [debug] [ThreadPool]: On list_INCREMENTALETL__staging: show objects in INCREMENTALETL._staging
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__staging"} */;
[0m10:17:12.323346 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:17:12.323346 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:17:13.100457 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.769 seconds
[0m10:17:13.157559 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.825 seconds
[0m10:17:13.157559 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '08552267-c37e-463b-85c5-ce4da7140f22', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001730CA33670>]}
[0m10:17:13.157559 [debug] [Thread-2 (]: Began running node model.sales_pipelines.stg_sales
[0m10:17:13.173400 [info ] [Thread-2 (]: 1 of 2 START sql incremental model _staging.stg_sales .......................... [RUN]
[0m10:17:13.173400 [debug] [Thread-2 (]: Acquiring new snowflake connection 'model.sales_pipelines.stg_sales'
[0m10:17:13.173400 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.stg_sales
[0m10:17:13.189398 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.stg_sales"
[0m10:17:13.205458 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.stg_sales
[0m10:17:13.286035 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:17:13.286035 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: create or replace  temporary view INCREMENTALETL._staging.stg_sales__dbt_tmp
  
  
  
  
  as (
    

WITH raw_data AS (
    SELECT * FROM INCREMENTALETL.LANDINGZONE.RAW_SALES
),

staged AS (
    SELECT
        -- â”€â”€ IDs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        CAST(INVOICENO       AS VARCHAR)    AS invoice_no,
        CAST(STOCKCODE       AS VARCHAR)    AS stock_code,
        CAST(DESCRIPTION     AS VARCHAR)    AS description,

        -- â”€â”€ Quantities & Prices â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        TRY_CAST(QUANTITY    AS INT)        AS quantity,
        TRY_CAST(UNITPRICE   AS FLOAT)      AS unit_price,
        TRY_CAST(DISCOUNT    AS FLOAT)      AS discount,
        TRY_CAST(SHIPPINGCOST AS FLOAT)     AS shipping_cost,
        TRY_CAST(CUSTOMERID  AS FLOAT)      AS customer_id,

        -- â”€â”€ Date Fix: VARCHAR â†’ proper TIMESTAMP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        -- Handles format: '2020-01-01 00:00:00'
        TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS') AS invoice_date,
        YEAR(TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS'))  AS invoice_year,
        MONTH(TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS')) AS invoice_month,

        -- â”€â”€ Categoricals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        TRIM(UPPER(COUNTRY))         AS country,
        TRIM(UPPER(PAYMENTMETHOD))   AS payment_method,
        TRIM(UPPER(CATEGORY))        AS category,
        TRIM(UPPER(SALESCHANNEL))    AS sales_channel,
        TRIM(UPPER(RETURNSTATUS))    AS return_status,
        TRIM(UPPER(SHIPMENTPROVIDER)) AS shipment_provider,
        TRIM(UPPER(WAREHOUSELOCATION)) AS warehouse_location,
        TRIM(UPPER(ORDERPRIORITY))   AS order_priority,

        -- â”€â”€ Audit columns â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        CURRENT_TIMESTAMP()          AS stg_loaded_at

    FROM raw_data
),

-- â”€â”€ Data Quality Flags â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
validated AS (
    SELECT
        *,
        CASE
            WHEN invoice_date IS NULL           THEN 'INVALID_DATE'
            WHEN quantity IS NULL               THEN 'INVALID_QUANTITY'
            WHEN unit_price IS NULL             THEN 'INVALID_PRICE'
            WHEN shipping_cost < 0              THEN 'NEGATIVE_SHIPPING'
            WHEN discount < 0 OR discount > 1   THEN 'INVALID_DISCOUNT'
            WHEN invoice_no IS NULL             THEN 'NULL_INVOICE'
            ELSE 'VALID'
        END AS data_quality_flag

    FROM staged
)

SELECT * FROM validated


    -- â”€â”€ Incremental: only process new months not already in staging â”€â”€
    WHERE (invoice_year, invoice_month) NOT IN (
        SELECT DISTINCT invoice_year, invoice_month
        FROM INCREMENTALETL._staging.stg_sales
    )

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:17:13.286035 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m10:17:14.557558 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 1.269 seconds
[0m10:17:14.573678 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:17:14.573678 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:17:14.717784 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.139 seconds
[0m10:17:14.733104 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:17:14.735116 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:17:14.871085 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.136 seconds
[0m10:17:14.922780 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:17:14.924796 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:17:15.062185 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.134 seconds
[0m10:17:15.179606 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:17:15.179606 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table "INCREMENTALETL"."_STAGING"."STG_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:17:15.312086 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.136 seconds
[0m10:17:15.358521 [debug] [Thread-2 (]: 
    In "INCREMENTALETL"."_STAGING"."STG_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m10:17:15.426055 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.stg_sales"
[0m10:17:15.426055 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:17:15.426055 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:17:15.579533 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.151 seconds
[0m10:17:15.587558 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:17:15.591261 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: merge into INCREMENTALETL._staging.stg_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._staging.stg_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on ((DBT_INTERNAL_SOURCE.INVOICENO = DBT_INTERNAL_DEST.INVOICENO))

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","DATA_QUALITY_FLAG" = DBT_INTERNAL_SOURCE."DATA_QUALITY_FLAG"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "DATA_QUALITY_FLAG")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "DATA_QUALITY_FLAG")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:17:15.775314 [debug] [Thread-2 (]: Snowflake adapter: Snowflake query id: 01c2829d-3202-5b3f-0013-d452000962be
[0m10:17:15.775314 [debug] [Thread-2 (]: Snowflake adapter: Snowflake error: 000904 (42000): SQL compilation error: error line 3 at position 13
invalid identifier 'DBT_INTERNAL_SOURCE.INVOICENO'
[0m10:17:15.782805 [debug] [Thread-2 (]: Database Error in model stg_sales (models\staging\stg_sales.sql)
  000904 (42000): SQL compilation error: error line 3 at position 13
  invalid identifier 'DBT_INTERNAL_SOURCE.INVOICENO'
  compiled code at target\run\sales_pipelines\models\staging\stg_sales.sql
[0m10:17:15.791247 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '08552267-c37e-463b-85c5-ce4da7140f22', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001730E77FD00>]}
[0m10:17:15.791247 [error] [Thread-2 (]: 1 of 2 ERROR creating sql incremental model _staging.stg_sales ................. [[31mERROR[0m in 2.62s]
[0m10:17:15.791247 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.stg_sales
[0m10:17:15.791247 [debug] [Thread-8 (]: Marking all children of 'model.sales_pipelines.stg_sales' to be skipped because of status 'error'.  Reason: Database Error in model stg_sales (models\staging\stg_sales.sql)
  000904 (42000): SQL compilation error: error line 3 at position 13
  invalid identifier 'DBT_INTERNAL_SOURCE.INVOICENO'
  compiled code at target\run\sales_pipelines\models\staging\stg_sales.sql.
[0m10:17:15.791247 [debug] [Thread-4 (]: Began running node model.sales_pipelines.processed_sales
[0m10:17:15.806798 [info ] [Thread-4 (]: 2 of 2 SKIP relation _processed.processed_sales ................................ [[33mSKIP[0m]
[0m10:17:15.808522 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.processed_sales
[0m10:17:15.808522 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:17:15.814862 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m10:17:15.814862 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m10:17:16.108477 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m10:17:16.108477 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m10:17:16.358467 [debug] [MainThread]: Connection 'list_INCREMENTALETL__processed' was left open.
[0m10:17:16.360093 [debug] [MainThread]: On list_INCREMENTALETL__processed: Close
[0m10:17:16.635513 [debug] [MainThread]: Connection 'list_INCREMENTALETL__staging' was left open.
[0m10:17:16.637893 [debug] [MainThread]: On list_INCREMENTALETL__staging: Close
[0m10:17:16.921236 [debug] [MainThread]: Connection 'model.sales_pipelines.stg_sales' was left open.
[0m10:17:16.921236 [debug] [MainThread]: On model.sales_pipelines.stg_sales: Close
[0m10:17:17.181771 [info ] [MainThread]: 
[0m10:17:17.181771 [info ] [MainThread]: Finished running 2 incremental models in 0 hours 0 minutes and 6.80 seconds (6.80s).
[0m10:17:17.188638 [debug] [MainThread]: Command end result
[0m10:17:17.265020 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m10:17:17.267809 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m10:17:17.282650 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\snowflake-incremental-pipeline\dbt_project\target\run_results.json
[0m10:17:17.282650 [info ] [MainThread]: 
[0m10:17:17.284660 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m10:17:17.286678 [info ] [MainThread]: 
[0m10:17:17.288963 [error] [MainThread]: [31mFailure in model stg_sales (models\staging\stg_sales.sql)[0m
[0m10:17:17.290706 [error] [MainThread]:   Database Error in model stg_sales (models\staging\stg_sales.sql)
  000904 (42000): SQL compilation error: error line 3 at position 13
  invalid identifier 'DBT_INTERNAL_SOURCE.INVOICENO'
  compiled code at target\run\sales_pipelines\models\staging\stg_sales.sql
[0m10:17:17.293018 [info ] [MainThread]: 
[0m10:17:17.295865 [info ] [MainThread]:   compiled code at target\compiled\sales_pipelines\models\staging\stg_sales.sql
[0m10:17:17.295865 [info ] [MainThread]: 
[0m10:17:17.298077 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=1 NO-OP=0 TOTAL=2
[0m10:17:17.299619 [debug] [MainThread]: Command `dbt run` failed at 10:17:17.299619 after 9.56 seconds
[0m10:17:17.301629 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001737A0376D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017378A2C400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001730CEFB130>]}
[0m10:17:17.301629 [debug] [MainThread]: Flushing usage events
[0m10:17:18.958681 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:18:44.808764 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E9FD947640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E9FEBF33A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E9FEBF35B0>]}


============================== 10:18:44.828448 | 7211c3a9-729a-44e2-9508-e4f443bc1949 ==============================
[0m10:18:44.828448 [info ] [MainThread]: Running with dbt=1.11.6
[0m10:18:44.830479 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'profiles_dir': 'D:\\snowflake-incremental-pipeline\\dbt_project', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'invocation_command': 'dbt run', 'indirect_selection': 'eager', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'D:\\snowflake-incremental-pipeline\\dbt_project\\logs'}
[0m10:18:45.785023 [debug] [MainThread]: Snowflake adapter: Setting snowflake.connector to ERROR (file logging only)
[0m10:18:45.785023 [debug] [MainThread]: Snowflake adapter: Setting botocore to ERROR (file logging only)
[0m10:18:45.787032 [debug] [MainThread]: Snowflake adapter: Setting boto3 to ERROR (file logging only)
[0m10:18:46.093226 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7211c3a9-729a-44e2-9508-e4f443bc1949', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E9FEBF2A40>]}
[0m10:18:46.172758 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7211c3a9-729a-44e2-9508-e4f443bc1949', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E9FEA4A110>]}
[0m10:18:46.172758 [info ] [MainThread]: Registered adapter: snowflake=1.11.2
[0m10:18:46.759931 [debug] [MainThread]: checksum: ec27062d18352e9cfca161c166fcc73d2201661a23a3cf3c89196a27f32cfcb8, vars: {}, profile: , target: , version: 1.11.6
[0m10:18:47.037530 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m10:18:47.037530 [debug] [MainThread]: Partial parsing: updated file: sales_pipelines://models\staging\stg_sales.sql
[0m10:18:47.617298 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7211c3a9-729a-44e2-9508-e4f443bc1949', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E991781AB0>]}
[0m10:18:47.769726 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m10:18:47.771733 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m10:18:47.791283 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7211c3a9-729a-44e2-9508-e4f443bc1949', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E99130F670>]}
[0m10:18:47.794313 [info ] [MainThread]: Found 2 models, 6 data tests, 1 source, 522 macros
[0m10:18:47.794313 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7211c3a9-729a-44e2-9508-e4f443bc1949', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E9912DDC90>]}
[0m10:18:47.799270 [info ] [MainThread]: 
[0m10:18:47.799270 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m10:18:47.799270 [info ] [MainThread]: 
[0m10:18:47.799270 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m10:18:47.809249 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m10:18:47.822927 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m10:18:48.021611 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m10:18:48.021611 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m10:18:48.021611 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m10:18:48.021611 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m10:18:48.027119 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:18:48.027119 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:18:50.145169 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 2.132 seconds
[0m10:18:50.326864 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 2.299 seconds
[0m10:18:50.338166 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__staging'
[0m10:18:50.338166 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__processed'
[0m10:18:50.369945 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__staging"
[0m10:18:50.385838 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__processed"
[0m10:18:50.385838 [debug] [ThreadPool]: On list_INCREMENTALETL__staging: show objects in INCREMENTALETL._staging
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__staging"} */;
[0m10:18:50.385838 [debug] [ThreadPool]: On list_INCREMENTALETL__processed: show objects in INCREMENTALETL._processed
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__processed"} */;
[0m10:18:50.385838 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:18:50.398039 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:18:51.210860 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.816 seconds
[0m10:18:51.210860 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.823 seconds
[0m10:18:51.236610 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7211c3a9-729a-44e2-9508-e4f443bc1949', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E98FE75360>]}
[0m10:18:51.252775 [debug] [Thread-2 (]: Began running node model.sales_pipelines.stg_sales
[0m10:18:51.252775 [info ] [Thread-2 (]: 1 of 2 START sql incremental model _staging.stg_sales .......................... [RUN]
[0m10:18:51.252775 [debug] [Thread-2 (]: Acquiring new snowflake connection 'model.sales_pipelines.stg_sales'
[0m10:18:51.252775 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.stg_sales
[0m10:18:51.300357 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.stg_sales"
[0m10:18:51.300357 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.stg_sales
[0m10:18:51.382188 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:18:51.382188 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: create or replace  temporary view INCREMENTALETL._staging.stg_sales__dbt_tmp
  
  
  
  
  as (
    

WITH raw_data AS (
    SELECT * FROM INCREMENTALETL.LANDINGZONE.RAW_SALES
),

staged AS (
    SELECT
        -- â”€â”€ IDs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        CAST(INVOICENO       AS VARCHAR)    AS invoice_no,
        CAST(STOCKCODE       AS VARCHAR)    AS stock_code,
        CAST(DESCRIPTION     AS VARCHAR)    AS description,

        -- â”€â”€ Quantities & Prices â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        TRY_CAST(QUANTITY    AS INT)        AS quantity,
        TRY_CAST(UNITPRICE   AS FLOAT)      AS unit_price,
        TRY_CAST(DISCOUNT    AS FLOAT)      AS discount,
        TRY_CAST(SHIPPINGCOST AS FLOAT)     AS shipping_cost,
        TRY_CAST(CUSTOMERID  AS FLOAT)      AS customer_id,

        -- â”€â”€ Date Fix: VARCHAR â†’ proper TIMESTAMP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        -- Handles format: '2020-01-01 00:00:00'
        TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS') AS invoice_date,
        YEAR(TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS'))  AS invoice_year,
        MONTH(TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS')) AS invoice_month,

        -- â”€â”€ Categoricals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        TRIM(UPPER(COUNTRY))         AS country,
        TRIM(UPPER(PAYMENTMETHOD))   AS payment_method,
        TRIM(UPPER(CATEGORY))        AS category,
        TRIM(UPPER(SALESCHANNEL))    AS sales_channel,
        TRIM(UPPER(RETURNSTATUS))    AS return_status,
        TRIM(UPPER(SHIPMENTPROVIDER)) AS shipment_provider,
        TRIM(UPPER(WAREHOUSELOCATION)) AS warehouse_location,
        TRIM(UPPER(ORDERPRIORITY))   AS order_priority,

        -- â”€â”€ Audit columns â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        CURRENT_TIMESTAMP()          AS stg_loaded_at

    FROM raw_data
),

-- â”€â”€ Data Quality Flags â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
validated AS (
    SELECT
        *,
        CASE
            WHEN invoice_date IS NULL           THEN 'INVALID_DATE'
            WHEN quantity IS NULL               THEN 'INVALID_QUANTITY'
            WHEN unit_price IS NULL             THEN 'INVALID_PRICE'
            WHEN shipping_cost < 0              THEN 'NEGATIVE_SHIPPING'
            WHEN discount < 0 OR discount > 1   THEN 'INVALID_DISCOUNT'
            WHEN invoice_no IS NULL             THEN 'NULL_INVOICE'
            ELSE 'VALID'
        END AS data_quality_flag

    FROM staged
)

SELECT * FROM validated


    -- â”€â”€ Incremental: only process new months not already in staging â”€â”€
    WHERE (invoice_year, invoice_month) NOT IN (
        SELECT DISTINCT invoice_year, invoice_month
        FROM INCREMENTALETL._staging.stg_sales
    )

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:18:51.382188 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m10:18:52.661523 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 1.273 seconds
[0m10:18:52.668130 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:18:52.668130 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:18:52.818262 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.142 seconds
[0m10:18:52.845875 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:18:52.845875 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:18:52.982028 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.136 seconds
[0m10:18:53.034139 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:18:53.034139 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:18:53.172375 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.132 seconds
[0m10:18:53.194097 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:18:53.198138 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table "INCREMENTALETL"."_STAGING"."STG_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:18:53.318790 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.131 seconds
[0m10:18:53.366427 [debug] [Thread-2 (]: 
    In "INCREMENTALETL"."_STAGING"."STG_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m10:18:53.413467 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.stg_sales"
[0m10:18:53.413467 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:18:53.413467 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:18:53.560876 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.156 seconds
[0m10:18:53.577021 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:18:53.577021 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: merge into INCREMENTALETL._staging.stg_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._staging.stg_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on ((DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no))

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","DATA_QUALITY_FLAG" = DBT_INTERNAL_SOURCE."DATA_QUALITY_FLAG"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "DATA_QUALITY_FLAG")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "DATA_QUALITY_FLAG")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:18:54.408451 [debug] [Thread-2 (]: SQL status: SUCCESS 696 in 0.831 seconds
[0m10:18:54.408451 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:18:54.408451 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:18:54.666283 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.253 seconds
[0m10:18:54.698197 [debug] [Thread-2 (]: Applying DROP to: INCREMENTALETL._staging.stg_sales__dbt_tmp
[0m10:18:54.698197 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:18:54.698197 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: drop view if exists INCREMENTALETL._staging.stg_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:18:54.859094 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.159 seconds
[0m10:18:54.922767 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7211c3a9-729a-44e2-9508-e4f443bc1949', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E9FE40ADD0>]}
[0m10:18:54.922767 [info ] [Thread-2 (]: 1 of 2 OK created sql incremental model _staging.stg_sales ..................... [[32mSUCCESS 696[0m in 3.67s]
[0m10:18:54.922767 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.stg_sales
[0m10:18:54.922767 [debug] [Thread-4 (]: Began running node model.sales_pipelines.processed_sales
[0m10:18:54.922767 [info ] [Thread-4 (]: 2 of 2 START sql incremental model _processed.processed_sales .................. [RUN]
[0m10:18:54.922767 [debug] [Thread-4 (]: Acquiring new snowflake connection 'model.sales_pipelines.processed_sales'
[0m10:18:54.922767 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.processed_sales
[0m10:18:54.938674 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.processed_sales"
[0m10:18:54.941829 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.processed_sales
[0m10:18:54.941829 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:18:54.941829 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: create or replace  temporary view INCREMENTALETL._processed.processed_sales__dbt_tmp
  
  
  
  
  as (
    

WITH staging AS (
    SELECT * FROM INCREMENTALETL._staging.stg_sales
),

-- â”€â”€ Step 1: Remove duplicates (keep latest loaded record) â”€â”€â”€â”€â”€â”€â”€â”€
deduped AS (
    SELECT *,
        ROW_NUMBER() OVER (
            PARTITION BY invoice_no
            ORDER BY stg_loaded_at DESC
        ) AS row_num
    FROM staging
),

-- â”€â”€ Step 2: Keep only VALID records, remove bad data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cleaned AS (
    SELECT
        invoice_no,
        stock_code,
        description,
        quantity,
        unit_price,
        discount,
        shipping_cost,
        customer_id,
        invoice_date,
        invoice_year,
        invoice_month,
        country,
        payment_method,
        category,
        sales_channel,
        return_status,
        shipment_provider,
        warehouse_location,
        order_priority,

        -- â”€â”€ Derived / Enriched columns â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        ROUND(quantity * unit_price, 2)                        AS gross_amount,
        ROUND(quantity * unit_price * (1 - discount), 2)       AS net_amount,
        ROUND(quantity * unit_price * (1 - discount)
              + COALESCE(shipping_cost, 0), 2)                 AS total_amount,

        CASE
            WHEN return_status = 'RETURNED' THEN TRUE
            ELSE FALSE
        END AS is_returned,

        CASE
            WHEN customer_id IS NULL THEN TRUE
            ELSE FALSE
        END AS is_guest_customer,

        -- â”€â”€ Audit â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        stg_loaded_at,
        CURRENT_TIMESTAMP() AS processed_at

    FROM deduped
    WHERE
        row_num = 1                     -- remove duplicates
        AND data_quality_flag = 'VALID' -- only clean records
        AND quantity > 0                -- remove returns/negatives
        AND unit_price > 0              -- remove invalid prices
        AND invoice_date IS NOT NULL    -- must have valid date
),

-- â”€â”€ Step 3: Final output â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
final AS (
    SELECT * FROM cleaned
)

SELECT * FROM final


    -- â”€â”€ Incremental: only process months not already in processed â”€â”€
    WHERE (invoice_year, invoice_month) NOT IN (
        SELECT DISTINCT invoice_year, invoice_month
        FROM INCREMENTALETL._processed.processed_sales
    )

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:18:54.954630 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m10:18:55.982815 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 1.028 seconds
[0m10:18:55.986578 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:18:55.986578 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:18:56.250710 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.257 seconds
[0m10:18:56.266556 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:18:56.268572 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:18:56.448432 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.189 seconds
[0m10:18:56.480405 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:18:56.480405 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:18:57.157818 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.686 seconds
[0m10:18:57.173654 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:18:57.173654 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:18:57.481154 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.300 seconds
[0m10:18:57.504038 [debug] [Thread-4 (]: 
    In "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m10:18:57.511918 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.processed_sales"
[0m10:18:57.515002 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:18:57.515002 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:18:57.687611 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.171 seconds
[0m10:18:57.687611 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:18:57.687611 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: merge into INCREMENTALETL._processed.processed_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._processed.processed_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on ((DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no))

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","GROSS_AMOUNT" = DBT_INTERNAL_SOURCE."GROSS_AMOUNT","NET_AMOUNT" = DBT_INTERNAL_SOURCE."NET_AMOUNT","TOTAL_AMOUNT" = DBT_INTERNAL_SOURCE."TOTAL_AMOUNT","IS_RETURNED" = DBT_INTERNAL_SOURCE."IS_RETURNED","IS_GUEST_CUSTOMER" = DBT_INTERNAL_SOURCE."IS_GUEST_CUSTOMER","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","PROCESSED_AT" = DBT_INTERNAL_SOURCE."PROCESSED_AT"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:18:58.732367 [debug] [Thread-4 (]: SQL status: SUCCESS 664 in 1.047 seconds
[0m10:18:58.732367 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:18:58.732367 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:18:59.119354 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.370 seconds
[0m10:18:59.123383 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._processed.processed_sales__dbt_tmp
[0m10:18:59.123383 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:18:59.123383 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: drop view if exists INCREMENTALETL._processed.processed_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:18:59.321969 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.195 seconds
[0m10:18:59.332420 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7211c3a9-729a-44e2-9508-e4f443bc1949', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E9FC7CA860>]}
[0m10:18:59.332420 [info ] [Thread-4 (]: 2 of 2 OK created sql incremental model _processed.processed_sales ............. [[32mSUCCESS 664[0m in 4.41s]
[0m10:18:59.348271 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.processed_sales
[0m10:18:59.356628 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:18:59.356628 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m10:18:59.356628 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m10:18:59.654221 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m10:18:59.656246 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m10:18:59.930762 [debug] [MainThread]: Connection 'list_INCREMENTALETL__staging' was left open.
[0m10:18:59.930762 [debug] [MainThread]: On list_INCREMENTALETL__staging: Close
[0m10:19:00.216856 [debug] [MainThread]: Connection 'list_INCREMENTALETL__processed' was left open.
[0m10:19:00.218866 [debug] [MainThread]: On list_INCREMENTALETL__processed: Close
[0m10:19:00.653598 [debug] [MainThread]: Connection 'model.sales_pipelines.stg_sales' was left open.
[0m10:19:00.655621 [debug] [MainThread]: On model.sales_pipelines.stg_sales: Close
[0m10:19:00.935240 [debug] [MainThread]: Connection 'model.sales_pipelines.processed_sales' was left open.
[0m10:19:00.935240 [debug] [MainThread]: On model.sales_pipelines.processed_sales: Close
[0m10:19:01.981693 [info ] [MainThread]: 
[0m10:19:01.981693 [info ] [MainThread]: Finished running 2 incremental models in 0 hours 0 minutes and 14.18 seconds (14.18s).
[0m10:19:01.999308 [debug] [MainThread]: Command end result
[0m10:19:02.045782 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m10:19:02.045782 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m10:19:02.045782 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\snowflake-incremental-pipeline\dbt_project\target\run_results.json
[0m10:19:02.045782 [info ] [MainThread]: 
[0m10:19:02.045782 [info ] [MainThread]: [32mCompleted successfully[0m
[0m10:19:02.061472 [info ] [MainThread]: 
[0m10:19:02.062951 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=2
[0m10:19:02.062951 [debug] [MainThread]: Command `dbt run` succeeded at 10:19:02.062951 after 17.37 seconds
[0m10:19:02.062951 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E9FD947640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E991780970>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E9FDD0B670>]}
[0m10:19:02.062951 [debug] [MainThread]: Flushing usage events
[0m10:19:04.790487 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:22:33.625023 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021FFF897640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021F81C633A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021F81C635B0>]}


============================== 10:22:33.625023 | 85ef9ac8-6b2c-4f3a-83eb-fadc531490d9 ==============================
[0m10:22:33.625023 [info ] [MainThread]: Running with dbt=1.11.6
[0m10:22:33.632891 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'D:\\snowflake-incremental-pipeline\\dbt_project', 'indirect_selection': 'eager', 'use_colors': 'True', 'invocation_command': 'dbt run', 'use_experimental_parser': 'False', 'log_path': 'D:\\snowflake-incremental-pipeline\\dbt_project\\logs'}
[0m10:22:34.552762 [debug] [MainThread]: Snowflake adapter: Setting snowflake.connector to ERROR (file logging only)
[0m10:22:34.552762 [debug] [MainThread]: Snowflake adapter: Setting botocore to ERROR (file logging only)
[0m10:22:34.568788 [debug] [MainThread]: Snowflake adapter: Setting boto3 to ERROR (file logging only)
[0m10:22:34.853669 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '85ef9ac8-6b2c-4f3a-83eb-fadc531490d9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021F81C62A40>]}
[0m10:22:34.948363 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '85ef9ac8-6b2c-4f3a-83eb-fadc531490d9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021F81ABA110>]}
[0m10:22:34.948363 [info ] [MainThread]: Registered adapter: snowflake=1.11.2
[0m10:22:35.454595 [debug] [MainThread]: checksum: ec27062d18352e9cfca161c166fcc73d2201661a23a3cf3c89196a27f32cfcb8, vars: {}, profile: , target: , version: 1.11.6
[0m10:22:35.739202 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m10:22:35.739202 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m10:22:35.739202 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m10:22:35.818902 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '85ef9ac8-6b2c-4f3a-83eb-fadc531490d9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021F9436C130>]}
[0m10:22:35.961573 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m10:22:35.961573 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m10:22:35.977679 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '85ef9ac8-6b2c-4f3a-83eb-fadc531490d9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021F9430FB80>]}
[0m10:22:35.993486 [info ] [MainThread]: Found 2 models, 6 data tests, 1 source, 522 macros
[0m10:22:35.993486 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '85ef9ac8-6b2c-4f3a-83eb-fadc531490d9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021F9430FC10>]}
[0m10:22:35.999612 [info ] [MainThread]: 
[0m10:22:35.999612 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m10:22:35.999612 [info ] [MainThread]: 
[0m10:22:35.999612 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m10:22:36.009539 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m10:22:36.025399 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m10:22:36.104016 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m10:22:36.104016 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m10:22:36.104016 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m10:22:36.104016 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m10:22:36.104016 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:22:36.104016 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:22:37.693480 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 1.585 seconds
[0m10:22:37.710705 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 1.598 seconds
[0m10:22:37.718988 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__staging'
[0m10:22:37.725105 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__processed'
[0m10:22:37.756763 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__staging"
[0m10:22:37.772704 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__processed"
[0m10:22:37.772704 [debug] [ThreadPool]: On list_INCREMENTALETL__staging: show objects in INCREMENTALETL._staging
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__staging"} */;
[0m10:22:37.772704 [debug] [ThreadPool]: On list_INCREMENTALETL__processed: show objects in INCREMENTALETL._processed
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__processed"} */;
[0m10:22:37.772704 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:22:37.772704 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:22:38.654925 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.883 seconds
[0m10:22:38.670578 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.906 seconds
[0m10:22:38.686492 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '85ef9ac8-6b2c-4f3a-83eb-fadc531490d9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021F943CE860>]}
[0m10:22:38.703229 [debug] [Thread-2 (]: Began running node model.sales_pipelines.stg_sales
[0m10:22:38.703229 [info ] [Thread-2 (]: 1 of 2 START sql incremental model _staging.stg_sales .......................... [RUN]
[0m10:22:38.714409 [debug] [Thread-2 (]: Acquiring new snowflake connection 'model.sales_pipelines.stg_sales'
[0m10:22:38.718214 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.stg_sales
[0m10:22:38.757998 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.stg_sales"
[0m10:22:38.760005 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.stg_sales
[0m10:22:38.836959 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:22:38.838964 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: create or replace  temporary view INCREMENTALETL._staging.stg_sales__dbt_tmp
  
  
  
  
  as (
    

WITH raw_data AS (
    SELECT * FROM INCREMENTALETL.LANDINGZONE.RAW_SALES
),

staged AS (
    SELECT
        -- â”€â”€ IDs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        CAST(INVOICENO       AS VARCHAR)    AS invoice_no,
        CAST(STOCKCODE       AS VARCHAR)    AS stock_code,
        CAST(DESCRIPTION     AS VARCHAR)    AS description,

        -- â”€â”€ Quantities & Prices â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        TRY_CAST(QUANTITY    AS INT)        AS quantity,
        TRY_CAST(UNITPRICE   AS FLOAT)      AS unit_price,
        TRY_CAST(DISCOUNT    AS FLOAT)      AS discount,
        TRY_CAST(SHIPPINGCOST AS FLOAT)     AS shipping_cost,
        TRY_CAST(CUSTOMERID  AS FLOAT)      AS customer_id,

        -- â”€â”€ Date Fix: VARCHAR â†’ proper TIMESTAMP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        -- Handles format: '2020-01-01 00:00:00'
        TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS') AS invoice_date,
        YEAR(TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS'))  AS invoice_year,
        MONTH(TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS')) AS invoice_month,

        -- â”€â”€ Categoricals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        TRIM(UPPER(COUNTRY))         AS country,
        TRIM(UPPER(PAYMENTMETHOD))   AS payment_method,
        TRIM(UPPER(CATEGORY))        AS category,
        TRIM(UPPER(SALESCHANNEL))    AS sales_channel,
        TRIM(UPPER(RETURNSTATUS))    AS return_status,
        TRIM(UPPER(SHIPMENTPROVIDER)) AS shipment_provider,
        TRIM(UPPER(WAREHOUSELOCATION)) AS warehouse_location,
        TRIM(UPPER(ORDERPRIORITY))   AS order_priority,

        -- â”€â”€ Audit columns â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        CURRENT_TIMESTAMP()          AS stg_loaded_at

    FROM raw_data
),

-- â”€â”€ Data Quality Flags â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
validated AS (
    SELECT
        *,
        CASE
            WHEN invoice_date IS NULL           THEN 'INVALID_DATE'
            WHEN quantity IS NULL               THEN 'INVALID_QUANTITY'
            WHEN unit_price IS NULL             THEN 'INVALID_PRICE'
            WHEN shipping_cost < 0              THEN 'NEGATIVE_SHIPPING'
            WHEN discount < 0 OR discount > 1   THEN 'INVALID_DISCOUNT'
            WHEN invoice_no IS NULL             THEN 'NULL_INVOICE'
            ELSE 'VALID'
        END AS data_quality_flag

    FROM staged
)

SELECT * FROM validated


    -- â”€â”€ Incremental: only process new months not already in staging â”€â”€
    WHERE (invoice_year, invoice_month) NOT IN (
        SELECT DISTINCT invoice_year, invoice_month
        FROM INCREMENTALETL._staging.stg_sales
    )

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:22:38.838964 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m10:22:39.670361 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.840 seconds
[0m10:22:39.715930 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:22:39.715930 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:22:39.890542 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.170 seconds
[0m10:22:39.906362 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:22:39.906362 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:22:40.035699 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.133 seconds
[0m10:22:40.099357 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:22:40.099357 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:22:40.227532 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.138 seconds
[0m10:22:40.354565 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:22:40.354565 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table "INCREMENTALETL"."_STAGING"."STG_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:22:40.487114 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.131 seconds
[0m10:22:40.529536 [debug] [Thread-2 (]: 
    In "INCREMENTALETL"."_STAGING"."STG_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m10:22:40.561382 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.stg_sales"
[0m10:22:40.577166 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:22:40.577166 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:22:40.724040 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.147 seconds
[0m10:22:40.724040 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:22:40.724040 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: merge into INCREMENTALETL._staging.stg_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._staging.stg_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on ((DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no))

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","DATA_QUALITY_FLAG" = DBT_INTERNAL_SOURCE."DATA_QUALITY_FLAG"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "DATA_QUALITY_FLAG")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "DATA_QUALITY_FLAG")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:22:41.517460 [debug] [Thread-2 (]: SQL status: SUCCESS 744 in 0.793 seconds
[0m10:22:41.517460 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:22:41.517460 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:22:41.755668 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.242 seconds
[0m10:22:41.804404 [debug] [Thread-2 (]: Applying DROP to: INCREMENTALETL._staging.stg_sales__dbt_tmp
[0m10:22:41.812474 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:22:41.812474 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: drop view if exists INCREMENTALETL._staging.stg_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:22:41.978043 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.159 seconds
[0m10:22:42.036458 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '85ef9ac8-6b2c-4f3a-83eb-fadc531490d9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021F81CF8EE0>]}
[0m10:22:42.036458 [info ] [Thread-2 (]: 1 of 2 OK created sql incremental model _staging.stg_sales ..................... [[32mSUCCESS 744[0m in 3.32s]
[0m10:22:42.036458 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.stg_sales
[0m10:22:42.036458 [debug] [Thread-4 (]: Began running node model.sales_pipelines.processed_sales
[0m10:22:42.036458 [info ] [Thread-4 (]: 2 of 2 START sql incremental model _processed.processed_sales .................. [RUN]
[0m10:22:42.043006 [debug] [Thread-4 (]: Acquiring new snowflake connection 'model.sales_pipelines.processed_sales'
[0m10:22:42.043006 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.processed_sales
[0m10:22:42.043006 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.processed_sales"
[0m10:22:42.052202 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.processed_sales
[0m10:22:42.052202 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:22:42.052202 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: create or replace  temporary view INCREMENTALETL._processed.processed_sales__dbt_tmp
  
  
  
  
  as (
    

WITH staging AS (
    SELECT * FROM INCREMENTALETL._staging.stg_sales
),

-- â”€â”€ Step 1: Remove duplicates (keep latest loaded record) â”€â”€â”€â”€â”€â”€â”€â”€
deduped AS (
    SELECT *,
        ROW_NUMBER() OVER (
            PARTITION BY invoice_no
            ORDER BY stg_loaded_at DESC
        ) AS row_num
    FROM staging
),

-- â”€â”€ Step 2: Keep only VALID records, remove bad data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cleaned AS (
    SELECT
        invoice_no,
        stock_code,
        description,
        quantity,
        unit_price,
        discount,
        shipping_cost,
        customer_id,
        invoice_date,
        invoice_year,
        invoice_month,
        country,
        payment_method,
        category,
        sales_channel,
        return_status,
        shipment_provider,
        warehouse_location,
        order_priority,

        -- â”€â”€ Derived / Enriched columns â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        ROUND(quantity * unit_price, 2)                        AS gross_amount,
        ROUND(quantity * unit_price * (1 - discount), 2)       AS net_amount,
        ROUND(quantity * unit_price * (1 - discount)
              + COALESCE(shipping_cost, 0), 2)                 AS total_amount,

        CASE
            WHEN return_status = 'RETURNED' THEN TRUE
            ELSE FALSE
        END AS is_returned,

        CASE
            WHEN customer_id IS NULL THEN TRUE
            ELSE FALSE
        END AS is_guest_customer,

        -- â”€â”€ Audit â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        stg_loaded_at,
        CURRENT_TIMESTAMP() AS processed_at

    FROM deduped
    WHERE
        row_num = 1                     -- remove duplicates
        AND data_quality_flag = 'VALID' -- only clean records
        AND quantity > 0                -- remove returns/negatives
        AND unit_price > 0              -- remove invalid prices
        AND invoice_date IS NOT NULL    -- must have valid date
),

-- â”€â”€ Step 3: Final output â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
final AS (
    SELECT * FROM cleaned
)

SELECT * FROM final


    -- â”€â”€ Incremental: only process months not already in processed â”€â”€
    WHERE (invoice_year, invoice_month) NOT IN (
        SELECT DISTINCT invoice_year, invoice_month
        FROM INCREMENTALETL._processed.processed_sales
    )

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:22:42.052202 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m10:22:42.846989 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.790 seconds
[0m10:22:42.857755 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:22:42.857755 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:22:43.035169 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.164 seconds
[0m10:22:43.057273 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:22:43.057273 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:22:43.193245 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.128 seconds
[0m10:22:43.196325 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:22:43.196325 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:22:43.357645 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.154 seconds
[0m10:22:43.373550 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:22:43.389354 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:22:43.534207 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.141 seconds
[0m10:22:43.550154 [debug] [Thread-4 (]: 
    In "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m10:22:43.566025 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.processed_sales"
[0m10:22:43.574865 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:22:43.574865 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:22:43.727346 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.161 seconds
[0m10:22:43.727346 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:22:43.743069 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: merge into INCREMENTALETL._processed.processed_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._processed.processed_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on ((DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no))

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","GROSS_AMOUNT" = DBT_INTERNAL_SOURCE."GROSS_AMOUNT","NET_AMOUNT" = DBT_INTERNAL_SOURCE."NET_AMOUNT","TOTAL_AMOUNT" = DBT_INTERNAL_SOURCE."TOTAL_AMOUNT","IS_RETURNED" = DBT_INTERNAL_SOURCE."IS_RETURNED","IS_GUEST_CUSTOMER" = DBT_INTERNAL_SOURCE."IS_GUEST_CUSTOMER","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","PROCESSED_AT" = DBT_INTERNAL_SOURCE."PROCESSED_AT"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:22:44.599649 [debug] [Thread-4 (]: SQL status: SUCCESS 708 in 0.852 seconds
[0m10:22:44.600673 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:22:44.602683 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:22:44.893823 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.299 seconds
[0m10:22:44.922475 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._processed.processed_sales__dbt_tmp
[0m10:22:44.922475 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:22:44.922475 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: drop view if exists INCREMENTALETL._processed.processed_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:22:45.084280 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.164 seconds
[0m10:22:45.100267 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '85ef9ac8-6b2c-4f3a-83eb-fadc531490d9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021F9438A080>]}
[0m10:22:45.100267 [info ] [Thread-4 (]: 2 of 2 OK created sql incremental model _processed.processed_sales ............. [[32mSUCCESS 708[0m in 3.06s]
[0m10:22:45.114576 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.processed_sales
[0m10:22:45.121388 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:22:45.121388 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m10:22:45.121388 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m10:22:45.357852 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m10:22:45.357852 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m10:22:45.953783 [debug] [MainThread]: Connection 'list_INCREMENTALETL__staging' was left open.
[0m10:22:45.953783 [debug] [MainThread]: On list_INCREMENTALETL__staging: Close
[0m10:22:46.339730 [debug] [MainThread]: Connection 'list_INCREMENTALETL__processed' was left open.
[0m10:22:46.342779 [debug] [MainThread]: On list_INCREMENTALETL__processed: Close
[0m10:22:46.644660 [debug] [MainThread]: Connection 'model.sales_pipelines.stg_sales' was left open.
[0m10:22:46.646427 [debug] [MainThread]: On model.sales_pipelines.stg_sales: Close
[0m10:22:46.885187 [debug] [MainThread]: Connection 'model.sales_pipelines.processed_sales' was left open.
[0m10:22:46.885187 [debug] [MainThread]: On model.sales_pipelines.processed_sales: Close
[0m10:22:47.125938 [info ] [MainThread]: 
[0m10:22:47.127952 [info ] [MainThread]: Finished running 2 incremental models in 0 hours 0 minutes and 11.13 seconds (11.13s).
[0m10:22:47.127952 [debug] [MainThread]: Command end result
[0m10:22:47.197207 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m10:22:47.199215 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m10:22:47.213028 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\snowflake-incremental-pipeline\dbt_project\target\run_results.json
[0m10:22:47.213028 [info ] [MainThread]: 
[0m10:22:47.213028 [info ] [MainThread]: [32mCompleted successfully[0m
[0m10:22:47.213028 [info ] [MainThread]: 
[0m10:22:47.218932 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=2
[0m10:22:47.218932 [debug] [MainThread]: Command `dbt run` succeeded at 10:22:47.218932 after 13.72 seconds
[0m10:22:47.222145 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021FFF897640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021F943AD1E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021F9431B5B0>]}
[0m10:22:47.224845 [debug] [MainThread]: Flushing usage events
[0m10:22:48.503615 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:23:47.469714 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022D247876D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022D26A02620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022D26A030D0>]}


============================== 10:23:47.469714 | 55eb9576-2ff6-4634-af24-bd8e35a4dc4a ==============================
[0m10:23:47.469714 [info ] [MainThread]: Running with dbt=1.11.6
[0m10:23:47.469714 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'profiles_dir': 'D:\\snowflake-incremental-pipeline\\dbt_project', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'indirect_selection': 'eager', 'use_colors': 'True', 'invocation_command': 'dbt run', 'use_experimental_parser': 'False', 'log_path': 'D:\\snowflake-incremental-pipeline\\dbt_project\\logs'}
[0m10:23:48.490074 [debug] [MainThread]: Snowflake adapter: Setting snowflake.connector to ERROR (file logging only)
[0m10:23:48.490074 [debug] [MainThread]: Snowflake adapter: Setting botocore to ERROR (file logging only)
[0m10:23:48.492080 [debug] [MainThread]: Snowflake adapter: Setting boto3 to ERROR (file logging only)
[0m10:23:48.871044 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '55eb9576-2ff6-4634-af24-bd8e35a4dc4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022D37A0E8C0>]}
[0m10:23:48.961825 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '55eb9576-2ff6-4634-af24-bd8e35a4dc4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022D26A97D60>]}
[0m10:23:48.963833 [info ] [MainThread]: Registered adapter: snowflake=1.11.2
[0m10:23:49.494167 [debug] [MainThread]: checksum: ec27062d18352e9cfca161c166fcc73d2201661a23a3cf3c89196a27f32cfcb8, vars: {}, profile: , target: , version: 1.11.6
[0m10:23:49.735371 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m10:23:49.735371 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m10:23:49.737377 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m10:23:49.814326 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '55eb9576-2ff6-4634-af24-bd8e35a4dc4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022D390FC130>]}
[0m10:23:49.962449 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m10:23:49.966462 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m10:23:49.988032 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '55eb9576-2ff6-4634-af24-bd8e35a4dc4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022D3909CA60>]}
[0m10:23:49.990041 [info ] [MainThread]: Found 2 models, 6 data tests, 1 source, 522 macros
[0m10:23:49.990041 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '55eb9576-2ff6-4634-af24-bd8e35a4dc4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022D3909CB20>]}
[0m10:23:49.990041 [info ] [MainThread]: 
[0m10:23:49.990041 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m10:23:49.990041 [info ] [MainThread]: 
[0m10:23:49.990041 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m10:23:49.999719 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m10:23:49.999719 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m10:23:50.111393 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m10:23:50.111393 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m10:23:50.111393 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m10:23:50.111393 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m10:23:50.111393 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:23:50.111393 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:23:52.039636 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 1.924 seconds
[0m10:23:52.045323 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 1.931 seconds
[0m10:23:52.049501 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__staging'
[0m10:23:52.057498 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__processed'
[0m10:23:52.079925 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__processed"
[0m10:23:52.084413 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__staging"
[0m10:23:52.084413 [debug] [ThreadPool]: On list_INCREMENTALETL__processed: show objects in INCREMENTALETL._processed
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__processed"} */;
[0m10:23:52.085643 [debug] [ThreadPool]: On list_INCREMENTALETL__staging: show objects in INCREMENTALETL._staging
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__staging"} */;
[0m10:23:52.087471 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:23:52.087471 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:23:52.788191 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.714 seconds
[0m10:23:52.899039 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.818 seconds
[0m10:23:52.911021 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '55eb9576-2ff6-4634-af24-bd8e35a4dc4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022D37A0D4B0>]}
[0m10:23:52.917774 [debug] [Thread-2 (]: Began running node model.sales_pipelines.stg_sales
[0m10:23:52.928419 [info ] [Thread-2 (]: 1 of 2 START sql incremental model _staging.stg_sales .......................... [RUN]
[0m10:23:52.932059 [debug] [Thread-2 (]: Acquiring new snowflake connection 'model.sales_pipelines.stg_sales'
[0m10:23:52.933551 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.stg_sales
[0m10:23:52.962781 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.stg_sales"
[0m10:23:52.962781 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.stg_sales
[0m10:23:53.057764 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:23:53.057764 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: create or replace  temporary view INCREMENTALETL._staging.stg_sales__dbt_tmp
  
  
  
  
  as (
    

WITH raw_data AS (
    SELECT * FROM INCREMENTALETL.LANDINGZONE.RAW_SALES
),

staged AS (
    SELECT
        -- â”€â”€ IDs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        CAST(INVOICENO       AS VARCHAR)    AS invoice_no,
        CAST(STOCKCODE       AS VARCHAR)    AS stock_code,
        CAST(DESCRIPTION     AS VARCHAR)    AS description,

        -- â”€â”€ Quantities & Prices â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        TRY_CAST(QUANTITY    AS INT)        AS quantity,
        TRY_CAST(UNITPRICE   AS FLOAT)      AS unit_price,
        TRY_CAST(DISCOUNT    AS FLOAT)      AS discount,
        TRY_CAST(SHIPPINGCOST AS FLOAT)     AS shipping_cost,
        TRY_CAST(CUSTOMERID  AS FLOAT)      AS customer_id,

        -- â”€â”€ Date Fix: VARCHAR â†’ proper TIMESTAMP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        -- Handles format: '2020-01-01 00:00:00'
        TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS') AS invoice_date,
        YEAR(TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS'))  AS invoice_year,
        MONTH(TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS')) AS invoice_month,

        -- â”€â”€ Categoricals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        TRIM(UPPER(COUNTRY))         AS country,
        TRIM(UPPER(PAYMENTMETHOD))   AS payment_method,
        TRIM(UPPER(CATEGORY))        AS category,
        TRIM(UPPER(SALESCHANNEL))    AS sales_channel,
        TRIM(UPPER(RETURNSTATUS))    AS return_status,
        TRIM(UPPER(SHIPMENTPROVIDER)) AS shipment_provider,
        TRIM(UPPER(WAREHOUSELOCATION)) AS warehouse_location,
        TRIM(UPPER(ORDERPRIORITY))   AS order_priority,

        -- â”€â”€ Audit columns â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        CURRENT_TIMESTAMP()          AS stg_loaded_at

    FROM raw_data
),

-- â”€â”€ Data Quality Flags â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
validated AS (
    SELECT
        *,
        CASE
            WHEN invoice_date IS NULL           THEN 'INVALID_DATE'
            WHEN quantity IS NULL               THEN 'INVALID_QUANTITY'
            WHEN unit_price IS NULL             THEN 'INVALID_PRICE'
            WHEN shipping_cost < 0              THEN 'NEGATIVE_SHIPPING'
            WHEN discount < 0 OR discount > 1   THEN 'INVALID_DISCOUNT'
            WHEN invoice_no IS NULL             THEN 'NULL_INVOICE'
            ELSE 'VALID'
        END AS data_quality_flag

    FROM staged
)

SELECT * FROM validated


    -- â”€â”€ Incremental: only process new months not already in staging â”€â”€
    WHERE (invoice_year, invoice_month) NOT IN (
        SELECT DISTINCT invoice_year, invoice_month
        FROM INCREMENTALETL._staging.stg_sales
    )

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:23:53.057764 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m10:23:57.706041 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 4.647 seconds
[0m10:23:57.721941 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:23:57.721941 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:23:57.916282 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.179 seconds
[0m10:23:57.942368 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:23:57.944386 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:23:58.137731 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.205 seconds
[0m10:23:58.185718 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:23:58.185718 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:23:58.378352 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.179 seconds
[0m10:23:58.489662 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:23:58.489662 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table "INCREMENTALETL"."_STAGING"."STG_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:23:58.731588 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.237 seconds
[0m10:23:58.763485 [debug] [Thread-2 (]: 
    In "INCREMENTALETL"."_STAGING"."STG_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m10:23:58.810787 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.stg_sales"
[0m10:23:58.821638 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:23:58.821638 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:23:59.134581 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.325 seconds
[0m10:23:59.150427 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:23:59.150427 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: merge into INCREMENTALETL._staging.stg_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._staging.stg_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on ((DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no))

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","DATA_QUALITY_FLAG" = DBT_INTERNAL_SOURCE."DATA_QUALITY_FLAG"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "DATA_QUALITY_FLAG")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "DATA_QUALITY_FLAG")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:23:59.960058 [debug] [Thread-2 (]: SQL status: SUCCESS 1460 in 0.808 seconds
[0m10:23:59.960058 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:23:59.960058 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:24:00.269318 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.302 seconds
[0m10:24:00.303168 [debug] [Thread-2 (]: Applying DROP to: INCREMENTALETL._staging.stg_sales__dbt_tmp
[0m10:24:00.314974 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:24:00.316983 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: drop view if exists INCREMENTALETL._staging.stg_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:24:00.490669 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.184 seconds
[0m10:24:00.553861 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '55eb9576-2ff6-4634-af24-bd8e35a4dc4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022D37C06950>]}
[0m10:24:00.553861 [info ] [Thread-2 (]: 1 of 2 OK created sql incremental model _staging.stg_sales ..................... [[32mSUCCESS 1460[0m in 7.62s]
[0m10:24:00.553861 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.stg_sales
[0m10:24:00.553861 [debug] [Thread-4 (]: Began running node model.sales_pipelines.processed_sales
[0m10:24:00.553861 [info ] [Thread-4 (]: 2 of 2 START sql incremental model _processed.processed_sales .................. [RUN]
[0m10:24:00.553861 [debug] [Thread-4 (]: Acquiring new snowflake connection 'model.sales_pipelines.processed_sales'
[0m10:24:00.553861 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.processed_sales
[0m10:24:00.570872 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.processed_sales"
[0m10:24:00.570872 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.processed_sales
[0m10:24:00.570872 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:24:00.585449 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: create or replace  temporary view INCREMENTALETL._processed.processed_sales__dbt_tmp
  
  
  
  
  as (
    

WITH staging AS (
    SELECT * FROM INCREMENTALETL._staging.stg_sales
),

-- â”€â”€ Step 1: Remove duplicates (keep latest loaded record) â”€â”€â”€â”€â”€â”€â”€â”€
deduped AS (
    SELECT *,
        ROW_NUMBER() OVER (
            PARTITION BY invoice_no
            ORDER BY stg_loaded_at DESC
        ) AS row_num
    FROM staging
),

-- â”€â”€ Step 2: Keep only VALID records, remove bad data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cleaned AS (
    SELECT
        invoice_no,
        stock_code,
        description,
        quantity,
        unit_price,
        discount,
        shipping_cost,
        customer_id,
        invoice_date,
        invoice_year,
        invoice_month,
        country,
        payment_method,
        category,
        sales_channel,
        return_status,
        shipment_provider,
        warehouse_location,
        order_priority,

        -- â”€â”€ Derived / Enriched columns â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        ROUND(quantity * unit_price, 2)                        AS gross_amount,
        ROUND(quantity * unit_price * (1 - discount), 2)       AS net_amount,
        ROUND(quantity * unit_price * (1 - discount)
              + COALESCE(shipping_cost, 0), 2)                 AS total_amount,

        CASE
            WHEN return_status = 'RETURNED' THEN TRUE
            ELSE FALSE
        END AS is_returned,

        CASE
            WHEN customer_id IS NULL THEN TRUE
            ELSE FALSE
        END AS is_guest_customer,

        -- â”€â”€ Audit â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        stg_loaded_at,
        CURRENT_TIMESTAMP() AS processed_at

    FROM deduped
    WHERE
        row_num = 1                     -- remove duplicates
        AND data_quality_flag = 'VALID' -- only clean records
        AND quantity > 0                -- remove returns/negatives
        AND unit_price > 0              -- remove invalid prices
        AND invoice_date IS NOT NULL    -- must have valid date
),

-- â”€â”€ Step 3: Final output â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
final AS (
    SELECT * FROM cleaned
)

SELECT * FROM final


    -- â”€â”€ Incremental: only process months not already in processed â”€â”€
    WHERE (invoice_year, invoice_month) NOT IN (
        SELECT DISTINCT invoice_year, invoice_month
        FROM INCREMENTALETL._processed.processed_sales
    )

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:24:00.585449 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m10:24:01.584264 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 1.011 seconds
[0m10:24:01.600441 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:24:01.600441 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:24:01.974176 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.372 seconds
[0m10:24:01.984978 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:24:01.984978 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:24:02.211217 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.217 seconds
[0m10:24:02.227315 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:24:02.227315 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:24:02.425164 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.189 seconds
[0m10:24:02.453092 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:24:02.453092 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:24:02.613054 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.163 seconds
[0m10:24:02.628895 [debug] [Thread-4 (]: 
    In "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m10:24:02.644751 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.processed_sales"
[0m10:24:02.644751 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:24:02.644751 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:24:02.830455 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.173 seconds
[0m10:24:02.832478 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:24:02.836273 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: merge into INCREMENTALETL._processed.processed_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._processed.processed_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on ((DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no))

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","GROSS_AMOUNT" = DBT_INTERNAL_SOURCE."GROSS_AMOUNT","NET_AMOUNT" = DBT_INTERNAL_SOURCE."NET_AMOUNT","TOTAL_AMOUNT" = DBT_INTERNAL_SOURCE."TOTAL_AMOUNT","IS_RETURNED" = DBT_INTERNAL_SOURCE."IS_RETURNED","IS_GUEST_CUSTOMER" = DBT_INTERNAL_SOURCE."IS_GUEST_CUSTOMER","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","PROCESSED_AT" = DBT_INTERNAL_SOURCE."PROCESSED_AT"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:24:03.697707 [debug] [Thread-4 (]: SQL status: SUCCESS 1387 in 0.863 seconds
[0m10:24:03.697707 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:24:03.697707 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:24:04.007575 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.311 seconds
[0m10:24:04.023382 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._processed.processed_sales__dbt_tmp
[0m10:24:04.039565 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:24:04.039565 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: drop view if exists INCREMENTALETL._processed.processed_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:24:04.258063 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.220 seconds
[0m10:24:04.271738 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '55eb9576-2ff6-4634-af24-bd8e35a4dc4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022D26858F70>]}
[0m10:24:04.271738 [info ] [Thread-4 (]: 2 of 2 OK created sql incremental model _processed.processed_sales ............. [[32mSUCCESS 1387[0m in 3.72s]
[0m10:24:04.271738 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.processed_sales
[0m10:24:04.277489 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:24:04.278777 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m10:24:04.278777 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m10:24:04.566926 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m10:24:04.579289 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m10:24:04.868698 [debug] [MainThread]: Connection 'list_INCREMENTALETL__staging' was left open.
[0m10:24:04.884417 [debug] [MainThread]: On list_INCREMENTALETL__staging: Close
[0m10:24:05.127208 [debug] [MainThread]: Connection 'list_INCREMENTALETL__processed' was left open.
[0m10:24:05.127208 [debug] [MainThread]: On list_INCREMENTALETL__processed: Close
[0m10:24:05.406073 [debug] [MainThread]: Connection 'model.sales_pipelines.stg_sales' was left open.
[0m10:24:05.408097 [debug] [MainThread]: On model.sales_pipelines.stg_sales: Close
[0m10:24:05.727208 [debug] [MainThread]: Connection 'model.sales_pipelines.processed_sales' was left open.
[0m10:24:05.729233 [debug] [MainThread]: On model.sales_pipelines.processed_sales: Close
[0m10:24:06.016642 [info ] [MainThread]: 
[0m10:24:06.016642 [info ] [MainThread]: Finished running 2 incremental models in 0 hours 0 minutes and 16.03 seconds (16.03s).
[0m10:24:06.027791 [debug] [MainThread]: Command end result
[0m10:24:06.095773 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m10:24:06.095773 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m10:24:06.111423 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\snowflake-incremental-pipeline\dbt_project\target\run_results.json
[0m10:24:06.111423 [info ] [MainThread]: 
[0m10:24:06.111423 [info ] [MainThread]: [32mCompleted successfully[0m
[0m10:24:06.111423 [info ] [MainThread]: 
[0m10:24:06.111423 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=2
[0m10:24:06.111423 [debug] [MainThread]: Command `dbt run` succeeded at 10:24:06.111423 after 18.79 seconds
[0m10:24:06.111423 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022D247876D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022D39149360>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022D391496F0>]}
[0m10:24:06.111423 [debug] [MainThread]: Flushing usage events
[0m10:24:07.627483 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:25:15.577907 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E819FD75E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E81B266EF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E81B2677F0>]}


============================== 10:25:15.577907 | 7507418e-2fab-4670-b7c9-98b3cb95f352 ==============================
[0m10:25:15.577907 [info ] [MainThread]: Running with dbt=1.11.6
[0m10:25:15.577907 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'profiles_dir': 'D:\\snowflake-incremental-pipeline\\dbt_project', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'invocation_command': 'dbt run', 'indirect_selection': 'eager', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'D:\\snowflake-incremental-pipeline\\dbt_project\\logs'}
[0m10:25:17.059099 [debug] [MainThread]: Snowflake adapter: Setting snowflake.connector to ERROR (file logging only)
[0m10:25:17.059099 [debug] [MainThread]: Snowflake adapter: Setting botocore to ERROR (file logging only)
[0m10:25:17.059099 [debug] [MainThread]: Snowflake adapter: Setting boto3 to ERROR (file logging only)
[0m10:25:17.797757 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7507418e-2fab-4670-b7c9-98b3cb95f352', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E81B0EA6E0>]}
[0m10:25:18.080370 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7507418e-2fab-4670-b7c9-98b3cb95f352', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E82C26BCA0>]}
[0m10:25:18.080370 [info ] [MainThread]: Registered adapter: snowflake=1.11.2
[0m10:25:19.346625 [debug] [MainThread]: checksum: ec27062d18352e9cfca161c166fcc73d2201661a23a3cf3c89196a27f32cfcb8, vars: {}, profile: , target: , version: 1.11.6
[0m10:25:19.873894 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m10:25:19.873894 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m10:25:19.873894 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m10:25:20.047270 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7507418e-2fab-4670-b7c9-98b3cb95f352', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E82D990130>]}
[0m10:25:20.406669 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m10:25:20.421864 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m10:25:20.473927 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7507418e-2fab-4670-b7c9-98b3cb95f352', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E82D92E440>]}
[0m10:25:20.473927 [info ] [MainThread]: Found 2 models, 6 data tests, 1 source, 522 macros
[0m10:25:20.478987 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7507418e-2fab-4670-b7c9-98b3cb95f352', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E82D92FF10>]}
[0m10:25:20.480624 [info ] [MainThread]: 
[0m10:25:20.485233 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m10:25:20.489501 [info ] [MainThread]: 
[0m10:25:20.489501 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m10:25:20.514275 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m10:25:20.523285 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m10:25:20.723100 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m10:25:20.723100 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m10:25:20.737165 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m10:25:20.737165 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m10:25:20.737165 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:25:20.739758 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:25:21.896692 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 1.164 seconds
[0m10:25:21.907664 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 1.169 seconds
[0m10:25:21.924581 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__processed'
[0m10:25:21.924581 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__staging'
[0m10:25:21.970518 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__staging"
[0m10:25:21.970518 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__processed"
[0m10:25:21.970518 [debug] [ThreadPool]: On list_INCREMENTALETL__staging: show objects in INCREMENTALETL._staging
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__staging"} */;
[0m10:25:21.986423 [debug] [ThreadPool]: On list_INCREMENTALETL__processed: show objects in INCREMENTALETL._processed
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__processed"} */;
[0m10:25:21.987538 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:25:21.987538 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:25:22.739139 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.754 seconds
[0m10:25:22.747454 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.759 seconds
[0m10:25:22.747454 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7507418e-2fab-4670-b7c9-98b3cb95f352', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E817E56E60>]}
[0m10:25:22.763574 [debug] [Thread-2 (]: Began running node model.sales_pipelines.stg_sales
[0m10:25:22.779323 [info ] [Thread-2 (]: 1 of 2 START sql incremental model _staging.stg_sales .......................... [RUN]
[0m10:25:22.779323 [debug] [Thread-2 (]: Acquiring new snowflake connection 'model.sales_pipelines.stg_sales'
[0m10:25:22.779323 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.stg_sales
[0m10:25:22.826731 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.stg_sales"
[0m10:25:22.826731 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.stg_sales
[0m10:25:22.906157 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:25:22.907484 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: create or replace  temporary view INCREMENTALETL._staging.stg_sales__dbt_tmp
  
  
  
  
  as (
    

WITH raw_data AS (
    SELECT * FROM INCREMENTALETL.LANDINGZONE.RAW_SALES
),

staged AS (
    SELECT
        -- â”€â”€ IDs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        CAST(INVOICENO       AS VARCHAR)    AS invoice_no,
        CAST(STOCKCODE       AS VARCHAR)    AS stock_code,
        CAST(DESCRIPTION     AS VARCHAR)    AS description,

        -- â”€â”€ Quantities & Prices â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        TRY_CAST(QUANTITY    AS INT)        AS quantity,
        TRY_CAST(UNITPRICE   AS FLOAT)      AS unit_price,
        TRY_CAST(DISCOUNT    AS FLOAT)      AS discount,
        TRY_CAST(SHIPPINGCOST AS FLOAT)     AS shipping_cost,
        TRY_CAST(CUSTOMERID  AS FLOAT)      AS customer_id,

        -- â”€â”€ Date Fix: VARCHAR â†’ proper TIMESTAMP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        -- Handles format: '2020-01-01 00:00:00'
        TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS') AS invoice_date,
        YEAR(TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS'))  AS invoice_year,
        MONTH(TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS')) AS invoice_month,

        -- â”€â”€ Categoricals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        TRIM(UPPER(COUNTRY))         AS country,
        TRIM(UPPER(PAYMENTMETHOD))   AS payment_method,
        TRIM(UPPER(CATEGORY))        AS category,
        TRIM(UPPER(SALESCHANNEL))    AS sales_channel,
        TRIM(UPPER(RETURNSTATUS))    AS return_status,
        TRIM(UPPER(SHIPMENTPROVIDER)) AS shipment_provider,
        TRIM(UPPER(WAREHOUSELOCATION)) AS warehouse_location,
        TRIM(UPPER(ORDERPRIORITY))   AS order_priority,

        -- â”€â”€ Audit columns â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        CURRENT_TIMESTAMP()          AS stg_loaded_at

    FROM raw_data
),

-- â”€â”€ Data Quality Flags â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
validated AS (
    SELECT
        *,
        CASE
            WHEN invoice_date IS NULL           THEN 'INVALID_DATE'
            WHEN quantity IS NULL               THEN 'INVALID_QUANTITY'
            WHEN unit_price IS NULL             THEN 'INVALID_PRICE'
            WHEN shipping_cost < 0              THEN 'NEGATIVE_SHIPPING'
            WHEN discount < 0 OR discount > 1   THEN 'INVALID_DISCOUNT'
            WHEN invoice_no IS NULL             THEN 'NULL_INVOICE'
            ELSE 'VALID'
        END AS data_quality_flag

    FROM staged
)

SELECT * FROM validated


    -- â”€â”€ Incremental: only process new months not already in staging â”€â”€
    WHERE (invoice_year, invoice_month) NOT IN (
        SELECT DISTINCT invoice_year, invoice_month
        FROM INCREMENTALETL._staging.stg_sales
    )

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:25:22.909025 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m10:25:23.713243 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.811 seconds
[0m10:25:23.745067 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:25:23.745067 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:25:23.904805 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.149 seconds
[0m10:25:23.926033 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:25:23.926033 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:25:24.067036 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.135 seconds
[0m10:25:24.116411 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:25:24.116411 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:25:24.247624 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.131 seconds
[0m10:25:24.362847 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:25:24.362847 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table "INCREMENTALETL"."_STAGING"."STG_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:25:24.492510 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.131 seconds
[0m10:25:24.508282 [debug] [Thread-2 (]: 
    In "INCREMENTALETL"."_STAGING"."STG_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m10:25:24.556162 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.stg_sales"
[0m10:25:24.556162 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:25:24.556162 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:25:24.716888 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.155 seconds
[0m10:25:24.716888 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:25:24.716888 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: merge into INCREMENTALETL._staging.stg_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._staging.stg_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on ((DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no))

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","DATA_QUALITY_FLAG" = DBT_INTERNAL_SOURCE."DATA_QUALITY_FLAG"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "DATA_QUALITY_FLAG")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "DATA_QUALITY_FLAG")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:25:25.660450 [debug] [Thread-2 (]: SQL status: SUCCESS 2914 in 0.931 seconds
[0m10:25:25.660450 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:25:25.660450 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:25:25.922648 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.256 seconds
[0m10:25:25.950499 [debug] [Thread-2 (]: Applying DROP to: INCREMENTALETL._staging.stg_sales__dbt_tmp
[0m10:25:25.966252 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:25:25.966252 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: drop view if exists INCREMENTALETL._staging.stg_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:25:26.129305 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.156 seconds
[0m10:25:26.184959 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7507418e-2fab-4670-b7c9-98b3cb95f352', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E818E57970>]}
[0m10:25:26.186968 [info ] [Thread-2 (]: 1 of 2 OK created sql incremental model _staging.stg_sales ..................... [[32mSUCCESS 2914[0m in 3.40s]
[0m10:25:26.186968 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.stg_sales
[0m10:25:26.186968 [debug] [Thread-4 (]: Began running node model.sales_pipelines.processed_sales
[0m10:25:26.191349 [info ] [Thread-4 (]: 2 of 2 START sql incremental model _processed.processed_sales .................. [RUN]
[0m10:25:26.191349 [debug] [Thread-4 (]: Acquiring new snowflake connection 'model.sales_pipelines.processed_sales'
[0m10:25:26.191349 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.processed_sales
[0m10:25:26.191349 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.processed_sales"
[0m10:25:26.191349 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.processed_sales
[0m10:25:26.206558 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:25:26.206558 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: create or replace  temporary view INCREMENTALETL._processed.processed_sales__dbt_tmp
  
  
  
  
  as (
    

WITH staging AS (
    SELECT * FROM INCREMENTALETL._staging.stg_sales
),

-- â”€â”€ Step 1: Remove duplicates (keep latest loaded record) â”€â”€â”€â”€â”€â”€â”€â”€
deduped AS (
    SELECT *,
        ROW_NUMBER() OVER (
            PARTITION BY invoice_no
            ORDER BY stg_loaded_at DESC
        ) AS row_num
    FROM staging
),

-- â”€â”€ Step 2: Keep only VALID records, remove bad data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cleaned AS (
    SELECT
        invoice_no,
        stock_code,
        description,
        quantity,
        unit_price,
        discount,
        shipping_cost,
        customer_id,
        invoice_date,
        invoice_year,
        invoice_month,
        country,
        payment_method,
        category,
        sales_channel,
        return_status,
        shipment_provider,
        warehouse_location,
        order_priority,

        -- â”€â”€ Derived / Enriched columns â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        ROUND(quantity * unit_price, 2)                        AS gross_amount,
        ROUND(quantity * unit_price * (1 - discount), 2)       AS net_amount,
        ROUND(quantity * unit_price * (1 - discount)
              + COALESCE(shipping_cost, 0), 2)                 AS total_amount,

        CASE
            WHEN return_status = 'RETURNED' THEN TRUE
            ELSE FALSE
        END AS is_returned,

        CASE
            WHEN customer_id IS NULL THEN TRUE
            ELSE FALSE
        END AS is_guest_customer,

        -- â”€â”€ Audit â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        stg_loaded_at,
        CURRENT_TIMESTAMP() AS processed_at

    FROM deduped
    WHERE
        row_num = 1                     -- remove duplicates
        AND data_quality_flag = 'VALID' -- only clean records
        AND quantity > 0                -- remove returns/negatives
        AND unit_price > 0              -- remove invalid prices
        AND invoice_date IS NOT NULL    -- must have valid date
),

-- â”€â”€ Step 3: Final output â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
final AS (
    SELECT * FROM cleaned
)

SELECT * FROM final


    -- â”€â”€ Incremental: only process months not already in processed â”€â”€
    WHERE (invoice_year, invoice_month) NOT IN (
        SELECT DISTINCT invoice_year, invoice_month
        FROM INCREMENTALETL._processed.processed_sales
    )

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:25:26.206558 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m10:25:27.409594 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 1.194 seconds
[0m10:25:27.427107 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:25:27.427107 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:25:27.587473 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.156 seconds
[0m10:25:27.603230 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:25:27.603230 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:25:27.746954 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.130 seconds
[0m10:25:27.762835 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:25:27.762835 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:25:27.923490 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.155 seconds
[0m10:25:27.923490 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:25:27.939220 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:25:28.080141 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.141 seconds
[0m10:25:28.116887 [debug] [Thread-4 (]: 
    In "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m10:25:28.125773 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.processed_sales"
[0m10:25:28.136873 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:25:28.136873 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:25:28.295201 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.161 seconds
[0m10:25:28.295201 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:25:28.295201 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: merge into INCREMENTALETL._processed.processed_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._processed.processed_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on ((DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no))

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","GROSS_AMOUNT" = DBT_INTERNAL_SOURCE."GROSS_AMOUNT","NET_AMOUNT" = DBT_INTERNAL_SOURCE."NET_AMOUNT","TOTAL_AMOUNT" = DBT_INTERNAL_SOURCE."TOTAL_AMOUNT","IS_RETURNED" = DBT_INTERNAL_SOURCE."IS_RETURNED","IS_GUEST_CUSTOMER" = DBT_INTERNAL_SOURCE."IS_GUEST_CUSTOMER","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","PROCESSED_AT" = DBT_INTERNAL_SOURCE."PROCESSED_AT"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:25:29.754062 [debug] [Thread-4 (]: SQL status: SUCCESS 2762 in 1.452 seconds
[0m10:25:29.754062 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:25:29.766185 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:25:30.043018 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.283 seconds
[0m10:25:30.058959 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._processed.processed_sales__dbt_tmp
[0m10:25:30.074680 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:25:30.074680 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: drop view if exists INCREMENTALETL._processed.processed_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:25:30.286539 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.210 seconds
[0m10:25:30.299159 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7507418e-2fab-4670-b7c9-98b3cb95f352', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E82C315C90>]}
[0m10:25:30.299159 [info ] [Thread-4 (]: 2 of 2 OK created sql incremental model _processed.processed_sales ............. [[32mSUCCESS 2762[0m in 4.11s]
[0m10:25:30.299159 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.processed_sales
[0m10:25:30.311941 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:25:30.315054 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m10:25:30.315054 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m10:25:30.556701 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m10:25:30.556701 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m10:25:30.878963 [debug] [MainThread]: Connection 'list_INCREMENTALETL__processed' was left open.
[0m10:25:30.878963 [debug] [MainThread]: On list_INCREMENTALETL__processed: Close
[0m10:25:31.139466 [debug] [MainThread]: Connection 'list_INCREMENTALETL__staging' was left open.
[0m10:25:31.139466 [debug] [MainThread]: On list_INCREMENTALETL__staging: Close
[0m10:25:31.378893 [debug] [MainThread]: Connection 'model.sales_pipelines.stg_sales' was left open.
[0m10:25:31.395027 [debug] [MainThread]: On model.sales_pipelines.stg_sales: Close
[0m10:25:31.635482 [debug] [MainThread]: Connection 'model.sales_pipelines.processed_sales' was left open.
[0m10:25:31.635482 [debug] [MainThread]: On model.sales_pipelines.processed_sales: Close
[0m10:25:31.875501 [info ] [MainThread]: 
[0m10:25:31.891231 [info ] [MainThread]: Finished running 2 incremental models in 0 hours 0 minutes and 11.39 seconds (11.39s).
[0m10:25:31.891231 [debug] [MainThread]: Command end result
[0m10:25:31.954696 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m10:25:31.958714 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m10:25:31.968758 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\snowflake-incremental-pipeline\dbt_project\target\run_results.json
[0m10:25:31.970508 [info ] [MainThread]: 
[0m10:25:31.970508 [info ] [MainThread]: [32mCompleted successfully[0m
[0m10:25:31.970508 [info ] [MainThread]: 
[0m10:25:31.970508 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=2
[0m10:25:31.970508 [debug] [MainThread]: Command `dbt run` succeeded at 10:25:31.970508 after 16.52 seconds
[0m10:25:31.976926 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E819FD75E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E81A348DC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E81A36BDF0>]}
[0m10:25:31.976926 [debug] [MainThread]: Flushing usage events
[0m10:25:33.337511 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:26:49.847845 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FBF04E76D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FBF17A2620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FBF17A30D0>]}


============================== 10:26:49.861224 | c28fb5f3-01ee-4769-bec7-f0697c9edffe ==============================
[0m10:26:49.861224 [info ] [MainThread]: Running with dbt=1.11.6
[0m10:26:49.861224 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'D:\\snowflake-incremental-pipeline\\dbt_project', 'invocation_command': 'dbt run', 'use_colors': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'D:\\snowflake-incremental-pipeline\\dbt_project\\logs'}
[0m10:26:50.834904 [debug] [MainThread]: Snowflake adapter: Setting snowflake.connector to ERROR (file logging only)
[0m10:26:50.834904 [debug] [MainThread]: Snowflake adapter: Setting botocore to ERROR (file logging only)
[0m10:26:50.834904 [debug] [MainThread]: Snowflake adapter: Setting boto3 to ERROR (file logging only)
[0m10:26:51.134982 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c28fb5f3-01ee-4769-bec7-f0697c9edffe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FB82A1E8C0>]}
[0m10:26:51.214529 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c28fb5f3-01ee-4769-bec7-f0697c9edffe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FBF1837D60>]}
[0m10:26:51.214529 [info ] [MainThread]: Registered adapter: snowflake=1.11.2
[0m10:26:51.753077 [debug] [MainThread]: checksum: ec27062d18352e9cfca161c166fcc73d2201661a23a3cf3c89196a27f32cfcb8, vars: {}, profile: , target: , version: 1.11.6
[0m10:26:51.973462 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m10:26:51.973462 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m10:26:51.973462 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m10:26:52.052547 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c28fb5f3-01ee-4769-bec7-f0697c9edffe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FB8410C130>]}
[0m10:26:52.195256 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m10:26:52.195256 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m10:26:52.210947 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c28fb5f3-01ee-4769-bec7-f0697c9edffe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FB840A4A60>]}
[0m10:26:52.210947 [info ] [MainThread]: Found 2 models, 6 data tests, 1 source, 522 macros
[0m10:26:52.210947 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c28fb5f3-01ee-4769-bec7-f0697c9edffe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FB840A4B20>]}
[0m10:26:52.226714 [info ] [MainThread]: 
[0m10:26:52.226714 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m10:26:52.226714 [info ] [MainThread]: 
[0m10:26:52.226714 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m10:26:52.226714 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m10:26:52.242614 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m10:26:52.336866 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m10:26:52.336866 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m10:26:52.336866 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m10:26:52.336866 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m10:26:52.336866 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:26:52.347579 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:26:53.573589 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 1.227 seconds
[0m10:26:53.579207 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 1.239 seconds
[0m10:26:53.595038 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__staging'
[0m10:26:53.598310 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__processed'
[0m10:26:53.642702 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__staging"
[0m10:26:53.654424 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__processed"
[0m10:26:53.656437 [debug] [ThreadPool]: On list_INCREMENTALETL__staging: show objects in INCREMENTALETL._staging
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__staging"} */;
[0m10:26:53.658466 [debug] [ThreadPool]: On list_INCREMENTALETL__processed: show objects in INCREMENTALETL._processed
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__processed"} */;
[0m10:26:53.660484 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:26:53.662263 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:26:54.522823 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.862 seconds
[0m10:26:54.547409 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.887 seconds
[0m10:26:54.548851 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c28fb5f3-01ee-4769-bec7-f0697c9edffe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FB841594B0>]}
[0m10:26:54.563569 [debug] [Thread-2 (]: Began running node model.sales_pipelines.stg_sales
[0m10:26:54.579354 [info ] [Thread-2 (]: 1 of 2 START sql incremental model _staging.stg_sales .......................... [RUN]
[0m10:26:54.583391 [debug] [Thread-2 (]: Acquiring new snowflake connection 'model.sales_pipelines.stg_sales'
[0m10:26:54.585408 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.stg_sales
[0m10:26:54.626813 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.stg_sales"
[0m10:26:54.626813 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.stg_sales
[0m10:26:54.691772 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:26:54.706325 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: create or replace  temporary view INCREMENTALETL._staging.stg_sales__dbt_tmp
  
  
  
  
  as (
    

WITH raw_data AS (
    SELECT * FROM INCREMENTALETL.LANDINGZONE.RAW_SALES
),

staged AS (
    SELECT
        -- â”€â”€ IDs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        CAST(INVOICENO       AS VARCHAR)    AS invoice_no,
        CAST(STOCKCODE       AS VARCHAR)    AS stock_code,
        CAST(DESCRIPTION     AS VARCHAR)    AS description,

        -- â”€â”€ Quantities & Prices â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        TRY_CAST(QUANTITY    AS INT)        AS quantity,
        TRY_CAST(UNITPRICE   AS FLOAT)      AS unit_price,
        TRY_CAST(DISCOUNT    AS FLOAT)      AS discount,
        TRY_CAST(SHIPPINGCOST AS FLOAT)     AS shipping_cost,
        TRY_CAST(CUSTOMERID  AS FLOAT)      AS customer_id,

        -- â”€â”€ Date Fix: VARCHAR â†’ proper TIMESTAMP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        -- Handles format: '2020-01-01 00:00:00'
        TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS') AS invoice_date,
        YEAR(TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS'))  AS invoice_year,
        MONTH(TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS')) AS invoice_month,

        -- â”€â”€ Categoricals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        TRIM(UPPER(COUNTRY))         AS country,
        TRIM(UPPER(PAYMENTMETHOD))   AS payment_method,
        TRIM(UPPER(CATEGORY))        AS category,
        TRIM(UPPER(SALESCHANNEL))    AS sales_channel,
        TRIM(UPPER(RETURNSTATUS))    AS return_status,
        TRIM(UPPER(SHIPMENTPROVIDER)) AS shipment_provider,
        TRIM(UPPER(WAREHOUSELOCATION)) AS warehouse_location,
        TRIM(UPPER(ORDERPRIORITY))   AS order_priority,

        -- â”€â”€ Audit columns â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        CURRENT_TIMESTAMP()          AS stg_loaded_at

    FROM raw_data
),

-- â”€â”€ Data Quality Flags â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
validated AS (
    SELECT
        *,
        CASE
            WHEN invoice_date IS NULL           THEN 'INVALID_DATE'
            WHEN quantity IS NULL               THEN 'INVALID_QUANTITY'
            WHEN unit_price IS NULL             THEN 'INVALID_PRICE'
            WHEN shipping_cost < 0              THEN 'NEGATIVE_SHIPPING'
            WHEN discount < 0 OR discount > 1   THEN 'INVALID_DISCOUNT'
            WHEN invoice_no IS NULL             THEN 'NULL_INVOICE'
            ELSE 'VALID'
        END AS data_quality_flag

    FROM staged
)

SELECT * FROM validated


    -- â”€â”€ Incremental: only process new months not already in staging â”€â”€
    WHERE (invoice_year, invoice_month) NOT IN (
        SELECT DISTINCT invoice_year, invoice_month
        FROM INCREMENTALETL._staging.stg_sales
    )

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:26:54.706325 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m10:26:55.668438 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.962 seconds
[0m10:26:55.702508 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:26:55.704519 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:26:56.061282 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.365 seconds
[0m10:26:56.077308 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:26:56.077308 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:26:56.222308 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.141 seconds
[0m10:26:56.240470 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:26:56.240470 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:26:56.398996 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.147 seconds
[0m10:26:56.510019 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:26:56.510019 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table "INCREMENTALETL"."_STAGING"."STG_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:26:56.706450 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.191 seconds
[0m10:26:56.738737 [debug] [Thread-2 (]: 
    In "INCREMENTALETL"."_STAGING"."STG_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m10:26:56.788339 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.stg_sales"
[0m10:26:56.788339 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:26:56.788339 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:26:56.931700 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.149 seconds
[0m10:26:56.947893 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:26:56.947893 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: merge into INCREMENTALETL._staging.stg_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._staging.stg_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on ((DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no))

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","DATA_QUALITY_FLAG" = DBT_INTERNAL_SOURCE."DATA_QUALITY_FLAG"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "DATA_QUALITY_FLAG")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "DATA_QUALITY_FLAG")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:26:57.928173 [debug] [Thread-2 (]: SQL status: SUCCESS 2190 in 0.975 seconds
[0m10:26:57.932224 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:26:57.934251 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:26:58.239958 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.304 seconds
[0m10:26:58.275813 [debug] [Thread-2 (]: Applying DROP to: INCREMENTALETL._staging.stg_sales__dbt_tmp
[0m10:26:58.287618 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:26:58.287618 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: drop view if exists INCREMENTALETL._staging.stg_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:26:58.448897 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.171 seconds
[0m10:26:58.512243 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c28fb5f3-01ee-4769-bec7-f0697c9edffe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FB82C16950>]}
[0m10:26:58.512243 [info ] [Thread-2 (]: 1 of 2 OK created sql incremental model _staging.stg_sales ..................... [[32mSUCCESS 2190[0m in 3.93s]
[0m10:26:58.512243 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.stg_sales
[0m10:26:58.519963 [debug] [Thread-4 (]: Began running node model.sales_pipelines.processed_sales
[0m10:26:58.521642 [info ] [Thread-4 (]: 2 of 2 START sql incremental model _processed.processed_sales .................. [RUN]
[0m10:26:58.521642 [debug] [Thread-4 (]: Acquiring new snowflake connection 'model.sales_pipelines.processed_sales'
[0m10:26:58.524828 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.processed_sales
[0m10:26:58.528384 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.processed_sales"
[0m10:26:58.528384 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.processed_sales
[0m10:26:58.540544 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:26:58.544068 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: create or replace  temporary view INCREMENTALETL._processed.processed_sales__dbt_tmp
  
  
  
  
  as (
    

WITH staging AS (
    SELECT * FROM INCREMENTALETL._staging.stg_sales
),

-- â”€â”€ Step 1: Remove duplicates (keep latest loaded record) â”€â”€â”€â”€â”€â”€â”€â”€
deduped AS (
    SELECT *,
        ROW_NUMBER() OVER (
            PARTITION BY invoice_no
            ORDER BY stg_loaded_at DESC
        ) AS row_num
    FROM staging
),

-- â”€â”€ Step 2: Keep only VALID records, remove bad data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cleaned AS (
    SELECT
        invoice_no,
        stock_code,
        description,
        quantity,
        unit_price,
        discount,
        shipping_cost,
        customer_id,
        invoice_date,
        invoice_year,
        invoice_month,
        country,
        payment_method,
        category,
        sales_channel,
        return_status,
        shipment_provider,
        warehouse_location,
        order_priority,

        -- â”€â”€ Derived / Enriched columns â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        ROUND(quantity * unit_price, 2)                        AS gross_amount,
        ROUND(quantity * unit_price * (1 - discount), 2)       AS net_amount,
        ROUND(quantity * unit_price * (1 - discount)
              + COALESCE(shipping_cost, 0), 2)                 AS total_amount,

        CASE
            WHEN return_status = 'RETURNED' THEN TRUE
            ELSE FALSE
        END AS is_returned,

        CASE
            WHEN customer_id IS NULL THEN TRUE
            ELSE FALSE
        END AS is_guest_customer,

        -- â”€â”€ Audit â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        stg_loaded_at,
        CURRENT_TIMESTAMP() AS processed_at

    FROM deduped
    WHERE
        row_num = 1                     -- remove duplicates
        AND data_quality_flag = 'VALID' -- only clean records
        AND quantity > 0                -- remove returns/negatives
        AND unit_price > 0              -- remove invalid prices
        AND invoice_date IS NOT NULL    -- must have valid date
),

-- â”€â”€ Step 3: Final output â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
final AS (
    SELECT * FROM cleaned
)

SELECT * FROM final


    -- â”€â”€ Incremental: only process months not already in processed â”€â”€
    WHERE (invoice_year, invoice_month) NOT IN (
        SELECT DISTINCT invoice_year, invoice_month
        FROM INCREMENTALETL._processed.processed_sales
    )

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:26:58.545255 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m10:26:59.355268 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.814 seconds
[0m10:26:59.363820 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:26:59.363820 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:26:59.528685 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.154 seconds
[0m10:26:59.556617 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:26:59.560599 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:26:59.767015 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.210 seconds
[0m10:26:59.798597 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:26:59.798597 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:26:59.943446 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.148 seconds
[0m10:26:59.974952 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:26:59.974952 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:27:00.110339 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.133 seconds
[0m10:27:00.135064 [debug] [Thread-4 (]: 
    In "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m10:27:00.135064 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.processed_sales"
[0m10:27:00.151137 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:27:00.151137 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:27:00.294276 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.142 seconds
[0m10:27:00.294276 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:27:00.294276 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: merge into INCREMENTALETL._processed.processed_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._processed.processed_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on ((DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no))

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","GROSS_AMOUNT" = DBT_INTERNAL_SOURCE."GROSS_AMOUNT","NET_AMOUNT" = DBT_INTERNAL_SOURCE."NET_AMOUNT","TOTAL_AMOUNT" = DBT_INTERNAL_SOURCE."TOTAL_AMOUNT","IS_RETURNED" = DBT_INTERNAL_SOURCE."IS_RETURNED","IS_GUEST_CUSTOMER" = DBT_INTERNAL_SOURCE."IS_GUEST_CUSTOMER","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","PROCESSED_AT" = DBT_INTERNAL_SOURCE."PROCESSED_AT"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:27:02.532910 [debug] [Thread-4 (]: SQL status: SUCCESS 2090 in 2.232 seconds
[0m10:27:02.532910 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:27:02.532910 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:27:02.791497 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.263 seconds
[0m10:27:02.807445 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._processed.processed_sales__dbt_tmp
[0m10:27:02.823274 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:27:02.823274 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: drop view if exists INCREMENTALETL._processed.processed_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:27:02.986341 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.160 seconds
[0m10:27:03.000403 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c28fb5f3-01ee-4769-bec7-f0697c9edffe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FB848F46D0>]}
[0m10:27:03.000403 [info ] [Thread-4 (]: 2 of 2 OK created sql incremental model _processed.processed_sales ............. [[32mSUCCESS 2090[0m in 4.48s]
[0m10:27:03.000403 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.processed_sales
[0m10:27:03.016195 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:27:03.020247 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m10:27:03.022273 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m10:27:03.352794 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m10:27:03.352794 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m10:27:03.809464 [debug] [MainThread]: Connection 'list_INCREMENTALETL__staging' was left open.
[0m10:27:03.820449 [debug] [MainThread]: On list_INCREMENTALETL__staging: Close
[0m10:27:04.110534 [debug] [MainThread]: Connection 'list_INCREMENTALETL__processed' was left open.
[0m10:27:04.110534 [debug] [MainThread]: On list_INCREMENTALETL__processed: Close
[0m10:27:04.402003 [debug] [MainThread]: Connection 'model.sales_pipelines.stg_sales' was left open.
[0m10:27:04.402003 [debug] [MainThread]: On model.sales_pipelines.stg_sales: Close
[0m10:27:04.656187 [debug] [MainThread]: Connection 'model.sales_pipelines.processed_sales' was left open.
[0m10:27:04.656187 [debug] [MainThread]: On model.sales_pipelines.processed_sales: Close
[0m10:27:04.922454 [info ] [MainThread]: 
[0m10:27:04.922454 [info ] [MainThread]: Finished running 2 incremental models in 0 hours 0 minutes and 12.70 seconds (12.70s).
[0m10:27:04.929482 [debug] [MainThread]: Command end result
[0m10:27:04.987750 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m10:27:04.987750 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m10:27:05.000877 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\snowflake-incremental-pipeline\dbt_project\target\run_results.json
[0m10:27:05.000877 [info ] [MainThread]: 
[0m10:27:05.000877 [info ] [MainThread]: [32mCompleted successfully[0m
[0m10:27:05.000877 [info ] [MainThread]: 
[0m10:27:05.000877 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=2
[0m10:27:05.000877 [debug] [MainThread]: Command `dbt run` succeeded at 10:27:05.000877 after 15.29 seconds
[0m10:27:05.015001 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FBF04E76D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FBEEF03700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FBF1668070>]}
[0m10:27:05.015001 [debug] [MainThread]: Flushing usage events
[0m10:27:06.432057 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:30:14.524452 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020504537700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000205067B2230>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000205067B0AF0>]}


============================== 10:30:14.532186 | 4e682c04-9820-474a-80ff-50fd58965e6a ==============================
[0m10:30:14.532186 [info ] [MainThread]: Running with dbt=1.11.6
[0m10:30:14.532186 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'profiles_dir': 'D:\\snowflake-incremental-pipeline\\dbt_project', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'use_colors': 'True', 'indirect_selection': 'eager', 'invocation_command': 'dbt run', 'use_experimental_parser': 'False', 'log_path': 'D:\\snowflake-incremental-pipeline\\dbt_project\\logs'}
[0m10:30:15.522530 [debug] [MainThread]: Snowflake adapter: Setting snowflake.connector to ERROR (file logging only)
[0m10:30:15.522530 [debug] [MainThread]: Snowflake adapter: Setting botocore to ERROR (file logging only)
[0m10:30:15.522530 [debug] [MainThread]: Snowflake adapter: Setting boto3 to ERROR (file logging only)
[0m10:30:15.838984 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4e682c04-9820-474a-80ff-50fd58965e6a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020506783370>]}
[0m10:30:15.917966 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4e682c04-9820-474a-80ff-50fd58965e6a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020505F7B9D0>]}
[0m10:30:15.917966 [info ] [MainThread]: Registered adapter: snowflake=1.11.2
[0m10:30:16.425478 [debug] [MainThread]: checksum: ec27062d18352e9cfca161c166fcc73d2201661a23a3cf3c89196a27f32cfcb8, vars: {}, profile: , target: , version: 1.11.6
[0m10:30:16.648055 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m10:30:16.664055 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m10:30:16.664055 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m10:30:16.727315 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4e682c04-9820-474a-80ff-50fd58965e6a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020518EAC130>]}
[0m10:30:16.870117 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m10:30:16.870117 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m10:30:16.901792 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4e682c04-9820-474a-80ff-50fd58965e6a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020518E4C880>]}
[0m10:30:16.901792 [info ] [MainThread]: Found 2 models, 6 data tests, 1 source, 522 macros
[0m10:30:16.901792 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4e682c04-9820-474a-80ff-50fd58965e6a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020518E4C8B0>]}
[0m10:30:16.901792 [info ] [MainThread]: 
[0m10:30:16.901792 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m10:30:16.901792 [info ] [MainThread]: 
[0m10:30:16.901792 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m10:30:16.919599 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m10:30:16.921605 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m10:30:17.014309 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m10:30:17.014309 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m10:30:17.014309 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m10:30:17.014309 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m10:30:17.014309 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:30:17.014309 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:30:18.201583 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 1.183 seconds
[0m10:30:18.272261 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 1.247 seconds
[0m10:30:18.283645 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__processed'
[0m10:30:18.283645 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__staging'
[0m10:30:18.345149 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__processed"
[0m10:30:18.345149 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__staging"
[0m10:30:18.345149 [debug] [ThreadPool]: On list_INCREMENTALETL__processed: show objects in INCREMENTALETL._processed
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__processed"} */;
[0m10:30:18.345149 [debug] [ThreadPool]: On list_INCREMENTALETL__staging: show objects in INCREMENTALETL._staging
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__staging"} */;
[0m10:30:18.345149 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:30:18.360935 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:30:19.098997 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.740 seconds
[0m10:30:19.756119 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 1.396 seconds
[0m10:30:19.764213 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4e682c04-9820-474a-80ff-50fd58965e6a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000205177BED10>]}
[0m10:30:19.775693 [debug] [Thread-2 (]: Began running node model.sales_pipelines.stg_sales
[0m10:30:19.788585 [info ] [Thread-2 (]: 1 of 2 START sql incremental model _staging.stg_sales .......................... [RUN]
[0m10:30:19.791755 [debug] [Thread-2 (]: Acquiring new snowflake connection 'model.sales_pipelines.stg_sales'
[0m10:30:19.795814 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.stg_sales
[0m10:30:19.825997 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.stg_sales"
[0m10:30:19.839341 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.stg_sales
[0m10:30:19.919024 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:30:19.919024 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: create or replace  temporary view INCREMENTALETL._staging.stg_sales__dbt_tmp
  
  
  
  
  as (
    

WITH raw_data AS (
    SELECT * FROM INCREMENTALETL.LANDINGZONE.RAW_SALES
),

staged AS (
    SELECT
        -- â”€â”€ IDs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        CAST(INVOICENO       AS VARCHAR)    AS invoice_no,
        CAST(STOCKCODE       AS VARCHAR)    AS stock_code,
        CAST(DESCRIPTION     AS VARCHAR)    AS description,

        -- â”€â”€ Quantities & Prices â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        TRY_CAST(QUANTITY    AS INT)        AS quantity,
        TRY_CAST(UNITPRICE   AS FLOAT)      AS unit_price,
        TRY_CAST(DISCOUNT    AS FLOAT)      AS discount,
        TRY_CAST(SHIPPINGCOST AS FLOAT)     AS shipping_cost,
        TRY_CAST(CUSTOMERID  AS FLOAT)      AS customer_id,

        -- â”€â”€ Date Fix: VARCHAR â†’ proper TIMESTAMP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        -- Handles format: '2020-01-01 00:00:00'
        TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS') AS invoice_date,
        YEAR(TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS'))  AS invoice_year,
        MONTH(TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS')) AS invoice_month,

        -- â”€â”€ Categoricals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        TRIM(UPPER(COUNTRY))         AS country,
        TRIM(UPPER(PAYMENTMETHOD))   AS payment_method,
        TRIM(UPPER(CATEGORY))        AS category,
        TRIM(UPPER(SALESCHANNEL))    AS sales_channel,
        TRIM(UPPER(RETURNSTATUS))    AS return_status,
        TRIM(UPPER(SHIPMENTPROVIDER)) AS shipment_provider,
        TRIM(UPPER(WAREHOUSELOCATION)) AS warehouse_location,
        TRIM(UPPER(ORDERPRIORITY))   AS order_priority,

        -- â”€â”€ Audit columns â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        CURRENT_TIMESTAMP()          AS stg_loaded_at

    FROM raw_data
),

-- â”€â”€ Data Quality Flags â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
validated AS (
    SELECT
        *,
        CASE
            WHEN invoice_date IS NULL           THEN 'INVALID_DATE'
            WHEN quantity IS NULL               THEN 'INVALID_QUANTITY'
            WHEN unit_price IS NULL             THEN 'INVALID_PRICE'
            WHEN shipping_cost < 0              THEN 'NEGATIVE_SHIPPING'
            WHEN discount < 0 OR discount > 1   THEN 'INVALID_DISCOUNT'
            WHEN invoice_no IS NULL             THEN 'NULL_INVOICE'
            ELSE 'VALID'
        END AS data_quality_flag

    FROM staged
)

SELECT * FROM validated


    -- â”€â”€ Incremental: only process new months not already in staging â”€â”€
    WHERE (invoice_year, invoice_month) NOT IN (
        SELECT DISTINCT invoice_year, invoice_month
        FROM INCREMENTALETL._staging.stg_sales
    )

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:30:19.919024 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m10:30:20.769800 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.861 seconds
[0m10:30:20.801651 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:30:20.801651 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:30:20.944083 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.141 seconds
[0m10:30:20.959997 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:30:20.959997 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:30:21.088289 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.131 seconds
[0m10:30:21.136076 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:30:21.136076 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:30:21.267870 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.130 seconds
[0m10:30:21.376400 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:30:21.392110 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table "INCREMENTALETL"."_STAGING"."STG_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:30:21.519823 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.132 seconds
[0m10:30:21.551600 [debug] [Thread-2 (]: 
    In "INCREMENTALETL"."_STAGING"."STG_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m10:30:21.599531 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.stg_sales"
[0m10:30:21.599531 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:30:21.599531 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:30:21.808999 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.203 seconds
[0m10:30:21.808999 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:30:21.808999 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: merge into INCREMENTALETL._staging.stg_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._staging.stg_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on ((DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no))

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","DATA_QUALITY_FLAG" = DBT_INTERNAL_SOURCE."DATA_QUALITY_FLAG"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "DATA_QUALITY_FLAG")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "DATA_QUALITY_FLAG")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:30:22.799691 [debug] [Thread-2 (]: SQL status: SUCCESS 2144 in 0.985 seconds
[0m10:30:22.799691 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:30:22.799691 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:30:23.052016 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.239 seconds
[0m10:30:23.081054 [debug] [Thread-2 (]: Applying DROP to: INCREMENTALETL._staging.stg_sales__dbt_tmp
[0m10:30:23.090838 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:30:23.092846 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: drop view if exists INCREMENTALETL._staging.stg_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:30:23.260254 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.168 seconds
[0m10:30:23.313999 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4e682c04-9820-474a-80ff-50fd58965e6a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020506844F40>]}
[0m10:30:23.313999 [info ] [Thread-2 (]: 1 of 2 OK created sql incremental model _staging.stg_sales ..................... [[32mSUCCESS 2144[0m in 3.52s]
[0m10:30:23.313999 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.stg_sales
[0m10:30:23.323206 [debug] [Thread-4 (]: Began running node model.sales_pipelines.processed_sales
[0m10:30:23.325004 [info ] [Thread-4 (]: 2 of 2 START sql incremental model _processed.processed_sales .................. [RUN]
[0m10:30:23.325004 [debug] [Thread-4 (]: Acquiring new snowflake connection 'model.sales_pipelines.processed_sales'
[0m10:30:23.325004 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.processed_sales
[0m10:30:23.330062 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.processed_sales"
[0m10:30:23.330062 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.processed_sales
[0m10:30:23.330062 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:30:23.345822 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: create or replace  temporary view INCREMENTALETL._processed.processed_sales__dbt_tmp
  
  
  
  
  as (
    

WITH staging AS (
    SELECT * FROM INCREMENTALETL._staging.stg_sales
),

-- â”€â”€ Step 1: Remove duplicates (keep latest loaded record) â”€â”€â”€â”€â”€â”€â”€â”€
deduped AS (
    SELECT *,
        ROW_NUMBER() OVER (
            PARTITION BY invoice_no
            ORDER BY stg_loaded_at DESC
        ) AS row_num
    FROM staging
),

-- â”€â”€ Step 2: Keep only VALID records, remove bad data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cleaned AS (
    SELECT
        invoice_no,
        stock_code,
        description,
        quantity,
        unit_price,
        discount,
        shipping_cost,
        customer_id,
        invoice_date,
        invoice_year,
        invoice_month,
        country,
        payment_method,
        category,
        sales_channel,
        return_status,
        shipment_provider,
        warehouse_location,
        order_priority,

        -- â”€â”€ Derived / Enriched columns â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        ROUND(quantity * unit_price, 2)                        AS gross_amount,
        ROUND(quantity * unit_price * (1 - discount), 2)       AS net_amount,
        ROUND(quantity * unit_price * (1 - discount)
              + COALESCE(shipping_cost, 0), 2)                 AS total_amount,

        CASE
            WHEN return_status = 'RETURNED' THEN TRUE
            ELSE FALSE
        END AS is_returned,

        CASE
            WHEN customer_id IS NULL THEN TRUE
            ELSE FALSE
        END AS is_guest_customer,

        -- â”€â”€ Audit â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        stg_loaded_at,
        CURRENT_TIMESTAMP() AS processed_at

    FROM deduped
    WHERE
        row_num = 1                     -- remove duplicates
        AND data_quality_flag = 'VALID' -- only clean records
        AND quantity > 0                -- remove returns/negatives
        AND unit_price > 0              -- remove invalid prices
        AND invoice_date IS NOT NULL    -- must have valid date
),

-- â”€â”€ Step 3: Final output â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
final AS (
    SELECT * FROM cleaned
)

SELECT * FROM final


    -- â”€â”€ Incremental: only process months not already in processed â”€â”€
    WHERE (invoice_year, invoice_month) NOT IN (
        SELECT DISTINCT invoice_year, invoice_month
        FROM INCREMENTALETL._processed.processed_sales
    )

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:30:23.347061 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m10:30:24.153386 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.811 seconds
[0m10:30:24.167526 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:30:24.167526 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:30:24.327947 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.160 seconds
[0m10:30:24.359565 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:30:24.359565 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:30:24.553159 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.200 seconds
[0m10:30:24.585145 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:30:24.585145 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:30:24.730145 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.137 seconds
[0m10:30:24.746260 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:30:24.746260 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:30:24.894597 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.144 seconds
[0m10:30:24.923627 [debug] [Thread-4 (]: 
    In "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m10:30:24.940358 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.processed_sales"
[0m10:30:24.940358 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:30:24.940358 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:30:25.099848 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.153 seconds
[0m10:30:25.103900 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:30:25.105922 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: merge into INCREMENTALETL._processed.processed_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._processed.processed_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on ((DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no))

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","GROSS_AMOUNT" = DBT_INTERNAL_SOURCE."GROSS_AMOUNT","NET_AMOUNT" = DBT_INTERNAL_SOURCE."NET_AMOUNT","TOTAL_AMOUNT" = DBT_INTERNAL_SOURCE."TOTAL_AMOUNT","IS_RETURNED" = DBT_INTERNAL_SOURCE."IS_RETURNED","IS_GUEST_CUSTOMER" = DBT_INTERNAL_SOURCE."IS_GUEST_CUSTOMER","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","PROCESSED_AT" = DBT_INTERNAL_SOURCE."PROCESSED_AT"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:30:26.203293 [debug] [Thread-4 (]: SQL status: SUCCESS 2034 in 1.095 seconds
[0m10:30:26.203293 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:30:26.203293 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:30:26.504624 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.302 seconds
[0m10:30:26.520595 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._processed.processed_sales__dbt_tmp
[0m10:30:26.520595 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:30:26.520595 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: drop view if exists INCREMENTALETL._processed.processed_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:30:26.686989 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.161 seconds
[0m10:30:26.697105 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4e682c04-9820-474a-80ff-50fd58965e6a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020519674A30>]}
[0m10:30:26.697105 [info ] [Thread-4 (]: 2 of 2 OK created sql incremental model _processed.processed_sales ............. [[32mSUCCESS 2034[0m in 3.37s]
[0m10:30:26.697105 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.processed_sales
[0m10:30:26.714642 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:30:26.714642 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m10:30:26.714642 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m10:30:27.134140 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m10:30:27.147378 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m10:30:27.403236 [debug] [MainThread]: Connection 'list_INCREMENTALETL__processed' was left open.
[0m10:30:27.406277 [debug] [MainThread]: On list_INCREMENTALETL__processed: Close
[0m10:30:27.645815 [debug] [MainThread]: Connection 'list_INCREMENTALETL__staging' was left open.
[0m10:30:27.645815 [debug] [MainThread]: On list_INCREMENTALETL__staging: Close
[0m10:30:27.916175 [debug] [MainThread]: Connection 'model.sales_pipelines.stg_sales' was left open.
[0m10:30:27.916175 [debug] [MainThread]: On model.sales_pipelines.stg_sales: Close
[0m10:30:28.162622 [debug] [MainThread]: Connection 'model.sales_pipelines.processed_sales' was left open.
[0m10:30:28.162622 [debug] [MainThread]: On model.sales_pipelines.processed_sales: Close
[0m10:30:28.418900 [info ] [MainThread]: 
[0m10:30:28.418900 [info ] [MainThread]: Finished running 2 incremental models in 0 hours 0 minutes and 11.50 seconds (11.50s).
[0m10:30:28.418900 [debug] [MainThread]: Command end result
[0m10:30:28.482596 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m10:30:28.498249 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m10:30:28.498249 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\snowflake-incremental-pipeline\dbt_project\target\run_results.json
[0m10:30:28.498249 [info ] [MainThread]: 
[0m10:30:28.498249 [info ] [MainThread]: [32mCompleted successfully[0m
[0m10:30:28.498249 [info ] [MainThread]: 
[0m10:30:28.498249 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=2
[0m10:30:28.513886 [debug] [MainThread]: Command `dbt run` succeeded at 10:30:28.513886 after 14.19 seconds
[0m10:30:28.513886 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020504537700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000205043AD7E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020505878D90>]}
[0m10:30:28.513886 [debug] [MainThread]: Flushing usage events
[0m10:30:29.497918 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:32:16.757079 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CCB3D77700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CCB5FF2230>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CCB5FF0AF0>]}


============================== 10:32:16.763109 | 685366ec-bb99-40b3-97f7-ec3ae3ad4b83 ==============================
[0m10:32:16.763109 [info ] [MainThread]: Running with dbt=1.11.6
[0m10:32:16.763109 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'D:\\snowflake-incremental-pipeline\\dbt_project', 'indirect_selection': 'eager', 'use_colors': 'True', 'invocation_command': 'dbt run', 'use_experimental_parser': 'False', 'log_path': 'D:\\snowflake-incremental-pipeline\\dbt_project\\logs'}
[0m10:32:17.725995 [debug] [MainThread]: Snowflake adapter: Setting snowflake.connector to ERROR (file logging only)
[0m10:32:17.725995 [debug] [MainThread]: Snowflake adapter: Setting botocore to ERROR (file logging only)
[0m10:32:17.725995 [debug] [MainThread]: Snowflake adapter: Setting boto3 to ERROR (file logging only)
[0m10:32:18.033351 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '685366ec-bb99-40b3-97f7-ec3ae3ad4b83', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CCB5FC3370>]}
[0m10:32:18.112215 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '685366ec-bb99-40b3-97f7-ec3ae3ad4b83', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CCB57BB9D0>]}
[0m10:32:18.112215 [info ] [MainThread]: Registered adapter: snowflake=1.11.2
[0m10:32:18.652274 [debug] [MainThread]: checksum: ec27062d18352e9cfca161c166fcc73d2201661a23a3cf3c89196a27f32cfcb8, vars: {}, profile: , target: , version: 1.11.6
[0m10:32:18.887335 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m10:32:18.889027 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m10:32:18.889027 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m10:32:18.971587 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '685366ec-bb99-40b3-97f7-ec3ae3ad4b83', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CCC86EC130>]}
[0m10:32:19.145549 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m10:32:19.145549 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m10:32:19.175311 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '685366ec-bb99-40b3-97f7-ec3ae3ad4b83', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CCC8684880>]}
[0m10:32:19.175311 [info ] [MainThread]: Found 2 models, 6 data tests, 1 source, 522 macros
[0m10:32:19.175311 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '685366ec-bb99-40b3-97f7-ec3ae3ad4b83', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CCC86848B0>]}
[0m10:32:19.175311 [info ] [MainThread]: 
[0m10:32:19.183812 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m10:32:19.183812 [info ] [MainThread]: 
[0m10:32:19.183812 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m10:32:19.192330 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m10:32:19.196830 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m10:32:19.301609 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m10:32:19.301609 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m10:32:19.301609 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m10:32:19.301609 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m10:32:19.301609 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:32:19.301609 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:32:20.519354 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 1.207 seconds
[0m10:32:20.523816 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 1.211 seconds
[0m10:32:20.545784 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__staging'
[0m10:32:20.553628 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__processed'
[0m10:32:20.580192 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__processed"
[0m10:32:20.580192 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__staging"
[0m10:32:20.588181 [debug] [ThreadPool]: On list_INCREMENTALETL__processed: show objects in INCREMENTALETL._processed
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__processed"} */;
[0m10:32:20.588181 [debug] [ThreadPool]: On list_INCREMENTALETL__staging: show objects in INCREMENTALETL._staging
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__staging"} */;
[0m10:32:20.588181 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:32:20.590944 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:32:21.544833 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.955 seconds
[0m10:32:21.546861 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.961 seconds
[0m10:32:21.558732 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '685366ec-bb99-40b3-97f7-ec3ae3ad4b83', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CCC6FFED10>]}
[0m10:32:21.574693 [debug] [Thread-2 (]: Began running node model.sales_pipelines.stg_sales
[0m10:32:21.594282 [info ] [Thread-2 (]: 1 of 2 START sql incremental model _staging.stg_sales .......................... [RUN]
[0m10:32:21.598145 [debug] [Thread-2 (]: Acquiring new snowflake connection 'model.sales_pipelines.stg_sales'
[0m10:32:21.602203 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.stg_sales
[0m10:32:21.647018 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.stg_sales"
[0m10:32:21.655422 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.stg_sales
[0m10:32:21.832994 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:32:21.841810 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: create or replace  temporary view INCREMENTALETL._staging.stg_sales__dbt_tmp
  
  
  
  
  as (
    

WITH raw_data AS (
    SELECT * FROM INCREMENTALETL.LANDINGZONE.RAW_SALES
),

staged AS (
    SELECT
        -- â”€â”€ IDs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        CAST(INVOICENO       AS VARCHAR)    AS invoice_no,
        CAST(STOCKCODE       AS VARCHAR)    AS stock_code,
        CAST(DESCRIPTION     AS VARCHAR)    AS description,

        -- â”€â”€ Quantities & Prices â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        TRY_CAST(QUANTITY    AS INT)        AS quantity,
        TRY_CAST(UNITPRICE   AS FLOAT)      AS unit_price,
        TRY_CAST(DISCOUNT    AS FLOAT)      AS discount,
        TRY_CAST(SHIPPINGCOST AS FLOAT)     AS shipping_cost,
        TRY_CAST(CUSTOMERID  AS FLOAT)      AS customer_id,

        -- â”€â”€ Date Fix: VARCHAR â†’ proper TIMESTAMP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        -- Handles format: '2020-01-01 00:00:00'
        TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS') AS invoice_date,
        YEAR(TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS'))  AS invoice_year,
        MONTH(TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS')) AS invoice_month,

        -- â”€â”€ Categoricals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        TRIM(UPPER(COUNTRY))         AS country,
        TRIM(UPPER(PAYMENTMETHOD))   AS payment_method,
        TRIM(UPPER(CATEGORY))        AS category,
        TRIM(UPPER(SALESCHANNEL))    AS sales_channel,
        TRIM(UPPER(RETURNSTATUS))    AS return_status,
        TRIM(UPPER(SHIPMENTPROVIDER)) AS shipment_provider,
        TRIM(UPPER(WAREHOUSELOCATION)) AS warehouse_location,
        TRIM(UPPER(ORDERPRIORITY))   AS order_priority,

        -- â”€â”€ Audit columns â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        CURRENT_TIMESTAMP()          AS stg_loaded_at

    FROM raw_data
),

-- â”€â”€ Data Quality Flags â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
validated AS (
    SELECT
        *,
        CASE
            WHEN invoice_date IS NULL           THEN 'INVALID_DATE'
            WHEN quantity IS NULL               THEN 'INVALID_QUANTITY'
            WHEN unit_price IS NULL             THEN 'INVALID_PRICE'
            WHEN shipping_cost < 0              THEN 'NEGATIVE_SHIPPING'
            WHEN discount < 0 OR discount > 1   THEN 'INVALID_DISCOUNT'
            WHEN invoice_no IS NULL             THEN 'NULL_INVOICE'
            ELSE 'VALID'
        END AS data_quality_flag

    FROM staged
)

SELECT * FROM validated


    -- â”€â”€ Incremental: only process new months not already in staging â”€â”€
    WHERE (invoice_year, invoice_month) NOT IN (
        SELECT DISTINCT invoice_year, invoice_month
        FROM INCREMENTALETL._staging.stg_sales
    )

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:32:21.845125 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m10:32:22.707176 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.863 seconds
[0m10:32:22.752492 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:32:22.755011 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:32:22.911220 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.156 seconds
[0m10:32:22.935838 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:32:22.938411 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:32:23.077425 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.137 seconds
[0m10:32:23.129495 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:32:23.135832 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:32:23.271541 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.137 seconds
[0m10:32:23.402829 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:32:23.402829 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table "INCREMENTALETL"."_STAGING"."STG_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:32:23.562957 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.153 seconds
[0m10:32:23.611948 [debug] [Thread-2 (]: 
    In "INCREMENTALETL"."_STAGING"."STG_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m10:32:23.703471 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.stg_sales"
[0m10:32:23.703471 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:32:23.703471 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:32:23.866828 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.155 seconds
[0m10:32:23.874281 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:32:23.874281 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: merge into INCREMENTALETL._staging.stg_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._staging.stg_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on ((DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no))

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","DATA_QUALITY_FLAG" = DBT_INTERNAL_SOURCE."DATA_QUALITY_FLAG"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "DATA_QUALITY_FLAG")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "DATA_QUALITY_FLAG")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:32:24.886509 [debug] [Thread-2 (]: SQL status: SUCCESS 2896 in 1.001 seconds
[0m10:32:24.886509 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:32:24.894925 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:32:25.171690 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.274 seconds
[0m10:32:25.188597 [debug] [Thread-2 (]: Applying DROP to: INCREMENTALETL._staging.stg_sales__dbt_tmp
[0m10:32:25.208038 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:32:25.210581 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: drop view if exists INCREMENTALETL._staging.stg_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:32:25.389957 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.180 seconds
[0m10:32:25.444856 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '685366ec-bb99-40b3-97f7-ec3ae3ad4b83', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CCC865E140>]}
[0m10:32:25.446869 [info ] [Thread-2 (]: 1 of 2 OK created sql incremental model _staging.stg_sales ..................... [[32mSUCCESS 2896[0m in 3.85s]
[0m10:32:25.450895 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.stg_sales
[0m10:32:25.450895 [debug] [Thread-4 (]: Began running node model.sales_pipelines.processed_sales
[0m10:32:25.455137 [info ] [Thread-4 (]: 2 of 2 START sql incremental model _processed.processed_sales .................. [RUN]
[0m10:32:25.456280 [debug] [Thread-4 (]: Acquiring new snowflake connection 'model.sales_pipelines.processed_sales'
[0m10:32:25.456280 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.processed_sales
[0m10:32:25.472070 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.processed_sales"
[0m10:32:25.473238 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.processed_sales
[0m10:32:25.489140 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:32:25.495643 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: create or replace  temporary view INCREMENTALETL._processed.processed_sales__dbt_tmp
  
  
  
  
  as (
    

WITH staging AS (
    SELECT * FROM INCREMENTALETL._staging.stg_sales
),

-- â”€â”€ Step 1: Remove duplicates (keep latest loaded record) â”€â”€â”€â”€â”€â”€â”€â”€
deduped AS (
    SELECT *,
        ROW_NUMBER() OVER (
            PARTITION BY invoice_no
            ORDER BY stg_loaded_at DESC
        ) AS row_num
    FROM staging
),

-- â”€â”€ Step 2: Keep only VALID records, remove bad data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cleaned AS (
    SELECT
        invoice_no,
        stock_code,
        description,
        quantity,
        unit_price,
        discount,
        shipping_cost,
        customer_id,
        invoice_date,
        invoice_year,
        invoice_month,
        country,
        payment_method,
        category,
        sales_channel,
        return_status,
        shipment_provider,
        warehouse_location,
        order_priority,

        -- â”€â”€ Derived / Enriched columns â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        ROUND(quantity * unit_price, 2)                        AS gross_amount,
        ROUND(quantity * unit_price * (1 - discount), 2)       AS net_amount,
        ROUND(quantity * unit_price * (1 - discount)
              + COALESCE(shipping_cost, 0), 2)                 AS total_amount,

        CASE
            WHEN return_status = 'RETURNED' THEN TRUE
            ELSE FALSE
        END AS is_returned,

        CASE
            WHEN customer_id IS NULL THEN TRUE
            ELSE FALSE
        END AS is_guest_customer,

        -- â”€â”€ Audit â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        stg_loaded_at,
        CURRENT_TIMESTAMP() AS processed_at

    FROM deduped
    WHERE
        row_num = 1                     -- remove duplicates
        AND data_quality_flag = 'VALID' -- only clean records
        AND quantity > 0                -- remove returns/negatives
        AND unit_price > 0              -- remove invalid prices
        AND invoice_date IS NOT NULL    -- must have valid date
),

-- â”€â”€ Step 3: Final output â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
final AS (
    SELECT * FROM cleaned
)

SELECT * FROM final


    -- â”€â”€ Incremental: only process months not already in processed â”€â”€
    WHERE (invoice_year, invoice_month) NOT IN (
        SELECT DISTINCT invoice_year, invoice_month
        FROM INCREMENTALETL._processed.processed_sales
    )

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:32:25.495643 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m10:32:26.402302 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.914 seconds
[0m10:32:26.421077 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:32:26.434813 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:32:26.618557 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.192 seconds
[0m10:32:26.658484 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:32:26.666863 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:32:26.846452 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.174 seconds
[0m10:32:26.886527 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:32:26.886527 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:32:27.061740 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.173 seconds
[0m10:32:27.076537 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:32:27.076537 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:32:27.256920 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.173 seconds
[0m10:32:27.289773 [debug] [Thread-4 (]: 
    In "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m10:32:27.297920 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.processed_sales"
[0m10:32:27.306652 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:32:27.308279 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:32:27.491945 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.191 seconds
[0m10:32:27.505110 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:32:27.509503 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: merge into INCREMENTALETL._processed.processed_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._processed.processed_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on ((DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no))

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","GROSS_AMOUNT" = DBT_INTERNAL_SOURCE."GROSS_AMOUNT","NET_AMOUNT" = DBT_INTERNAL_SOURCE."NET_AMOUNT","TOTAL_AMOUNT" = DBT_INTERNAL_SOURCE."TOTAL_AMOUNT","IS_RETURNED" = DBT_INTERNAL_SOURCE."IS_RETURNED","IS_GUEST_CUSTOMER" = DBT_INTERNAL_SOURCE."IS_GUEST_CUSTOMER","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","PROCESSED_AT" = DBT_INTERNAL_SOURCE."PROCESSED_AT"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:32:28.572627 [debug] [Thread-4 (]: SQL status: SUCCESS 2755 in 1.061 seconds
[0m10:32:28.572627 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:32:28.572627 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:32:28.873647 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.294 seconds
[0m10:32:28.889422 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._processed.processed_sales__dbt_tmp
[0m10:32:28.903261 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:32:28.907315 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: drop view if exists INCREMENTALETL._processed.processed_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:32:29.109516 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.200 seconds
[0m10:32:29.109516 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '685366ec-bb99-40b3-97f7-ec3ae3ad4b83', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CCC71F6230>]}
[0m10:32:29.117560 [info ] [Thread-4 (]: 2 of 2 OK created sql incremental model _processed.processed_sales ............. [[32mSUCCESS 2755[0m in 3.65s]
[0m10:32:29.120519 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.processed_sales
[0m10:32:29.125947 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:32:29.125947 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m10:32:29.131886 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m10:32:29.379361 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m10:32:29.388792 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m10:32:29.671473 [debug] [MainThread]: Connection 'list_INCREMENTALETL__staging' was left open.
[0m10:32:29.680011 [debug] [MainThread]: On list_INCREMENTALETL__staging: Close
[0m10:32:30.006781 [debug] [MainThread]: Connection 'list_INCREMENTALETL__processed' was left open.
[0m10:32:30.006781 [debug] [MainThread]: On list_INCREMENTALETL__processed: Close
[0m10:32:30.940490 [debug] [MainThread]: Connection 'model.sales_pipelines.stg_sales' was left open.
[0m10:32:30.944491 [debug] [MainThread]: On model.sales_pipelines.stg_sales: Close
[0m10:32:31.201919 [debug] [MainThread]: Connection 'model.sales_pipelines.processed_sales' was left open.
[0m10:32:31.206809 [debug] [MainThread]: On model.sales_pipelines.processed_sales: Close
[0m10:32:31.508701 [info ] [MainThread]: 
[0m10:32:31.511797 [info ] [MainThread]: Finished running 2 incremental models in 0 hours 0 minutes and 12.32 seconds (12.32s).
[0m10:32:31.513200 [debug] [MainThread]: Command end result
[0m10:32:31.608929 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m10:32:31.615866 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m10:32:31.641415 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\snowflake-incremental-pipeline\dbt_project\target\run_results.json
[0m10:32:31.641415 [info ] [MainThread]: 
[0m10:32:31.645608 [info ] [MainThread]: [32mCompleted successfully[0m
[0m10:32:31.645608 [info ] [MainThread]: 
[0m10:32:31.645608 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=2
[0m10:32:31.645608 [debug] [MainThread]: Command `dbt run` succeeded at 10:32:31.645608 after 15.04 seconds
[0m10:32:31.654927 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CCB3D77700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CCC869B730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CCB3763730>]}
[0m10:32:31.657214 [debug] [MainThread]: Flushing usage events
[0m10:32:33.175816 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:46:22.576056 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C2EF3476D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C2F15C1030>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C2F15C0880>]}


============================== 10:46:22.578290 | 27fdac5c-f8e1-456b-a2e9-3ebb870cc9c1 ==============================
[0m10:46:22.578290 [info ] [MainThread]: Running with dbt=1.11.6
[0m10:46:22.578290 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'empty': 'None', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'D:\\snowflake-incremental-pipeline\\dbt_project', 'indirect_selection': 'eager', 'invocation_command': 'dbt clean', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'D:\\snowflake-incremental-pipeline\\dbt_project\\logs'}
[0m10:46:23.107315 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '27fdac5c-f8e1-456b-a2e9-3ebb870cc9c1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C2F15C0880>]}
[0m10:46:23.170692 [debug] [MainThread]: Command `dbt clean` succeeded at 10:46:23.168677 after 0.74 seconds
[0m10:46:23.172708 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C2EF3476D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C2F15935B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C2F165D390>]}
[0m10:46:23.174467 [debug] [MainThread]: Flushing usage events
[0m10:46:24.658422 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:46:35.651107 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000010C330276A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000010C342D1000>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000010C342D23B0>]}


============================== 10:46:35.651107 | a6d40cb8-3051-459b-980c-e6f4b1de68a5 ==============================
[0m10:46:35.651107 [info ] [MainThread]: Running with dbt=1.11.6
[0m10:46:35.651107 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'profiles_dir': 'D:\\snowflake-incremental-pipeline\\dbt_project', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'use_colors': 'True', 'indirect_selection': 'eager', 'invocation_command': 'dbt run --full-refresh', 'use_experimental_parser': 'False', 'log_path': 'D:\\snowflake-incremental-pipeline\\dbt_project\\logs'}
[0m10:46:36.615198 [debug] [MainThread]: Snowflake adapter: Setting snowflake.connector to ERROR (file logging only)
[0m10:46:36.615198 [debug] [MainThread]: Snowflake adapter: Setting botocore to ERROR (file logging only)
[0m10:46:36.615198 [debug] [MainThread]: Snowflake adapter: Setting boto3 to ERROR (file logging only)
[0m10:46:36.916351 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a6d40cb8-3051-459b-980c-e6f4b1de68a5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000010C33AC52A0>]}
[0m10:46:36.996246 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a6d40cb8-3051-459b-980c-e6f4b1de68a5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000010C3436A890>]}
[0m10:46:36.996246 [info ] [MainThread]: Registered adapter: snowflake=1.11.2
[0m10:46:37.503333 [debug] [MainThread]: checksum: ec27062d18352e9cfca161c166fcc73d2201661a23a3cf3c89196a27f32cfcb8, vars: {}, profile: , target: , version: 1.11.6
[0m10:46:37.503333 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m10:46:37.519364 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'a6d40cb8-3051-459b-980c-e6f4b1de68a5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000010C33B27160>]}
[0m10:46:39.566563 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a6d40cb8-3051-459b-980c-e6f4b1de68a5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000010C46A9DF60>]}
[0m10:46:39.710276 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m10:46:39.710276 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m10:46:39.725067 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a6d40cb8-3051-459b-980c-e6f4b1de68a5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000010C459408B0>]}
[0m10:46:39.725067 [info ] [MainThread]: Found 2 models, 6 data tests, 1 source, 522 macros
[0m10:46:39.725067 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a6d40cb8-3051-459b-980c-e6f4b1de68a5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000010C45940790>]}
[0m10:46:39.740959 [info ] [MainThread]: 
[0m10:46:39.740959 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m10:46:39.744308 [info ] [MainThread]: 
[0m10:46:39.744308 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m10:46:39.744308 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m10:46:39.756863 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m10:46:39.851619 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m10:46:39.851619 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m10:46:39.851619 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m10:46:39.851619 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m10:46:39.851619 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:46:39.851619 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:46:41.564049 [debug] [ThreadPool]: SQL status: SUCCESS 2 in 1.705 seconds
[0m10:46:41.567150 [debug] [ThreadPool]: SQL status: SUCCESS 2 in 1.708 seconds
[0m10:46:41.583417 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_INCREMENTALETL, now create_INCREMENTALETL__processed)
[0m10:46:41.583417 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_INCREMENTALETL, now create_INCREMENTALETL__staging)
[0m10:46:41.583417 [debug] [ThreadPool]: Creating schema "database: "INCREMENTALETL"
schema: "_processed"
"
[0m10:46:41.591549 [debug] [ThreadPool]: Creating schema "database: "INCREMENTALETL"
schema: "_staging"
"
[0m10:46:41.613226 [debug] [ThreadPool]: Using snowflake connection "create_INCREMENTALETL__processed"
[0m10:46:41.625636 [debug] [ThreadPool]: Using snowflake connection "create_INCREMENTALETL__staging"
[0m10:46:41.627660 [debug] [ThreadPool]: On create_INCREMENTALETL__processed: create schema if not exists INCREMENTALETL._processed
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "create_INCREMENTALETL__processed"} */
[0m10:46:41.629452 [debug] [ThreadPool]: On create_INCREMENTALETL__staging: create schema if not exists INCREMENTALETL._staging
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "create_INCREMENTALETL__staging"} */
[0m10:46:41.958513 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.322 seconds
[0m10:46:41.961799 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.334 seconds
[0m10:46:41.974762 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__staging'
[0m10:46:41.979103 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__processed'
[0m10:46:42.024953 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__staging"
[0m10:46:42.040802 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__processed"
[0m10:46:42.040802 [debug] [ThreadPool]: On list_INCREMENTALETL__staging: show objects in INCREMENTALETL._staging
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__staging"} */;
[0m10:46:42.040802 [debug] [ThreadPool]: On list_INCREMENTALETL__processed: show objects in INCREMENTALETL._processed
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__processed"} */;
[0m10:46:42.040802 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:46:42.040802 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:46:42.963507 [debug] [ThreadPool]: SQL status: SUCCESS 0 in 0.912 seconds
[0m10:46:43.001942 [debug] [ThreadPool]: SQL status: SUCCESS 0 in 0.960 seconds
[0m10:46:43.017597 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a6d40cb8-3051-459b-980c-e6f4b1de68a5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000010C452DDEA0>]}
[0m10:46:43.035598 [debug] [Thread-2 (]: Began running node model.sales_pipelines.stg_sales
[0m10:46:43.035598 [info ] [Thread-2 (]: 1 of 2 START sql incremental model _staging.stg_sales .......................... [RUN]
[0m10:46:43.035598 [debug] [Thread-2 (]: Acquiring new snowflake connection 'model.sales_pipelines.stg_sales'
[0m10:46:43.035598 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.stg_sales
[0m10:46:43.067131 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.stg_sales"
[0m10:46:43.067131 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.stg_sales
[0m10:46:43.161322 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.stg_sales"
[0m10:46:43.161322 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:46:43.161322 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: create or replace transient table INCREMENTALETL._staging.stg_sales
    
    
    
    as (

WITH raw_data AS (
    SELECT * 
    FROM INCREMENTALETL.LANDINGZONE.RAW_SALES
),

staged AS (
    SELECT
        -- IDs
        CAST(INVOICENO AS VARCHAR)    AS invoice_no,
        CAST(STOCKCODE AS VARCHAR)    AS stock_code,
        CAST(DESCRIPTION AS VARCHAR)  AS description,

        -- Quantities & Prices
        TRY_CAST(QUANTITY AS INT)      AS quantity,
        TRY_CAST(UNITPRICE AS FLOAT)   AS unit_price,
        TRY_CAST(DISCOUNT AS FLOAT)    AS discount,
        TRY_CAST(SHIPPINGCOST AS FLOAT) AS shipping_cost,
        TRY_CAST(CUSTOMERID AS FLOAT)  AS customer_id,

        -- Convert once
        TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS') AS invoice_date,

        TRIM(UPPER(COUNTRY))            AS country,
        TRIM(UPPER(PAYMENTMETHOD))      AS payment_method,
        TRIM(UPPER(CATEGORY))           AS category,
        TRIM(UPPER(SALESCHANNEL))       AS sales_channel,
        TRIM(UPPER(RETURNSTATUS))       AS return_status,
        TRIM(UPPER(SHIPMENTPROVIDER))   AS shipment_provider,
        TRIM(UPPER(WAREHOUSELOCATION))  AS warehouse_location,
        TRIM(UPPER(ORDERPRIORITY))      AS order_priority,

        CURRENT_TIMESTAMP()             AS stg_loaded_at

    FROM raw_data
),

validated AS (
    SELECT
        *,
        YEAR(invoice_date)  AS invoice_year,
        MONTH(invoice_date) AS invoice_month,

        CASE
            WHEN invoice_date IS NULL           THEN 'INVALID_DATE'
            WHEN quantity IS NULL               THEN 'INVALID_QUANTITY'
            WHEN unit_price IS NULL             THEN 'INVALID_PRICE'
            WHEN shipping_cost < 0              THEN 'NEGATIVE_SHIPPING'
            WHEN discount < 0 OR discount > 1   THEN 'INVALID_DISCOUNT'
            WHEN invoice_no IS NULL             THEN 'NULL_INVOICE'
            ELSE 'VALID'
        END AS data_quality_flag
    FROM staged
)

SELECT *
FROM validated


    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:46:43.161322 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m10:46:45.236500 [debug] [Thread-2 (]: SQL status: SUCCESS 13872 in 2.061 seconds
[0m10:46:45.270336 [debug] [Thread-2 (]: Applying DROP to: INCREMENTALETL._staging.stg_sales__dbt_tmp
[0m10:46:45.282392 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:46:45.282392 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: drop view if exists INCREMENTALETL._staging.stg_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:46:45.463950 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.179 seconds
[0m10:46:45.497586 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a6d40cb8-3051-459b-980c-e6f4b1de68a5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000010C4749CCA0>]}
[0m10:46:45.499595 [info ] [Thread-2 (]: 1 of 2 OK created sql incremental model _staging.stg_sales ..................... [[32mSUCCESS 13872[0m in 2.46s]
[0m10:46:45.499595 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.stg_sales
[0m10:46:45.501977 [debug] [Thread-4 (]: Began running node model.sales_pipelines.processed_sales
[0m10:46:45.501977 [info ] [Thread-4 (]: 2 of 2 START sql incremental model _processed.processed_sales .................. [RUN]
[0m10:46:45.501977 [debug] [Thread-4 (]: Acquiring new snowflake connection 'model.sales_pipelines.processed_sales'
[0m10:46:45.501977 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.processed_sales
[0m10:46:45.509584 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.processed_sales"
[0m10:46:45.509584 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.processed_sales
[0m10:46:45.518349 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.processed_sales"
[0m10:46:45.527654 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:46:45.527654 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: create or replace transient table INCREMENTALETL._processed.processed_sales
    
    
    
    as (

WITH staging AS (
    SELECT * 
    FROM INCREMENTALETL._staging.stg_sales
),

deduped AS (
    SELECT *,
        ROW_NUMBER() OVER (
            PARTITION BY invoice_no, stock_code
            ORDER BY stg_loaded_at DESC
        ) AS row_num
    FROM staging
),

cleaned AS (
    SELECT
        invoice_no,
        stock_code,
        description,
        quantity,
        unit_price,
        discount,
        shipping_cost,
        customer_id,
        invoice_date,
        invoice_year,
        invoice_month,
        country,
        payment_method,
        category,
        sales_channel,
        return_status,
        shipment_provider,
        warehouse_location,
        order_priority,

        -- Derived metrics
        ROUND(quantity * unit_price, 2) AS gross_amount,
        ROUND(quantity * unit_price * (1 - discount), 2) AS net_amount,
        ROUND(
            quantity * unit_price * (1 - discount)
            + COALESCE(shipping_cost, 0),
        2) AS total_amount,

        CASE WHEN return_status = 'RETURNED' THEN TRUE ELSE FALSE END AS is_returned,
        CASE WHEN customer_id IS NULL THEN TRUE ELSE FALSE END AS is_guest_customer,

        stg_loaded_at,
        CURRENT_TIMESTAMP() AS processed_at

    FROM deduped
    WHERE
        row_num = 1
        AND data_quality_flag = 'VALID'
        AND invoice_date IS NOT NULL
)

SELECT *
FROM cleaned


    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:46:45.530354 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m10:46:50.094727 [debug] [Thread-4 (]: SQL status: SUCCESS 13606 in 4.571 seconds
[0m10:46:50.107639 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._processed.processed_sales__dbt_tmp
[0m10:46:50.107639 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:46:50.123684 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: drop view if exists INCREMENTALETL._processed.processed_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:46:50.270117 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.144 seconds
[0m10:46:50.270117 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a6d40cb8-3051-459b-980c-e6f4b1de68a5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000010C4745FCA0>]}
[0m10:46:50.283446 [info ] [Thread-4 (]: 2 of 2 OK created sql incremental model _processed.processed_sales ............. [[32mSUCCESS 13606[0m in 4.77s]
[0m10:46:50.283446 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.processed_sales
[0m10:46:50.283446 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:46:50.299383 [debug] [MainThread]: Connection 'create_INCREMENTALETL__staging' was left open.
[0m10:46:50.299383 [debug] [MainThread]: On create_INCREMENTALETL__staging: Close
[0m10:46:50.649122 [debug] [MainThread]: Connection 'create_INCREMENTALETL__processed' was left open.
[0m10:46:50.649122 [debug] [MainThread]: On create_INCREMENTALETL__processed: Close
[0m10:46:51.081372 [debug] [MainThread]: Connection 'list_INCREMENTALETL__staging' was left open.
[0m10:46:51.083388 [debug] [MainThread]: On list_INCREMENTALETL__staging: Close
[0m10:46:51.382122 [debug] [MainThread]: Connection 'list_INCREMENTALETL__processed' was left open.
[0m10:46:51.382122 [debug] [MainThread]: On list_INCREMENTALETL__processed: Close
[0m10:46:51.785067 [debug] [MainThread]: Connection 'model.sales_pipelines.stg_sales' was left open.
[0m10:46:51.785067 [debug] [MainThread]: On model.sales_pipelines.stg_sales: Close
[0m10:46:52.087186 [debug] [MainThread]: Connection 'model.sales_pipelines.processed_sales' was left open.
[0m10:46:52.089209 [debug] [MainThread]: On model.sales_pipelines.processed_sales: Close
[0m10:46:52.444829 [info ] [MainThread]: 
[0m10:46:52.460703 [info ] [MainThread]: Finished running 2 incremental models in 0 hours 0 minutes and 12.70 seconds (12.70s).
[0m10:46:52.464345 [debug] [MainThread]: Command end result
[0m10:46:52.508400 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m10:46:52.508400 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m10:46:52.524499 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\snowflake-incremental-pipeline\dbt_project\target\run_results.json
[0m10:46:52.524499 [info ] [MainThread]: 
[0m10:46:52.524499 [info ] [MainThread]: [32mCompleted successfully[0m
[0m10:46:52.524499 [info ] [MainThread]: 
[0m10:46:52.524499 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=2
[0m10:46:52.524499 [debug] [MainThread]: Command `dbt run` succeeded at 10:46:52.524499 after 17.01 seconds
[0m10:46:52.524499 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000010C330276A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000010C33AC52A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000010C31EC5FF0>]}
[0m10:46:52.524499 [debug] [MainThread]: Flushing usage events
[0m10:46:53.521446 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:49:31.837591 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015912A27730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015913CA5C30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015913CA79D0>]}


============================== 10:49:31.852422 | f98f1b2a-86c1-4a27-a79e-a2208cec61a1 ==============================
[0m10:49:31.852422 [info ] [MainThread]: Running with dbt=1.11.6
[0m10:49:31.854266 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'D:\\snowflake-incremental-pipeline\\dbt_project', 'invocation_command': 'dbt run', 'use_colors': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'D:\\snowflake-incremental-pipeline\\dbt_project\\logs'}
[0m10:49:32.776856 [debug] [MainThread]: Snowflake adapter: Setting snowflake.connector to ERROR (file logging only)
[0m10:49:32.776856 [debug] [MainThread]: Snowflake adapter: Setting botocore to ERROR (file logging only)
[0m10:49:32.778867 [debug] [MainThread]: Snowflake adapter: Setting boto3 to ERROR (file logging only)
[0m10:49:33.080891 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f98f1b2a-86c1-4a27-a79e-a2208cec61a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015924CCD4B0>]}
[0m10:49:33.161843 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f98f1b2a-86c1-4a27-a79e-a2208cec61a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015913527100>]}
[0m10:49:33.161843 [info ] [MainThread]: Registered adapter: snowflake=1.11.2
[0m10:49:33.684490 [debug] [MainThread]: checksum: ec27062d18352e9cfca161c166fcc73d2201661a23a3cf3c89196a27f32cfcb8, vars: {}, profile: , target: , version: 1.11.6
[0m10:49:33.968595 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m10:49:33.968595 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m10:49:33.968595 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m10:49:34.043528 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f98f1b2a-86c1-4a27-a79e-a2208cec61a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000159263BC130>]}
[0m10:49:34.177651 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m10:49:34.177651 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m10:49:34.209588 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f98f1b2a-86c1-4a27-a79e-a2208cec61a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001592635C700>]}
[0m10:49:34.209588 [info ] [MainThread]: Found 2 models, 6 data tests, 1 source, 522 macros
[0m10:49:34.209588 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f98f1b2a-86c1-4a27-a79e-a2208cec61a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001592635C790>]}
[0m10:49:34.216013 [info ] [MainThread]: 
[0m10:49:34.216013 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m10:49:34.216013 [info ] [MainThread]: 
[0m10:49:34.216013 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m10:49:34.229111 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m10:49:34.241706 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m10:49:34.336377 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m10:49:34.336377 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m10:49:34.336377 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m10:49:34.336377 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m10:49:34.336377 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:49:34.336377 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:49:35.635422 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 1.296 seconds
[0m10:49:35.672722 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 1.334 seconds
[0m10:49:35.672722 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__processed'
[0m10:49:35.688519 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__staging'
[0m10:49:35.736238 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__processed"
[0m10:49:35.736238 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__staging"
[0m10:49:35.736238 [debug] [ThreadPool]: On list_INCREMENTALETL__processed: show objects in INCREMENTALETL._processed
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__processed"} */;
[0m10:49:35.736238 [debug] [ThreadPool]: On list_INCREMENTALETL__staging: show objects in INCREMENTALETL._staging
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__staging"} */;
[0m10:49:35.752027 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:49:35.752027 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:49:36.519696 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.781 seconds
[0m10:49:36.600259 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.851 seconds
[0m10:49:36.600259 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f98f1b2a-86c1-4a27-a79e-a2208cec61a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015924CCC160>]}
[0m10:49:36.616298 [debug] [Thread-2 (]: Began running node model.sales_pipelines.stg_sales
[0m10:49:36.616298 [info ] [Thread-2 (]: 1 of 2 START sql incremental model _staging.stg_sales .......................... [RUN]
[0m10:49:36.635430 [debug] [Thread-2 (]: Acquiring new snowflake connection 'model.sales_pipelines.stg_sales'
[0m10:49:36.639491 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.stg_sales
[0m10:49:36.664846 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.stg_sales"
[0m10:49:36.664846 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.stg_sales
[0m10:49:36.743225 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:49:36.743225 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: create or replace  temporary view INCREMENTALETL._staging.stg_sales__dbt_tmp
  
  
  
  
  as (
    

WITH raw_data AS (
    SELECT * 
    FROM INCREMENTALETL.LANDINGZONE.RAW_SALES
),

staged AS (
    SELECT
        -- IDs
        CAST(INVOICENO AS VARCHAR)    AS invoice_no,
        CAST(STOCKCODE AS VARCHAR)    AS stock_code,
        CAST(DESCRIPTION AS VARCHAR)  AS description,

        -- Quantities & Prices
        TRY_CAST(QUANTITY AS INT)      AS quantity,
        TRY_CAST(UNITPRICE AS FLOAT)   AS unit_price,
        TRY_CAST(DISCOUNT AS FLOAT)    AS discount,
        TRY_CAST(SHIPPINGCOST AS FLOAT) AS shipping_cost,
        TRY_CAST(CUSTOMERID AS FLOAT)  AS customer_id,

        -- Convert once
        TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS') AS invoice_date,

        TRIM(UPPER(COUNTRY))            AS country,
        TRIM(UPPER(PAYMENTMETHOD))      AS payment_method,
        TRIM(UPPER(CATEGORY))           AS category,
        TRIM(UPPER(SALESCHANNEL))       AS sales_channel,
        TRIM(UPPER(RETURNSTATUS))       AS return_status,
        TRIM(UPPER(SHIPMENTPROVIDER))   AS shipment_provider,
        TRIM(UPPER(WAREHOUSELOCATION))  AS warehouse_location,
        TRIM(UPPER(ORDERPRIORITY))      AS order_priority,

        CURRENT_TIMESTAMP()             AS stg_loaded_at

    FROM raw_data
),

validated AS (
    SELECT
        *,
        YEAR(invoice_date)  AS invoice_year,
        MONTH(invoice_date) AS invoice_month,

        CASE
            WHEN invoice_date IS NULL           THEN 'INVALID_DATE'
            WHEN quantity IS NULL               THEN 'INVALID_QUANTITY'
            WHEN unit_price IS NULL             THEN 'INVALID_PRICE'
            WHEN shipping_cost < 0              THEN 'NEGATIVE_SHIPPING'
            WHEN discount < 0 OR discount > 1   THEN 'INVALID_DISCOUNT'
            WHEN invoice_no IS NULL             THEN 'NULL_INVOICE'
            ELSE 'VALID'
        END AS data_quality_flag
    FROM staged
)

SELECT *
FROM validated


WHERE NOT EXISTS (
    SELECT 1
    FROM INCREMENTALETL._staging.stg_sales t
    WHERE t.invoice_year  = validated.invoice_year
      AND t.invoice_month = validated.invoice_month
)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:49:36.743225 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m10:49:37.774287 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 1.017 seconds
[0m10:49:37.806322 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:49:37.806322 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:49:37.983472 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.174 seconds
[0m10:49:37.999296 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:49:37.999296 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:49:38.212645 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.197 seconds
[0m10:49:38.260231 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:49:38.260231 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:49:38.481321 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.233 seconds
[0m10:49:38.608843 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:49:38.608843 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table "INCREMENTALETL"."_STAGING"."STG_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:49:39.221586 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.619 seconds
[0m10:49:39.269741 [debug] [Thread-2 (]: 
    In "INCREMENTALETL"."_STAGING"."STG_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m10:49:39.301621 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.stg_sales"
[0m10:49:39.301621 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:49:39.301621 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:49:39.504160 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.187 seconds
[0m10:49:39.504160 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:49:39.504160 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: merge into INCREMENTALETL._staging.stg_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._staging.stg_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","DATA_QUALITY_FLAG" = DBT_INTERNAL_SOURCE."DATA_QUALITY_FLAG"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "INVOICE_YEAR", "INVOICE_MONTH", "DATA_QUALITY_FLAG")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "INVOICE_YEAR", "INVOICE_MONTH", "DATA_QUALITY_FLAG")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:49:40.435518 [debug] [Thread-2 (]: SQL status: SUCCESS 744 in 0.931 seconds
[0m10:49:40.435518 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:49:40.446885 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:49:40.848855 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.400 seconds
[0m10:49:40.867763 [debug] [Thread-2 (]: Applying DROP to: INCREMENTALETL._staging.stg_sales__dbt_tmp
[0m10:49:40.883901 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:49:40.883901 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: drop view if exists INCREMENTALETL._staging.stg_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:49:41.080722 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.198 seconds
[0m10:49:41.141142 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f98f1b2a-86c1-4a27-a79e-a2208cec61a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000159118ABF40>]}
[0m10:49:41.141142 [info ] [Thread-2 (]: 1 of 2 OK created sql incremental model _staging.stg_sales ..................... [[32mSUCCESS 744[0m in 4.51s]
[0m10:49:41.141142 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.stg_sales
[0m10:49:41.141142 [debug] [Thread-4 (]: Began running node model.sales_pipelines.processed_sales
[0m10:49:41.141142 [info ] [Thread-4 (]: 2 of 2 START sql incremental model _processed.processed_sales .................. [RUN]
[0m10:49:41.141142 [debug] [Thread-4 (]: Acquiring new snowflake connection 'model.sales_pipelines.processed_sales'
[0m10:49:41.141142 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.processed_sales
[0m10:49:41.157032 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.processed_sales"
[0m10:49:41.157032 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.processed_sales
[0m10:49:41.172876 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:49:41.172876 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: create or replace  temporary view INCREMENTALETL._processed.processed_sales__dbt_tmp
  
  
  
  
  as (
    

WITH staging AS (
    SELECT * 
    FROM INCREMENTALETL._staging.stg_sales
),

deduped AS (
    SELECT *,
        ROW_NUMBER() OVER (
            PARTITION BY invoice_no, stock_code
            ORDER BY stg_loaded_at DESC
        ) AS row_num
    FROM staging
),

cleaned AS (
    SELECT
        invoice_no,
        stock_code,
        description,
        quantity,
        unit_price,
        discount,
        shipping_cost,
        customer_id,
        invoice_date,
        invoice_year,
        invoice_month,
        country,
        payment_method,
        category,
        sales_channel,
        return_status,
        shipment_provider,
        warehouse_location,
        order_priority,

        -- Derived metrics
        ROUND(quantity * unit_price, 2) AS gross_amount,
        ROUND(quantity * unit_price * (1 - discount), 2) AS net_amount,
        ROUND(
            quantity * unit_price * (1 - discount)
            + COALESCE(shipping_cost, 0),
        2) AS total_amount,

        CASE WHEN return_status = 'RETURNED' THEN TRUE ELSE FALSE END AS is_returned,
        CASE WHEN customer_id IS NULL THEN TRUE ELSE FALSE END AS is_guest_customer,

        stg_loaded_at,
        CURRENT_TIMESTAMP() AS processed_at

    FROM deduped
    WHERE
        row_num = 1
        AND data_quality_flag = 'VALID'
        AND invoice_date IS NOT NULL
)

SELECT *
FROM cleaned


WHERE NOT EXISTS (
    SELECT 1
    FROM INCREMENTALETL._processed.processed_sales t
    WHERE t.invoice_year  = cleaned.invoice_year
      AND t.invoice_month = cleaned.invoice_month
)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:49:41.172876 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m10:49:41.956155 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.789 seconds
[0m10:49:41.971967 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:49:41.971967 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:49:42.137653 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.163 seconds
[0m10:49:42.165385 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:49:42.165385 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:49:42.390001 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.220 seconds
[0m10:49:42.415986 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:49:42.417994 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:49:42.586296 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.168 seconds
[0m10:49:42.598989 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:49:42.598989 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:49:42.743895 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.132 seconds
[0m10:49:42.775725 [debug] [Thread-4 (]: 
    In "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m10:49:42.775725 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.processed_sales"
[0m10:49:42.775725 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:49:42.775725 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:49:43.004277 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.216 seconds
[0m10:49:43.004277 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:49:43.004277 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: merge into INCREMENTALETL._processed.processed_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._processed.processed_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","GROSS_AMOUNT" = DBT_INTERNAL_SOURCE."GROSS_AMOUNT","NET_AMOUNT" = DBT_INTERNAL_SOURCE."NET_AMOUNT","TOTAL_AMOUNT" = DBT_INTERNAL_SOURCE."TOTAL_AMOUNT","IS_RETURNED" = DBT_INTERNAL_SOURCE."IS_RETURNED","IS_GUEST_CUSTOMER" = DBT_INTERNAL_SOURCE."IS_GUEST_CUSTOMER","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","PROCESSED_AT" = DBT_INTERNAL_SOURCE."PROCESSED_AT"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:49:44.020283 [debug] [Thread-4 (]: SQL status: SUCCESS 728 in 1.011 seconds
[0m10:49:44.020283 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:49:44.020283 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:49:44.325775 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.300 seconds
[0m10:49:44.341801 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._processed.processed_sales__dbt_tmp
[0m10:49:44.341801 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:49:44.357696 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: drop view if exists INCREMENTALETL._processed.processed_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:49:44.534862 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.176 seconds
[0m10:49:44.534862 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f98f1b2a-86c1-4a27-a79e-a2208cec61a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015926BA0BE0>]}
[0m10:49:44.550736 [info ] [Thread-4 (]: 2 of 2 OK created sql incremental model _processed.processed_sales ............. [[32mSUCCESS 728[0m in 3.39s]
[0m10:49:44.550736 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.processed_sales
[0m10:49:44.561703 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:49:44.561703 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m10:49:44.566943 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m10:49:44.871917 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m10:49:44.871917 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m10:49:45.115634 [debug] [MainThread]: Connection 'list_INCREMENTALETL__processed' was left open.
[0m10:49:45.115634 [debug] [MainThread]: On list_INCREMENTALETL__processed: Close
[0m10:49:45.370048 [debug] [MainThread]: Connection 'list_INCREMENTALETL__staging' was left open.
[0m10:49:45.371070 [debug] [MainThread]: On list_INCREMENTALETL__staging: Close
[0m10:49:45.660804 [debug] [MainThread]: Connection 'model.sales_pipelines.stg_sales' was left open.
[0m10:49:45.660804 [debug] [MainThread]: On model.sales_pipelines.stg_sales: Close
[0m10:49:45.966619 [debug] [MainThread]: Connection 'model.sales_pipelines.processed_sales' was left open.
[0m10:49:45.966619 [debug] [MainThread]: On model.sales_pipelines.processed_sales: Close
[0m10:49:46.209535 [info ] [MainThread]: 
[0m10:49:46.223540 [info ] [MainThread]: Finished running 2 incremental models in 0 hours 0 minutes and 11.99 seconds (11.99s).
[0m10:49:46.223540 [debug] [MainThread]: Command end result
[0m10:49:46.287182 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m10:49:46.287182 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m10:49:46.303191 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\snowflake-incremental-pipeline\dbt_project\target\run_results.json
[0m10:49:46.303191 [info ] [MainThread]: 
[0m10:49:46.303191 [info ] [MainThread]: [32mCompleted successfully[0m
[0m10:49:46.303191 [info ] [MainThread]: 
[0m10:49:46.303191 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=2
[0m10:49:46.303191 [debug] [MainThread]: Command `dbt run` succeeded at 10:49:46.303191 after 14.59 seconds
[0m10:49:46.312787 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015912A27730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000159269ECA90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000159269ECB50>]}
[0m10:49:46.312787 [debug] [MainThread]: Flushing usage events
[0m10:49:47.428047 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:49:58.857296 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0B10576A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0B32D1000>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0B32D23B0>]}


============================== 10:49:58.860077 | ad477342-ad7e-47f2-9ea3-fba58d080e1e ==============================
[0m10:49:58.860077 [info ] [MainThread]: Running with dbt=1.11.6
[0m10:49:58.860077 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'profiles_dir': 'D:\\snowflake-incremental-pipeline\\dbt_project', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'use_colors': 'True', 'invocation_command': 'dbt run', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'D:\\snowflake-incremental-pipeline\\dbt_project\\logs'}
[0m10:49:59.816204 [debug] [MainThread]: Snowflake adapter: Setting snowflake.connector to ERROR (file logging only)
[0m10:49:59.816204 [debug] [MainThread]: Snowflake adapter: Setting botocore to ERROR (file logging only)
[0m10:49:59.830978 [debug] [MainThread]: Snowflake adapter: Setting boto3 to ERROR (file logging only)
[0m10:50:00.130192 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ad477342-ad7e-47f2-9ea3-fba58d080e1e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0B2AC12A0>]}
[0m10:50:00.208572 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ad477342-ad7e-47f2-9ea3-fba58d080e1e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0B336A890>]}
[0m10:50:00.208572 [info ] [MainThread]: Registered adapter: snowflake=1.11.2
[0m10:50:00.746872 [debug] [MainThread]: checksum: ec27062d18352e9cfca161c166fcc73d2201661a23a3cf3c89196a27f32cfcb8, vars: {}, profile: , target: , version: 1.11.6
[0m10:50:00.969191 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m10:50:00.969191 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m10:50:00.969191 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m10:50:01.048387 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ad477342-ad7e-47f2-9ea3-fba58d080e1e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0C59CC130>]}
[0m10:50:01.193962 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m10:50:01.193962 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m10:50:01.206937 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ad477342-ad7e-47f2-9ea3-fba58d080e1e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0C596CCD0>]}
[0m10:50:01.206937 [info ] [MainThread]: Found 2 models, 6 data tests, 1 source, 522 macros
[0m10:50:01.206937 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ad477342-ad7e-47f2-9ea3-fba58d080e1e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0C596CC70>]}
[0m10:50:01.206937 [info ] [MainThread]: 
[0m10:50:01.222862 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m10:50:01.222862 [info ] [MainThread]: 
[0m10:50:01.224616 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m10:50:01.229360 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m10:50:01.238902 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m10:50:01.350633 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m10:50:01.350633 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m10:50:01.350633 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m10:50:01.350633 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m10:50:01.350633 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:50:01.350633 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:50:02.218057 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 0.865 seconds
[0m10:50:02.234052 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 0.883 seconds
[0m10:50:02.249870 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__staging'
[0m10:50:02.265265 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__processed'
[0m10:50:02.297339 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__staging"
[0m10:50:02.313185 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__processed"
[0m10:50:02.313185 [debug] [ThreadPool]: On list_INCREMENTALETL__staging: show objects in INCREMENTALETL._staging
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__staging"} */;
[0m10:50:02.313185 [debug] [ThreadPool]: On list_INCREMENTALETL__processed: show objects in INCREMENTALETL._processed
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__processed"} */;
[0m10:50:02.313185 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:50:02.327767 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:50:03.097110 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.785 seconds
[0m10:50:03.139578 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.812 seconds
[0m10:50:03.151107 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ad477342-ad7e-47f2-9ea3-fba58d080e1e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0C47ABA60>]}
[0m10:50:03.162789 [debug] [Thread-2 (]: Began running node model.sales_pipelines.stg_sales
[0m10:50:03.162789 [info ] [Thread-2 (]: 1 of 2 START sql incremental model _staging.stg_sales .......................... [RUN]
[0m10:50:03.176863 [debug] [Thread-2 (]: Acquiring new snowflake connection 'model.sales_pipelines.stg_sales'
[0m10:50:03.178893 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.stg_sales
[0m10:50:03.218663 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.stg_sales"
[0m10:50:03.220671 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.stg_sales
[0m10:50:03.295600 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:50:03.297606 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: create or replace  temporary view INCREMENTALETL._staging.stg_sales__dbt_tmp
  
  
  
  
  as (
    

WITH raw_data AS (
    SELECT * 
    FROM INCREMENTALETL.LANDINGZONE.RAW_SALES
),

staged AS (
    SELECT
        -- IDs
        CAST(INVOICENO AS VARCHAR)    AS invoice_no,
        CAST(STOCKCODE AS VARCHAR)    AS stock_code,
        CAST(DESCRIPTION AS VARCHAR)  AS description,

        -- Quantities & Prices
        TRY_CAST(QUANTITY AS INT)      AS quantity,
        TRY_CAST(UNITPRICE AS FLOAT)   AS unit_price,
        TRY_CAST(DISCOUNT AS FLOAT)    AS discount,
        TRY_CAST(SHIPPINGCOST AS FLOAT) AS shipping_cost,
        TRY_CAST(CUSTOMERID AS FLOAT)  AS customer_id,

        -- Convert once
        TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS') AS invoice_date,

        TRIM(UPPER(COUNTRY))            AS country,
        TRIM(UPPER(PAYMENTMETHOD))      AS payment_method,
        TRIM(UPPER(CATEGORY))           AS category,
        TRIM(UPPER(SALESCHANNEL))       AS sales_channel,
        TRIM(UPPER(RETURNSTATUS))       AS return_status,
        TRIM(UPPER(SHIPMENTPROVIDER))   AS shipment_provider,
        TRIM(UPPER(WAREHOUSELOCATION))  AS warehouse_location,
        TRIM(UPPER(ORDERPRIORITY))      AS order_priority,

        CURRENT_TIMESTAMP()             AS stg_loaded_at

    FROM raw_data
),

validated AS (
    SELECT
        *,
        YEAR(invoice_date)  AS invoice_year,
        MONTH(invoice_date) AS invoice_month,

        CASE
            WHEN invoice_date IS NULL           THEN 'INVALID_DATE'
            WHEN quantity IS NULL               THEN 'INVALID_QUANTITY'
            WHEN unit_price IS NULL             THEN 'INVALID_PRICE'
            WHEN shipping_cost < 0              THEN 'NEGATIVE_SHIPPING'
            WHEN discount < 0 OR discount > 1   THEN 'INVALID_DISCOUNT'
            WHEN invoice_no IS NULL             THEN 'NULL_INVOICE'
            ELSE 'VALID'
        END AS data_quality_flag
    FROM staged
)

SELECT *
FROM validated


WHERE NOT EXISTS (
    SELECT 1
    FROM INCREMENTALETL._staging.stg_sales t
    WHERE t.invoice_year  = validated.invoice_year
      AND t.invoice_month = validated.invoice_month
)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:50:03.297606 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m10:50:04.311183 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 1.013 seconds
[0m10:50:04.345241 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:50:04.345241 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:50:04.514471 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.171 seconds
[0m10:50:04.541747 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:50:04.541747 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:50:04.714068 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.168 seconds
[0m10:50:04.759897 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:50:04.759897 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:50:04.917487 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.158 seconds
[0m10:50:05.020731 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:50:05.020731 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table "INCREMENTALETL"."_STAGING"."STG_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:50:05.322303 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.288 seconds
[0m10:50:05.365918 [debug] [Thread-2 (]: 
    In "INCREMENTALETL"."_STAGING"."STG_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m10:50:05.403524 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.stg_sales"
[0m10:50:05.407280 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:50:05.407280 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:50:05.616380 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.218 seconds
[0m10:50:05.616380 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:50:05.632263 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: merge into INCREMENTALETL._staging.stg_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._staging.stg_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","DATA_QUALITY_FLAG" = DBT_INTERNAL_SOURCE."DATA_QUALITY_FLAG"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "INVOICE_YEAR", "INVOICE_MONTH", "DATA_QUALITY_FLAG")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "INVOICE_YEAR", "INVOICE_MONTH", "DATA_QUALITY_FLAG")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:50:06.651174 [debug] [Thread-2 (]: SQL status: SUCCESS 720 in 1.014 seconds
[0m10:50:06.651174 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:50:06.651174 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:50:06.936472 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.282 seconds
[0m10:50:06.952494 [debug] [Thread-2 (]: Applying DROP to: INCREMENTALETL._staging.stg_sales__dbt_tmp
[0m10:50:06.968174 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:50:06.968174 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: drop view if exists INCREMENTALETL._staging.stg_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:50:07.191350 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.215 seconds
[0m10:50:07.223279 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ad477342-ad7e-47f2-9ea3-fba58d080e1e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0C4438C10>]}
[0m10:50:07.223279 [info ] [Thread-2 (]: 1 of 2 OK created sql incremental model _staging.stg_sales ..................... [[32mSUCCESS 720[0m in 4.06s]
[0m10:50:07.223279 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.stg_sales
[0m10:50:07.223279 [debug] [Thread-4 (]: Began running node model.sales_pipelines.processed_sales
[0m10:50:07.230175 [info ] [Thread-4 (]: 2 of 2 START sql incremental model _processed.processed_sales .................. [RUN]
[0m10:50:07.230175 [debug] [Thread-4 (]: Acquiring new snowflake connection 'model.sales_pipelines.processed_sales'
[0m10:50:07.230175 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.processed_sales
[0m10:50:07.239297 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.processed_sales"
[0m10:50:07.239297 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.processed_sales
[0m10:50:07.243340 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:50:07.243340 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: create or replace  temporary view INCREMENTALETL._processed.processed_sales__dbt_tmp
  
  
  
  
  as (
    

WITH staging AS (
    SELECT * 
    FROM INCREMENTALETL._staging.stg_sales
),

deduped AS (
    SELECT *,
        ROW_NUMBER() OVER (
            PARTITION BY invoice_no, stock_code
            ORDER BY stg_loaded_at DESC
        ) AS row_num
    FROM staging
),

cleaned AS (
    SELECT
        invoice_no,
        stock_code,
        description,
        quantity,
        unit_price,
        discount,
        shipping_cost,
        customer_id,
        invoice_date,
        invoice_year,
        invoice_month,
        country,
        payment_method,
        category,
        sales_channel,
        return_status,
        shipment_provider,
        warehouse_location,
        order_priority,

        -- Derived metrics
        ROUND(quantity * unit_price, 2) AS gross_amount,
        ROUND(quantity * unit_price * (1 - discount), 2) AS net_amount,
        ROUND(
            quantity * unit_price * (1 - discount)
            + COALESCE(shipping_cost, 0),
        2) AS total_amount,

        CASE WHEN return_status = 'RETURNED' THEN TRUE ELSE FALSE END AS is_returned,
        CASE WHEN customer_id IS NULL THEN TRUE ELSE FALSE END AS is_guest_customer,

        stg_loaded_at,
        CURRENT_TIMESTAMP() AS processed_at

    FROM deduped
    WHERE
        row_num = 1
        AND data_quality_flag = 'VALID'
        AND invoice_date IS NOT NULL
)

SELECT *
FROM cleaned


WHERE NOT EXISTS (
    SELECT 1
    FROM INCREMENTALETL._processed.processed_sales t
    WHERE t.invoice_year  = cleaned.invoice_year
      AND t.invoice_month = cleaned.invoice_month
)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:50:07.243340 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m10:50:08.012410 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.761 seconds
[0m10:50:08.012410 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:50:08.012410 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:50:08.174052 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.153 seconds
[0m10:50:08.185845 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:50:08.201712 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:50:08.491152 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.297 seconds
[0m10:50:08.522823 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:50:08.522823 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:50:08.675010 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.145 seconds
[0m10:50:08.703547 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:50:08.705561 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:50:08.905524 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.197 seconds
[0m10:50:08.935527 [debug] [Thread-4 (]: 
    In "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m10:50:08.943347 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.processed_sales"
[0m10:50:08.951391 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:50:08.951391 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:50:09.113080 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.163 seconds
[0m10:50:09.113080 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:50:09.113080 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: merge into INCREMENTALETL._processed.processed_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._processed.processed_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","GROSS_AMOUNT" = DBT_INTERNAL_SOURCE."GROSS_AMOUNT","NET_AMOUNT" = DBT_INTERNAL_SOURCE."NET_AMOUNT","TOTAL_AMOUNT" = DBT_INTERNAL_SOURCE."TOTAL_AMOUNT","IS_RETURNED" = DBT_INTERNAL_SOURCE."IS_RETURNED","IS_GUEST_CUSTOMER" = DBT_INTERNAL_SOURCE."IS_GUEST_CUSTOMER","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","PROCESSED_AT" = DBT_INTERNAL_SOURCE."PROCESSED_AT"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:50:10.225615 [debug] [Thread-4 (]: SQL status: SUCCESS 712 in 1.110 seconds
[0m10:50:10.225615 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:50:10.225615 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:50:10.536247 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.303 seconds
[0m10:50:10.548988 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._processed.processed_sales__dbt_tmp
[0m10:50:10.564845 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:50:10.564845 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: drop view if exists INCREMENTALETL._processed.processed_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:50:10.774162 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.202 seconds
[0m10:50:10.774162 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ad477342-ad7e-47f2-9ea3-fba58d080e1e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0C61B17B0>]}
[0m10:50:10.789946 [info ] [Thread-4 (]: 2 of 2 OK created sql incremental model _processed.processed_sales ............. [[32mSUCCESS 712[0m in 3.54s]
[0m10:50:10.789946 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.processed_sales
[0m10:50:10.801392 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:50:10.801392 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m10:50:10.807347 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m10:50:11.045848 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m10:50:11.048457 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m10:50:11.306143 [debug] [MainThread]: Connection 'list_INCREMENTALETL__staging' was left open.
[0m10:50:11.306143 [debug] [MainThread]: On list_INCREMENTALETL__staging: Close
[0m10:50:11.546974 [debug] [MainThread]: Connection 'list_INCREMENTALETL__processed' was left open.
[0m10:50:11.546974 [debug] [MainThread]: On list_INCREMENTALETL__processed: Close
[0m10:50:11.884174 [debug] [MainThread]: Connection 'model.sales_pipelines.stg_sales' was left open.
[0m10:50:11.886197 [debug] [MainThread]: On model.sales_pipelines.stg_sales: Close
[0m10:50:12.187167 [debug] [MainThread]: Connection 'model.sales_pipelines.processed_sales' was left open.
[0m10:50:12.187167 [debug] [MainThread]: On model.sales_pipelines.processed_sales: Close
[0m10:50:12.428169 [info ] [MainThread]: 
[0m10:50:12.444355 [info ] [MainThread]: Finished running 2 incremental models in 0 hours 0 minutes and 11.20 seconds (11.20s).
[0m10:50:12.444355 [debug] [MainThread]: Command end result
[0m10:50:12.511473 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m10:50:12.511473 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m10:50:12.528926 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\snowflake-incremental-pipeline\dbt_project\target\run_results.json
[0m10:50:12.528926 [info ] [MainThread]: 
[0m10:50:12.528926 [info ] [MainThread]: [32mCompleted successfully[0m
[0m10:50:12.528926 [info ] [MainThread]: 
[0m10:50:12.528926 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=2
[0m10:50:12.528926 [debug] [MainThread]: Command `dbt run` succeeded at 10:50:12.528926 after 13.80 seconds
[0m10:50:12.528926 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0B10576A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0B23EDC60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0C596D540>]}
[0m10:50:12.528926 [debug] [MainThread]: Flushing usage events
[0m10:50:13.637613 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:50:32.442599 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023F9FA27670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023FA1CA3340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023FA1CA29B0>]}


============================== 10:50:32.442599 | 7483d62c-96c4-44cd-8a58-c2cfbd244492 ==============================
[0m10:50:32.442599 [info ] [MainThread]: Running with dbt=1.11.6
[0m10:50:32.454176 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'D:\\snowflake-incremental-pipeline\\dbt_project', 'indirect_selection': 'eager', 'use_colors': 'True', 'invocation_command': 'dbt run', 'use_experimental_parser': 'False', 'log_path': 'D:\\snowflake-incremental-pipeline\\dbt_project\\logs'}
[0m10:50:33.476291 [debug] [MainThread]: Snowflake adapter: Setting snowflake.connector to ERROR (file logging only)
[0m10:50:33.476291 [debug] [MainThread]: Snowflake adapter: Setting botocore to ERROR (file logging only)
[0m10:50:33.476291 [debug] [MainThread]: Snowflake adapter: Setting boto3 to ERROR (file logging only)
[0m10:50:33.777152 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7483d62c-96c4-44cd-8a58-c2cfbd244492', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023FA1B693F0>]}
[0m10:50:33.856635 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7483d62c-96c4-44cd-8a58-c2cfbd244492', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023FA1CA3010>]}
[0m10:50:33.872420 [info ] [MainThread]: Registered adapter: snowflake=1.11.2
[0m10:50:34.380672 [debug] [MainThread]: checksum: ec27062d18352e9cfca161c166fcc73d2201661a23a3cf3c89196a27f32cfcb8, vars: {}, profile: , target: , version: 1.11.6
[0m10:50:34.618242 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m10:50:34.618242 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m10:50:34.618242 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m10:50:34.681959 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7483d62c-96c4-44cd-8a58-c2cfbd244492', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023FB43A0130>]}
[0m10:50:34.824496 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m10:50:34.824496 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m10:50:34.856609 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7483d62c-96c4-44cd-8a58-c2cfbd244492', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023FB433C940>]}
[0m10:50:34.856609 [info ] [MainThread]: Found 2 models, 6 data tests, 1 source, 522 macros
[0m10:50:34.861700 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7483d62c-96c4-44cd-8a58-c2cfbd244492', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023FB433CA00>]}
[0m10:50:34.861700 [info ] [MainThread]: 
[0m10:50:34.861700 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m10:50:34.861700 [info ] [MainThread]: 
[0m10:50:34.868124 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m10:50:34.872441 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m10:50:34.888163 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m10:50:34.966990 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m10:50:34.966990 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m10:50:34.966990 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m10:50:34.982745 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m10:50:34.983924 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:50:34.984301 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:50:36.967324 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 1.994 seconds
[0m10:50:36.967324 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 1.997 seconds
[0m10:50:36.983311 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__staging'
[0m10:50:36.999090 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__processed'
[0m10:50:37.020813 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__staging"
[0m10:50:37.028400 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__processed"
[0m10:50:37.028400 [debug] [ThreadPool]: On list_INCREMENTALETL__staging: show objects in INCREMENTALETL._staging
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__staging"} */;
[0m10:50:37.030743 [debug] [ThreadPool]: On list_INCREMENTALETL__processed: show objects in INCREMENTALETL._processed
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__processed"} */;
[0m10:50:37.030743 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:50:37.030743 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:50:37.815999 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.782 seconds
[0m10:50:37.825887 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.797 seconds
[0m10:50:37.831858 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7483d62c-96c4-44cd-8a58-c2cfbd244492', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023F9E0570A0>]}
[0m10:50:37.855962 [debug] [Thread-2 (]: Began running node model.sales_pipelines.stg_sales
[0m10:50:37.860972 [info ] [Thread-2 (]: 1 of 2 START sql incremental model _staging.stg_sales .......................... [RUN]
[0m10:50:37.865951 [debug] [Thread-2 (]: Acquiring new snowflake connection 'model.sales_pipelines.stg_sales'
[0m10:50:37.865951 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.stg_sales
[0m10:50:37.911042 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.stg_sales"
[0m10:50:37.911588 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.stg_sales
[0m10:50:37.974254 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:50:37.974254 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: create or replace  temporary view INCREMENTALETL._staging.stg_sales__dbt_tmp
  
  
  
  
  as (
    

WITH raw_data AS (
    SELECT * 
    FROM INCREMENTALETL.LANDINGZONE.RAW_SALES
),

staged AS (
    SELECT
        -- IDs
        CAST(INVOICENO AS VARCHAR)    AS invoice_no,
        CAST(STOCKCODE AS VARCHAR)    AS stock_code,
        CAST(DESCRIPTION AS VARCHAR)  AS description,

        -- Quantities & Prices
        TRY_CAST(QUANTITY AS INT)      AS quantity,
        TRY_CAST(UNITPRICE AS FLOAT)   AS unit_price,
        TRY_CAST(DISCOUNT AS FLOAT)    AS discount,
        TRY_CAST(SHIPPINGCOST AS FLOAT) AS shipping_cost,
        TRY_CAST(CUSTOMERID AS FLOAT)  AS customer_id,

        -- Convert once
        TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS') AS invoice_date,

        TRIM(UPPER(COUNTRY))            AS country,
        TRIM(UPPER(PAYMENTMETHOD))      AS payment_method,
        TRIM(UPPER(CATEGORY))           AS category,
        TRIM(UPPER(SALESCHANNEL))       AS sales_channel,
        TRIM(UPPER(RETURNSTATUS))       AS return_status,
        TRIM(UPPER(SHIPMENTPROVIDER))   AS shipment_provider,
        TRIM(UPPER(WAREHOUSELOCATION))  AS warehouse_location,
        TRIM(UPPER(ORDERPRIORITY))      AS order_priority,

        CURRENT_TIMESTAMP()             AS stg_loaded_at

    FROM raw_data
),

validated AS (
    SELECT
        *,
        YEAR(invoice_date)  AS invoice_year,
        MONTH(invoice_date) AS invoice_month,

        CASE
            WHEN invoice_date IS NULL           THEN 'INVALID_DATE'
            WHEN quantity IS NULL               THEN 'INVALID_QUANTITY'
            WHEN unit_price IS NULL             THEN 'INVALID_PRICE'
            WHEN shipping_cost < 0              THEN 'NEGATIVE_SHIPPING'
            WHEN discount < 0 OR discount > 1   THEN 'INVALID_DISCOUNT'
            WHEN invoice_no IS NULL             THEN 'NULL_INVOICE'
            ELSE 'VALID'
        END AS data_quality_flag
    FROM staged
)

SELECT *
FROM validated


WHERE NOT EXISTS (
    SELECT 1
    FROM INCREMENTALETL._staging.stg_sales t
    WHERE t.invoice_year  = validated.invoice_year
      AND t.invoice_month = validated.invoice_month
)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:50:37.990349 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m10:50:38.867402 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.877 seconds
[0m10:50:38.901455 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:50:38.901455 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:50:39.050710 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.149 seconds
[0m10:50:39.074309 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:50:39.074309 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:50:39.235128 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.156 seconds
[0m10:50:39.284734 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:50:39.284734 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:50:39.438576 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.157 seconds
[0m10:50:39.547324 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:50:39.547324 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table "INCREMENTALETL"."_STAGING"."STG_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:50:39.689116 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.132 seconds
[0m10:50:39.732986 [debug] [Thread-2 (]: 
    In "INCREMENTALETL"."_STAGING"."STG_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m10:50:39.772363 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.stg_sales"
[0m10:50:39.776380 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:50:39.776380 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:50:39.929687 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.163 seconds
[0m10:50:39.929687 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:50:39.945355 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: merge into INCREMENTALETL._staging.stg_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._staging.stg_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","DATA_QUALITY_FLAG" = DBT_INTERNAL_SOURCE."DATA_QUALITY_FLAG"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "INVOICE_YEAR", "INVOICE_MONTH", "DATA_QUALITY_FLAG")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "INVOICE_YEAR", "INVOICE_MONTH", "DATA_QUALITY_FLAG")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:50:40.873197 [debug] [Thread-2 (]: SQL status: SUCCESS 744 in 0.923 seconds
[0m10:50:40.873197 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:50:40.875204 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:50:41.173185 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.296 seconds
[0m10:50:41.197087 [debug] [Thread-2 (]: Applying DROP to: INCREMENTALETL._staging.stg_sales__dbt_tmp
[0m10:50:41.197087 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:50:41.210877 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: drop view if exists INCREMENTALETL._staging.stg_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:50:41.398235 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.186 seconds
[0m10:50:41.442439 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7483d62c-96c4-44cd-8a58-c2cfbd244492', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023FB2EA75E0>]}
[0m10:50:41.442439 [info ] [Thread-2 (]: 1 of 2 OK created sql incremental model _staging.stg_sales ..................... [[32mSUCCESS 744[0m in 3.58s]
[0m10:50:41.442439 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.stg_sales
[0m10:50:41.458132 [debug] [Thread-4 (]: Began running node model.sales_pipelines.processed_sales
[0m10:50:41.458132 [info ] [Thread-4 (]: 2 of 2 START sql incremental model _processed.processed_sales .................. [RUN]
[0m10:50:41.460236 [debug] [Thread-4 (]: Acquiring new snowflake connection 'model.sales_pipelines.processed_sales'
[0m10:50:41.460236 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.processed_sales
[0m10:50:41.460236 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.processed_sales"
[0m10:50:41.460236 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.processed_sales
[0m10:50:41.483267 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:50:41.483267 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: create or replace  temporary view INCREMENTALETL._processed.processed_sales__dbt_tmp
  
  
  
  
  as (
    

WITH staging AS (
    SELECT * 
    FROM INCREMENTALETL._staging.stg_sales
),

deduped AS (
    SELECT *,
        ROW_NUMBER() OVER (
            PARTITION BY invoice_no, stock_code
            ORDER BY stg_loaded_at DESC
        ) AS row_num
    FROM staging
),

cleaned AS (
    SELECT
        invoice_no,
        stock_code,
        description,
        quantity,
        unit_price,
        discount,
        shipping_cost,
        customer_id,
        invoice_date,
        invoice_year,
        invoice_month,
        country,
        payment_method,
        category,
        sales_channel,
        return_status,
        shipment_provider,
        warehouse_location,
        order_priority,

        -- Derived metrics
        ROUND(quantity * unit_price, 2) AS gross_amount,
        ROUND(quantity * unit_price * (1 - discount), 2) AS net_amount,
        ROUND(
            quantity * unit_price * (1 - discount)
            + COALESCE(shipping_cost, 0),
        2) AS total_amount,

        CASE WHEN return_status = 'RETURNED' THEN TRUE ELSE FALSE END AS is_returned,
        CASE WHEN customer_id IS NULL THEN TRUE ELSE FALSE END AS is_guest_customer,

        stg_loaded_at,
        CURRENT_TIMESTAMP() AS processed_at

    FROM deduped
    WHERE
        row_num = 1
        AND data_quality_flag = 'VALID'
        AND invoice_date IS NOT NULL
)

SELECT *
FROM cleaned


WHERE NOT EXISTS (
    SELECT 1
    FROM INCREMENTALETL._processed.processed_sales t
    WHERE t.invoice_year  = cleaned.invoice_year
      AND t.invoice_month = cleaned.invoice_month
)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:50:41.483267 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m10:50:42.698453 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 1.214 seconds
[0m10:50:42.709103 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:50:42.709103 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:50:42.902990 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.182 seconds
[0m10:50:42.918917 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:50:42.918917 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:50:43.112779 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.186 seconds
[0m10:50:43.130989 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:50:43.130989 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:50:43.314980 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.172 seconds
[0m10:50:43.338372 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:50:43.338372 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:50:43.560287 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.217 seconds
[0m10:50:43.580481 [debug] [Thread-4 (]: 
    In "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m10:50:43.596445 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.processed_sales"
[0m10:50:43.600160 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:50:43.600160 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:50:43.808664 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.217 seconds
[0m10:50:43.824794 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:50:43.824794 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: merge into INCREMENTALETL._processed.processed_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._processed.processed_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","GROSS_AMOUNT" = DBT_INTERNAL_SOURCE."GROSS_AMOUNT","NET_AMOUNT" = DBT_INTERNAL_SOURCE."NET_AMOUNT","TOTAL_AMOUNT" = DBT_INTERNAL_SOURCE."TOTAL_AMOUNT","IS_RETURNED" = DBT_INTERNAL_SOURCE."IS_RETURNED","IS_GUEST_CUSTOMER" = DBT_INTERNAL_SOURCE."IS_GUEST_CUSTOMER","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","PROCESSED_AT" = DBT_INTERNAL_SOURCE."PROCESSED_AT"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:50:44.903410 [debug] [Thread-4 (]: SQL status: SUCCESS 726 in 1.076 seconds
[0m10:50:44.903410 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:50:44.903410 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:50:45.180215 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.275 seconds
[0m10:50:45.214805 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._processed.processed_sales__dbt_tmp
[0m10:50:45.218845 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:50:45.220855 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: drop view if exists INCREMENTALETL._processed.processed_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:50:45.467259 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.252 seconds
[0m10:50:45.483265 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7483d62c-96c4-44cd-8a58-c2cfbd244492', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023F9F443C40>]}
[0m10:50:45.483265 [info ] [Thread-4 (]: 2 of 2 OK created sql incremental model _processed.processed_sales ............. [[32mSUCCESS 726[0m in 4.02s]
[0m10:50:45.493470 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.processed_sales
[0m10:50:45.498982 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:50:45.498982 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m10:50:45.498982 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m10:50:45.783883 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m10:50:45.785903 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m10:50:46.187338 [debug] [MainThread]: Connection 'list_INCREMENTALETL__staging' was left open.
[0m10:50:46.190882 [debug] [MainThread]: On list_INCREMENTALETL__staging: Close
[0m10:50:46.605828 [debug] [MainThread]: Connection 'list_INCREMENTALETL__processed' was left open.
[0m10:50:46.605828 [debug] [MainThread]: On list_INCREMENTALETL__processed: Close
[0m10:50:46.844948 [debug] [MainThread]: Connection 'model.sales_pipelines.stg_sales' was left open.
[0m10:50:46.844948 [debug] [MainThread]: On model.sales_pipelines.stg_sales: Close
[0m10:50:47.109868 [debug] [MainThread]: Connection 'model.sales_pipelines.processed_sales' was left open.
[0m10:50:47.113919 [debug] [MainThread]: On model.sales_pipelines.processed_sales: Close
[0m10:50:47.512331 [info ] [MainThread]: 
[0m10:50:47.514355 [info ] [MainThread]: Finished running 2 incremental models in 0 hours 0 minutes and 12.64 seconds (12.64s).
[0m10:50:47.522105 [debug] [MainThread]: Command end result
[0m10:50:47.581659 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m10:50:47.583666 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m10:50:47.593442 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\snowflake-incremental-pipeline\dbt_project\target\run_results.json
[0m10:50:47.595450 [info ] [MainThread]: 
[0m10:50:47.595450 [info ] [MainThread]: [32mCompleted successfully[0m
[0m10:50:47.595450 [info ] [MainThread]: 
[0m10:50:47.598431 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=2
[0m10:50:47.598431 [debug] [MainThread]: Command `dbt run` succeeded at 10:50:47.598431 after 15.30 seconds
[0m10:50:47.601794 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023F9FA27670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023FB2CAEE00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023FB2CAEE60>]}
[0m10:50:47.601794 [debug] [MainThread]: Flushing usage events
[0m10:50:48.833454 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:51:06.553366 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC4ECA76A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC4FF51000>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC4FF523B0>]}


============================== 10:51:06.553366 | 2fd2c949-83d3-4a3e-8c80-30ad08813cbe ==============================
[0m10:51:06.553366 [info ] [MainThread]: Running with dbt=1.11.6
[0m10:51:06.553366 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'profiles_dir': 'D:\\snowflake-incremental-pipeline\\dbt_project', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'indirect_selection': 'eager', 'use_colors': 'True', 'invocation_command': 'dbt run', 'use_experimental_parser': 'False', 'log_path': 'D:\\snowflake-incremental-pipeline\\dbt_project\\logs'}
[0m10:51:07.615324 [debug] [MainThread]: Snowflake adapter: Setting snowflake.connector to ERROR (file logging only)
[0m10:51:07.615324 [debug] [MainThread]: Snowflake adapter: Setting botocore to ERROR (file logging only)
[0m10:51:07.615324 [debug] [MainThread]: Snowflake adapter: Setting boto3 to ERROR (file logging only)
[0m10:51:07.931573 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2fd2c949-83d3-4a3e-8c80-30ad08813cbe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC4F7412A0>]}
[0m10:51:08.011198 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2fd2c949-83d3-4a3e-8c80-30ad08813cbe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC4FFEA890>]}
[0m10:51:08.011198 [info ] [MainThread]: Registered adapter: snowflake=1.11.2
[0m10:51:08.502416 [debug] [MainThread]: checksum: ec27062d18352e9cfca161c166fcc73d2201661a23a3cf3c89196a27f32cfcb8, vars: {}, profile: , target: , version: 1.11.6
[0m10:51:08.755926 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m10:51:08.755926 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m10:51:08.755926 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m10:51:08.835509 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2fd2c949-83d3-4a3e-8c80-30ad08813cbe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC6264C130>]}
[0m10:51:08.978104 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m10:51:08.978104 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m10:51:08.993951 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2fd2c949-83d3-4a3e-8c80-30ad08813cbe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC625ECCD0>]}
[0m10:51:09.009622 [info ] [MainThread]: Found 2 models, 6 data tests, 1 source, 522 macros
[0m10:51:09.009622 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2fd2c949-83d3-4a3e-8c80-30ad08813cbe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC625ECC70>]}
[0m10:51:09.012920 [info ] [MainThread]: 
[0m10:51:09.012920 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m10:51:09.012920 [info ] [MainThread]: 
[0m10:51:09.012920 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m10:51:09.025600 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m10:51:09.041382 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m10:51:09.152946 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m10:51:09.152946 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m10:51:09.152946 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m10:51:09.152946 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m10:51:09.152946 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:51:09.152946 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:51:10.476838 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 1.315 seconds
[0m10:51:10.852307 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 1.690 seconds
[0m10:51:10.852307 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__processed'
[0m10:51:10.866874 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__staging'
[0m10:51:10.897061 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__processed"
[0m10:51:10.913118 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__staging"
[0m10:51:10.913118 [debug] [ThreadPool]: On list_INCREMENTALETL__processed: show objects in INCREMENTALETL._processed
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__processed"} */;
[0m10:51:10.913118 [debug] [ThreadPool]: On list_INCREMENTALETL__staging: show objects in INCREMENTALETL._staging
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__staging"} */;
[0m10:51:10.913118 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:51:10.913118 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:51:11.848156 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.929 seconds
[0m10:51:12.261560 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 1.339 seconds
[0m10:51:12.267591 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2fd2c949-83d3-4a3e-8c80-30ad08813cbe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC626AF6A0>]}
[0m10:51:12.273397 [debug] [Thread-2 (]: Began running node model.sales_pipelines.stg_sales
[0m10:51:12.278746 [info ] [Thread-2 (]: 1 of 2 START sql incremental model _staging.stg_sales .......................... [RUN]
[0m10:51:12.281862 [debug] [Thread-2 (]: Acquiring new snowflake connection 'model.sales_pipelines.stg_sales'
[0m10:51:12.281862 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.stg_sales
[0m10:51:12.314593 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.stg_sales"
[0m10:51:12.314593 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.stg_sales
[0m10:51:12.407425 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:51:12.407425 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: create or replace  temporary view INCREMENTALETL._staging.stg_sales__dbt_tmp
  
  
  
  
  as (
    

WITH raw_data AS (
    SELECT * 
    FROM INCREMENTALETL.LANDINGZONE.RAW_SALES
),

staged AS (
    SELECT
        -- IDs
        CAST(INVOICENO AS VARCHAR)    AS invoice_no,
        CAST(STOCKCODE AS VARCHAR)    AS stock_code,
        CAST(DESCRIPTION AS VARCHAR)  AS description,

        -- Quantities & Prices
        TRY_CAST(QUANTITY AS INT)      AS quantity,
        TRY_CAST(UNITPRICE AS FLOAT)   AS unit_price,
        TRY_CAST(DISCOUNT AS FLOAT)    AS discount,
        TRY_CAST(SHIPPINGCOST AS FLOAT) AS shipping_cost,
        TRY_CAST(CUSTOMERID AS FLOAT)  AS customer_id,

        -- Convert once
        TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS') AS invoice_date,

        TRIM(UPPER(COUNTRY))            AS country,
        TRIM(UPPER(PAYMENTMETHOD))      AS payment_method,
        TRIM(UPPER(CATEGORY))           AS category,
        TRIM(UPPER(SALESCHANNEL))       AS sales_channel,
        TRIM(UPPER(RETURNSTATUS))       AS return_status,
        TRIM(UPPER(SHIPMENTPROVIDER))   AS shipment_provider,
        TRIM(UPPER(WAREHOUSELOCATION))  AS warehouse_location,
        TRIM(UPPER(ORDERPRIORITY))      AS order_priority,

        CURRENT_TIMESTAMP()             AS stg_loaded_at

    FROM raw_data
),

validated AS (
    SELECT
        *,
        YEAR(invoice_date)  AS invoice_year,
        MONTH(invoice_date) AS invoice_month,

        CASE
            WHEN invoice_date IS NULL           THEN 'INVALID_DATE'
            WHEN quantity IS NULL               THEN 'INVALID_QUANTITY'
            WHEN unit_price IS NULL             THEN 'INVALID_PRICE'
            WHEN shipping_cost < 0              THEN 'NEGATIVE_SHIPPING'
            WHEN discount < 0 OR discount > 1   THEN 'INVALID_DISCOUNT'
            WHEN invoice_no IS NULL             THEN 'NULL_INVOICE'
            ELSE 'VALID'
        END AS data_quality_flag
    FROM staged
)

SELECT *
FROM validated


WHERE NOT EXISTS (
    SELECT 1
    FROM INCREMENTALETL._staging.stg_sales t
    WHERE t.invoice_year  = validated.invoice_year
      AND t.invoice_month = validated.invoice_month
)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:51:12.409472 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m10:51:13.346120 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.939 seconds
[0m10:51:13.370471 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:51:13.370471 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:51:13.565346 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.189 seconds
[0m10:51:13.581595 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:51:13.581595 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:51:13.772447 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.188 seconds
[0m10:51:13.820539 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:51:13.822550 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:51:13.968707 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.145 seconds
[0m10:51:14.060121 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:51:14.075291 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table "INCREMENTALETL"."_STAGING"."STG_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:51:14.666128 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.591 seconds
[0m10:51:14.705838 [debug] [Thread-2 (]: 
    In "INCREMENTALETL"."_STAGING"."STG_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m10:51:14.751789 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.stg_sales"
[0m10:51:14.757707 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:51:14.757707 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:51:14.936817 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.192 seconds
[0m10:51:14.952631 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:51:14.952631 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: merge into INCREMENTALETL._staging.stg_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._staging.stg_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","DATA_QUALITY_FLAG" = DBT_INTERNAL_SOURCE."DATA_QUALITY_FLAG"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "INVOICE_YEAR", "INVOICE_MONTH", "DATA_QUALITY_FLAG")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "INVOICE_YEAR", "INVOICE_MONTH", "DATA_QUALITY_FLAG")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:51:15.826059 [debug] [Thread-2 (]: SQL status: SUCCESS 720 in 0.875 seconds
[0m10:51:15.826059 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:51:15.826059 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:51:16.112705 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.285 seconds
[0m10:51:16.136011 [debug] [Thread-2 (]: Applying DROP to: INCREMENTALETL._staging.stg_sales__dbt_tmp
[0m10:51:16.144385 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:51:16.146652 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: drop view if exists INCREMENTALETL._staging.stg_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:51:16.337273 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.193 seconds
[0m10:51:16.385215 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2fd2c949-83d3-4a3e-8c80-30ad08813cbe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC4FFE8EE0>]}
[0m10:51:16.401248 [info ] [Thread-2 (]: 1 of 2 OK created sql incremental model _staging.stg_sales ..................... [[32mSUCCESS 720[0m in 4.10s]
[0m10:51:16.401248 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.stg_sales
[0m10:51:16.403634 [debug] [Thread-4 (]: Began running node model.sales_pipelines.processed_sales
[0m10:51:16.403634 [info ] [Thread-4 (]: 2 of 2 START sql incremental model _processed.processed_sales .................. [RUN]
[0m10:51:16.403634 [debug] [Thread-4 (]: Acquiring new snowflake connection 'model.sales_pipelines.processed_sales'
[0m10:51:16.408205 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.processed_sales
[0m10:51:16.408205 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.processed_sales"
[0m10:51:16.417126 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.processed_sales
[0m10:51:16.417126 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:51:16.417126 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: create or replace  temporary view INCREMENTALETL._processed.processed_sales__dbt_tmp
  
  
  
  
  as (
    

WITH staging AS (
    SELECT * 
    FROM INCREMENTALETL._staging.stg_sales
),

deduped AS (
    SELECT *,
        ROW_NUMBER() OVER (
            PARTITION BY invoice_no, stock_code
            ORDER BY stg_loaded_at DESC
        ) AS row_num
    FROM staging
),

cleaned AS (
    SELECT
        invoice_no,
        stock_code,
        description,
        quantity,
        unit_price,
        discount,
        shipping_cost,
        customer_id,
        invoice_date,
        invoice_year,
        invoice_month,
        country,
        payment_method,
        category,
        sales_channel,
        return_status,
        shipment_provider,
        warehouse_location,
        order_priority,

        -- Derived metrics
        ROUND(quantity * unit_price, 2) AS gross_amount,
        ROUND(quantity * unit_price * (1 - discount), 2) AS net_amount,
        ROUND(
            quantity * unit_price * (1 - discount)
            + COALESCE(shipping_cost, 0),
        2) AS total_amount,

        CASE WHEN return_status = 'RETURNED' THEN TRUE ELSE FALSE END AS is_returned,
        CASE WHEN customer_id IS NULL THEN TRUE ELSE FALSE END AS is_guest_customer,

        stg_loaded_at,
        CURRENT_TIMESTAMP() AS processed_at

    FROM deduped
    WHERE
        row_num = 1
        AND data_quality_flag = 'VALID'
        AND invoice_date IS NOT NULL
)

SELECT *
FROM cleaned


WHERE NOT EXISTS (
    SELECT 1
    FROM INCREMENTALETL._processed.processed_sales t
    WHERE t.invoice_year  = cleaned.invoice_year
      AND t.invoice_month = cleaned.invoice_month
)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:51:16.417126 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m10:51:17.315100 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.887 seconds
[0m10:51:17.316702 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:51:17.332837 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:51:17.484811 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.149 seconds
[0m10:51:17.509572 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:51:17.513338 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:51:17.657508 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.151 seconds
[0m10:51:17.689483 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:51:17.689483 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:51:17.834451 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.141 seconds
[0m10:51:17.850364 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:51:17.850364 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:51:17.991826 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.133 seconds
[0m10:51:18.023574 [debug] [Thread-4 (]: 
    In "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m10:51:18.023574 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.processed_sales"
[0m10:51:18.023574 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:51:18.023574 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:51:18.216341 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.192 seconds
[0m10:51:18.216341 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:51:18.232406 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: merge into INCREMENTALETL._processed.processed_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._processed.processed_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","GROSS_AMOUNT" = DBT_INTERNAL_SOURCE."GROSS_AMOUNT","NET_AMOUNT" = DBT_INTERNAL_SOURCE."NET_AMOUNT","TOTAL_AMOUNT" = DBT_INTERNAL_SOURCE."TOTAL_AMOUNT","IS_RETURNED" = DBT_INTERNAL_SOURCE."IS_RETURNED","IS_GUEST_CUSTOMER" = DBT_INTERNAL_SOURCE."IS_GUEST_CUSTOMER","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","PROCESSED_AT" = DBT_INTERNAL_SOURCE."PROCESSED_AT"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:51:19.355589 [debug] [Thread-4 (]: SQL status: SUCCESS 701 in 1.135 seconds
[0m10:51:19.355589 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:51:19.371608 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:51:19.634166 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.260 seconds
[0m10:51:19.654378 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._processed.processed_sales__dbt_tmp
[0m10:51:19.660194 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:51:19.662212 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: drop view if exists INCREMENTALETL._processed.processed_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:51:19.842209 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.181 seconds
[0m10:51:19.856323 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2fd2c949-83d3-4a3e-8c80-30ad08813cbe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC62E08070>]}
[0m10:51:19.856323 [info ] [Thread-4 (]: 2 of 2 OK created sql incremental model _processed.processed_sales ............. [[32mSUCCESS 701[0m in 3.45s]
[0m10:51:19.863894 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.processed_sales
[0m10:51:19.870510 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:51:19.870510 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m10:51:19.870510 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m10:51:20.158648 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m10:51:20.158648 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m10:51:20.488844 [debug] [MainThread]: Connection 'list_INCREMENTALETL__processed' was left open.
[0m10:51:20.490870 [debug] [MainThread]: On list_INCREMENTALETL__processed: Close
[0m10:51:23.653142 [debug] [MainThread]: Connection 'list_INCREMENTALETL__staging' was left open.
[0m10:51:23.655162 [debug] [MainThread]: On list_INCREMENTALETL__staging: Close
[0m10:51:23.949850 [debug] [MainThread]: Connection 'model.sales_pipelines.stg_sales' was left open.
[0m10:51:23.951871 [debug] [MainThread]: On model.sales_pipelines.stg_sales: Close
[0m10:51:24.635711 [debug] [MainThread]: Connection 'model.sales_pipelines.processed_sales' was left open.
[0m10:51:24.635711 [debug] [MainThread]: On model.sales_pipelines.processed_sales: Close
[0m10:51:24.910519 [info ] [MainThread]: 
[0m10:51:24.910519 [info ] [MainThread]: Finished running 2 incremental models in 0 hours 0 minutes and 15.90 seconds (15.90s).
[0m10:51:24.910519 [debug] [MainThread]: Command end result
[0m10:51:24.974296 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m10:51:24.974296 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m10:51:24.990172 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\snowflake-incremental-pipeline\dbt_project\target\run_results.json
[0m10:51:24.990172 [info ] [MainThread]: 
[0m10:51:24.990172 [info ] [MainThread]: [32mCompleted successfully[0m
[0m10:51:24.990172 [info ] [MainThread]: 
[0m10:51:24.990172 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=2
[0m10:51:24.990172 [debug] [MainThread]: Command `dbt run` succeeded at 10:51:24.990172 after 18.57 seconds
[0m10:51:24.990172 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC4ECA76A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC4C06D360>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC60F5F940>]}
[0m10:51:24.990172 [debug] [MainThread]: Flushing usage events
[0m10:51:25.980289 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:51:40.585031 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001580AF17610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001580C1C2020>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001580C1C3790>]}


============================== 10:51:40.585031 | 15290ead-3dfb-4908-8af3-adb11ac18979 ==============================
[0m10:51:40.585031 [info ] [MainThread]: Running with dbt=1.11.6
[0m10:51:40.597102 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'profiles_dir': 'D:\\snowflake-incremental-pipeline\\dbt_project', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'use_colors': 'True', 'invocation_command': 'dbt run', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'D:\\snowflake-incremental-pipeline\\dbt_project\\logs'}
[0m10:51:41.543678 [debug] [MainThread]: Snowflake adapter: Setting snowflake.connector to ERROR (file logging only)
[0m10:51:41.543678 [debug] [MainThread]: Snowflake adapter: Setting botocore to ERROR (file logging only)
[0m10:51:41.543678 [debug] [MainThread]: Snowflake adapter: Setting boto3 to ERROR (file logging only)
[0m10:51:41.838311 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '15290ead-3dfb-4908-8af3-adb11ac18979', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001580C201210>]}
[0m10:51:41.917184 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '15290ead-3dfb-4908-8af3-adb11ac18979', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001581D249540>]}
[0m10:51:41.917184 [info ] [MainThread]: Registered adapter: snowflake=1.11.2
[0m10:51:42.423946 [debug] [MainThread]: checksum: ec27062d18352e9cfca161c166fcc73d2201661a23a3cf3c89196a27f32cfcb8, vars: {}, profile: , target: , version: 1.11.6
[0m10:51:42.645044 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m10:51:42.645044 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m10:51:42.645044 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m10:51:42.741372 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '15290ead-3dfb-4908-8af3-adb11ac18979', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001581E8BC130>]}
[0m10:51:42.883242 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m10:51:42.883242 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m10:51:42.914781 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '15290ead-3dfb-4908-8af3-adb11ac18979', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001581E85E8F0>]}
[0m10:51:42.914781 [info ] [MainThread]: Found 2 models, 6 data tests, 1 source, 522 macros
[0m10:51:42.914781 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '15290ead-3dfb-4908-8af3-adb11ac18979', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001581E85EA10>]}
[0m10:51:42.920359 [info ] [MainThread]: 
[0m10:51:42.920359 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m10:51:42.920359 [info ] [MainThread]: 
[0m10:51:42.920359 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m10:51:42.930911 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m10:51:42.946604 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m10:51:43.027483 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m10:51:43.027483 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m10:51:43.041997 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m10:51:43.043166 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m10:51:43.043166 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:51:43.044337 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:51:44.787805 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 1.749 seconds
[0m10:51:44.813904 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 1.775 seconds
[0m10:51:44.829164 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__processed'
[0m10:51:44.829164 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__staging'
[0m10:51:44.860878 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__processed"
[0m10:51:44.880818 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__staging"
[0m10:51:44.880818 [debug] [ThreadPool]: On list_INCREMENTALETL__processed: show objects in INCREMENTALETL._processed
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__processed"} */;
[0m10:51:44.883897 [debug] [ThreadPool]: On list_INCREMENTALETL__staging: show objects in INCREMENTALETL._staging
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__staging"} */;
[0m10:51:44.883897 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:51:44.883897 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:51:45.775227 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.891 seconds
[0m10:51:45.777251 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.896 seconds
[0m10:51:45.788869 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '15290ead-3dfb-4908-8af3-adb11ac18979', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001581D1CCD30>]}
[0m10:51:45.809479 [debug] [Thread-2 (]: Began running node model.sales_pipelines.stg_sales
[0m10:51:45.820455 [info ] [Thread-2 (]: 1 of 2 START sql incremental model _staging.stg_sales .......................... [RUN]
[0m10:51:45.822663 [debug] [Thread-2 (]: Acquiring new snowflake connection 'model.sales_pipelines.stg_sales'
[0m10:51:45.822663 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.stg_sales
[0m10:51:45.853705 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.stg_sales"
[0m10:51:45.853705 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.stg_sales
[0m10:51:45.931423 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:51:45.931423 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: create or replace  temporary view INCREMENTALETL._staging.stg_sales__dbt_tmp
  
  
  
  
  as (
    

WITH raw_data AS (
    SELECT * 
    FROM INCREMENTALETL.LANDINGZONE.RAW_SALES
),

staged AS (
    SELECT
        -- IDs
        CAST(INVOICENO AS VARCHAR)    AS invoice_no,
        CAST(STOCKCODE AS VARCHAR)    AS stock_code,
        CAST(DESCRIPTION AS VARCHAR)  AS description,

        -- Quantities & Prices
        TRY_CAST(QUANTITY AS INT)      AS quantity,
        TRY_CAST(UNITPRICE AS FLOAT)   AS unit_price,
        TRY_CAST(DISCOUNT AS FLOAT)    AS discount,
        TRY_CAST(SHIPPINGCOST AS FLOAT) AS shipping_cost,
        TRY_CAST(CUSTOMERID AS FLOAT)  AS customer_id,

        -- Convert once
        TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS') AS invoice_date,

        TRIM(UPPER(COUNTRY))            AS country,
        TRIM(UPPER(PAYMENTMETHOD))      AS payment_method,
        TRIM(UPPER(CATEGORY))           AS category,
        TRIM(UPPER(SALESCHANNEL))       AS sales_channel,
        TRIM(UPPER(RETURNSTATUS))       AS return_status,
        TRIM(UPPER(SHIPMENTPROVIDER))   AS shipment_provider,
        TRIM(UPPER(WAREHOUSELOCATION))  AS warehouse_location,
        TRIM(UPPER(ORDERPRIORITY))      AS order_priority,

        CURRENT_TIMESTAMP()             AS stg_loaded_at

    FROM raw_data
),

validated AS (
    SELECT
        *,
        YEAR(invoice_date)  AS invoice_year,
        MONTH(invoice_date) AS invoice_month,

        CASE
            WHEN invoice_date IS NULL           THEN 'INVALID_DATE'
            WHEN quantity IS NULL               THEN 'INVALID_QUANTITY'
            WHEN unit_price IS NULL             THEN 'INVALID_PRICE'
            WHEN shipping_cost < 0              THEN 'NEGATIVE_SHIPPING'
            WHEN discount < 0 OR discount > 1   THEN 'INVALID_DISCOUNT'
            WHEN invoice_no IS NULL             THEN 'NULL_INVOICE'
            ELSE 'VALID'
        END AS data_quality_flag
    FROM staged
)

SELECT *
FROM validated


WHERE NOT EXISTS (
    SELECT 1
    FROM INCREMENTALETL._staging.stg_sales t
    WHERE t.invoice_year  = validated.invoice_year
      AND t.invoice_month = validated.invoice_month
)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:51:45.931423 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m10:51:46.862828 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.937 seconds
[0m10:51:46.894339 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:51:46.894339 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:51:47.069647 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.168 seconds
[0m10:51:47.069647 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:51:47.069647 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:51:47.294244 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.209 seconds
[0m10:51:47.343362 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:51:47.343362 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:51:47.475448 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.138 seconds
[0m10:51:47.584115 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:51:47.584115 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table "INCREMENTALETL"."_STAGING"."STG_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:51:47.744542 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.163 seconds
[0m10:51:47.792300 [debug] [Thread-2 (]: 
    In "INCREMENTALETL"."_STAGING"."STG_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m10:51:47.828463 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.stg_sales"
[0m10:51:47.828463 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:51:47.828463 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:51:47.999478 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.161 seconds
[0m10:51:48.001500 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:51:48.005294 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: merge into INCREMENTALETL._staging.stg_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._staging.stg_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","DATA_QUALITY_FLAG" = DBT_INTERNAL_SOURCE."DATA_QUALITY_FLAG"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "INVOICE_YEAR", "INVOICE_MONTH", "DATA_QUALITY_FLAG")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "INVOICE_YEAR", "INVOICE_MONTH", "DATA_QUALITY_FLAG")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:51:48.931911 [debug] [Thread-2 (]: SQL status: SUCCESS 744 in 0.923 seconds
[0m10:51:48.931911 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:51:48.931911 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:51:49.347772 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.420 seconds
[0m10:51:49.381479 [debug] [Thread-2 (]: Applying DROP to: INCREMENTALETL._staging.stg_sales__dbt_tmp
[0m10:51:49.395568 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:51:49.395568 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: drop view if exists INCREMENTALETL._staging.stg_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:51:49.572362 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.175 seconds
[0m10:51:49.604007 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '15290ead-3dfb-4908-8af3-adb11ac18979', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001580C258E20>]}
[0m10:51:49.619911 [info ] [Thread-2 (]: 1 of 2 OK created sql incremental model _staging.stg_sales ..................... [[32mSUCCESS 744[0m in 3.78s]
[0m10:51:49.619911 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.stg_sales
[0m10:51:49.619911 [debug] [Thread-4 (]: Began running node model.sales_pipelines.processed_sales
[0m10:51:49.619911 [info ] [Thread-4 (]: 2 of 2 START sql incremental model _processed.processed_sales .................. [RUN]
[0m10:51:49.619911 [debug] [Thread-4 (]: Acquiring new snowflake connection 'model.sales_pipelines.processed_sales'
[0m10:51:49.619911 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.processed_sales
[0m10:51:49.635973 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.processed_sales"
[0m10:51:49.635973 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.processed_sales
[0m10:51:49.651983 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:51:49.651983 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: create or replace  temporary view INCREMENTALETL._processed.processed_sales__dbt_tmp
  
  
  
  
  as (
    

WITH staging AS (
    SELECT * 
    FROM INCREMENTALETL._staging.stg_sales
),

deduped AS (
    SELECT *,
        ROW_NUMBER() OVER (
            PARTITION BY invoice_no, stock_code
            ORDER BY stg_loaded_at DESC
        ) AS row_num
    FROM staging
),

cleaned AS (
    SELECT
        invoice_no,
        stock_code,
        description,
        quantity,
        unit_price,
        discount,
        shipping_cost,
        customer_id,
        invoice_date,
        invoice_year,
        invoice_month,
        country,
        payment_method,
        category,
        sales_channel,
        return_status,
        shipment_provider,
        warehouse_location,
        order_priority,

        -- Derived metrics
        ROUND(quantity * unit_price, 2) AS gross_amount,
        ROUND(quantity * unit_price * (1 - discount), 2) AS net_amount,
        ROUND(
            quantity * unit_price * (1 - discount)
            + COALESCE(shipping_cost, 0),
        2) AS total_amount,

        CASE WHEN return_status = 'RETURNED' THEN TRUE ELSE FALSE END AS is_returned,
        CASE WHEN customer_id IS NULL THEN TRUE ELSE FALSE END AS is_guest_customer,

        stg_loaded_at,
        CURRENT_TIMESTAMP() AS processed_at

    FROM deduped
    WHERE
        row_num = 1
        AND data_quality_flag = 'VALID'
        AND invoice_date IS NOT NULL
)

SELECT *
FROM cleaned


WHERE NOT EXISTS (
    SELECT 1
    FROM INCREMENTALETL._processed.processed_sales t
    WHERE t.invoice_year  = cleaned.invoice_year
      AND t.invoice_month = cleaned.invoice_month
)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:51:49.651983 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m10:51:50.551955 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.892 seconds
[0m10:51:50.565111 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:51:50.569155 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:51:50.735083 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.167 seconds
[0m10:51:50.749066 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:51:50.749066 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:51:50.901820 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.138 seconds
[0m10:51:50.929016 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:51:50.929016 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:51:51.073027 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.140 seconds
[0m10:51:51.086135 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:51:51.102078 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:51:51.256684 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.153 seconds
[0m10:51:51.278340 [debug] [Thread-4 (]: 
    In "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m10:51:51.278340 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.processed_sales"
[0m10:51:51.294348 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:51:51.294348 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:51:51.508265 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.214 seconds
[0m10:51:51.508265 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:51:51.508265 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: merge into INCREMENTALETL._processed.processed_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._processed.processed_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","GROSS_AMOUNT" = DBT_INTERNAL_SOURCE."GROSS_AMOUNT","NET_AMOUNT" = DBT_INTERNAL_SOURCE."NET_AMOUNT","TOTAL_AMOUNT" = DBT_INTERNAL_SOURCE."TOTAL_AMOUNT","IS_RETURNED" = DBT_INTERNAL_SOURCE."IS_RETURNED","IS_GUEST_CUSTOMER" = DBT_INTERNAL_SOURCE."IS_GUEST_CUSTOMER","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","PROCESSED_AT" = DBT_INTERNAL_SOURCE."PROCESSED_AT"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:51:52.592842 [debug] [Thread-4 (]: SQL status: SUCCESS 732 in 1.072 seconds
[0m10:51:52.592842 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:51:52.592842 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:51:52.891923 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.300 seconds
[0m10:51:52.908092 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._processed.processed_sales__dbt_tmp
[0m10:51:52.924105 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:51:52.924105 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: drop view if exists INCREMENTALETL._processed.processed_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:51:53.103065 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.179 seconds
[0m10:51:53.103065 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '15290ead-3dfb-4908-8af3-adb11ac18979', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001581F087A30>]}
[0m10:51:53.103065 [info ] [Thread-4 (]: 2 of 2 OK created sql incremental model _processed.processed_sales ............. [[32mSUCCESS 732[0m in 3.48s]
[0m10:51:53.103065 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.processed_sales
[0m10:51:53.119202 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:51:53.119202 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m10:51:53.119202 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m10:51:53.454291 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m10:51:53.454291 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m10:51:53.760623 [debug] [MainThread]: Connection 'list_INCREMENTALETL__processed' was left open.
[0m10:51:53.760623 [debug] [MainThread]: On list_INCREMENTALETL__processed: Close
[0m10:51:54.195810 [debug] [MainThread]: Connection 'list_INCREMENTALETL__staging' was left open.
[0m10:51:54.195810 [debug] [MainThread]: On list_INCREMENTALETL__staging: Close
[0m10:51:54.584481 [debug] [MainThread]: Connection 'model.sales_pipelines.stg_sales' was left open.
[0m10:51:54.586003 [debug] [MainThread]: On model.sales_pipelines.stg_sales: Close
[0m10:51:54.876733 [debug] [MainThread]: Connection 'model.sales_pipelines.processed_sales' was left open.
[0m10:51:54.892676 [debug] [MainThread]: On model.sales_pipelines.processed_sales: Close
[0m10:51:55.207394 [info ] [MainThread]: 
[0m10:51:55.211443 [info ] [MainThread]: Finished running 2 incremental models in 0 hours 0 minutes and 12.29 seconds (12.29s).
[0m10:51:55.220540 [debug] [MainThread]: Command end result
[0m10:51:55.261435 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m10:51:55.261435 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m10:51:55.277217 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\snowflake-incremental-pipeline\dbt_project\target\run_results.json
[0m10:51:55.277217 [info ] [MainThread]: 
[0m10:51:55.277217 [info ] [MainThread]: [32mCompleted successfully[0m
[0m10:51:55.277217 [info ] [MainThread]: 
[0m10:51:55.277217 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=2
[0m10:51:55.277217 [debug] [MainThread]: Command `dbt run` succeeded at 10:51:55.277217 after 14.84 seconds
[0m10:51:55.289348 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001580AF17610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001580BA4A650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001581E90D510>]}
[0m10:51:55.289348 [debug] [MainThread]: Flushing usage events
[0m10:51:57.024921 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:52:22.432617 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000280193F7700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002801A6A2230>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002801A6A0AF0>]}


============================== 10:52:22.436137 | cd956227-bbf0-4854-b570-64eb315f217d ==============================
[0m10:52:22.436137 [info ] [MainThread]: Running with dbt=1.11.6
[0m10:52:22.436137 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'D:\\snowflake-incremental-pipeline\\dbt_project', 'invocation_command': 'dbt run', 'use_colors': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'D:\\snowflake-incremental-pipeline\\dbt_project\\logs'}
[0m10:52:23.355187 [debug] [MainThread]: Snowflake adapter: Setting snowflake.connector to ERROR (file logging only)
[0m10:52:23.355187 [debug] [MainThread]: Snowflake adapter: Setting botocore to ERROR (file logging only)
[0m10:52:23.355187 [debug] [MainThread]: Snowflake adapter: Setting boto3 to ERROR (file logging only)
[0m10:52:23.688355 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'cd956227-bbf0-4854-b570-64eb315f217d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002801A673370>]}
[0m10:52:23.767814 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'cd956227-bbf0-4854-b570-64eb315f217d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028019E679D0>]}
[0m10:52:23.767814 [info ] [MainThread]: Registered adapter: snowflake=1.11.2
[0m10:52:24.293636 [debug] [MainThread]: checksum: ec27062d18352e9cfca161c166fcc73d2201661a23a3cf3c89196a27f32cfcb8, vars: {}, profile: , target: , version: 1.11.6
[0m10:52:24.531080 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m10:52:24.532321 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m10:52:24.532321 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m10:52:24.625675 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'cd956227-bbf0-4854-b570-64eb315f217d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002802CD9C130>]}
[0m10:52:24.799786 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m10:52:24.799786 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m10:52:24.815452 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'cd956227-bbf0-4854-b570-64eb315f217d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002802CD3C880>]}
[0m10:52:24.815452 [info ] [MainThread]: Found 2 models, 6 data tests, 1 source, 522 macros
[0m10:52:24.831444 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'cd956227-bbf0-4854-b570-64eb315f217d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002802CD3C970>]}
[0m10:52:24.831444 [info ] [MainThread]: 
[0m10:52:24.835541 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m10:52:24.835541 [info ] [MainThread]: 
[0m10:52:24.835541 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m10:52:24.847087 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m10:52:24.862449 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m10:52:24.957860 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m10:52:24.957860 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m10:52:24.957860 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m10:52:24.957860 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m10:52:24.973565 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:52:24.973565 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:52:26.214206 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 1.241 seconds
[0m10:52:26.258859 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 1.285 seconds
[0m10:52:26.265202 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__staging'
[0m10:52:26.265202 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__processed'
[0m10:52:26.286536 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__staging"
[0m10:52:26.292129 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__processed"
[0m10:52:26.292129 [debug] [ThreadPool]: On list_INCREMENTALETL__staging: show objects in INCREMENTALETL._staging
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__staging"} */;
[0m10:52:26.292129 [debug] [ThreadPool]: On list_INCREMENTALETL__processed: show objects in INCREMENTALETL._processed
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__processed"} */;
[0m10:52:26.294136 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:52:26.294136 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:52:27.369547 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 1.077 seconds
[0m10:52:27.376658 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 1.082 seconds
[0m10:52:27.390498 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'cd956227-bbf0-4854-b570-64eb315f217d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002802B6AED10>]}
[0m10:52:27.406169 [debug] [Thread-2 (]: Began running node model.sales_pipelines.stg_sales
[0m10:52:27.406169 [info ] [Thread-2 (]: 1 of 2 START sql incremental model _staging.stg_sales .......................... [RUN]
[0m10:52:27.418078 [debug] [Thread-2 (]: Acquiring new snowflake connection 'model.sales_pipelines.stg_sales'
[0m10:52:27.418078 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.stg_sales
[0m10:52:27.439388 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.stg_sales"
[0m10:52:27.439388 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.stg_sales
[0m10:52:27.533282 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:52:27.533282 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: create or replace  temporary view INCREMENTALETL._staging.stg_sales__dbt_tmp
  
  
  
  
  as (
    

WITH raw_data AS (
    SELECT * 
    FROM INCREMENTALETL.LANDINGZONE.RAW_SALES
),

staged AS (
    SELECT
        -- IDs
        CAST(INVOICENO AS VARCHAR)    AS invoice_no,
        CAST(STOCKCODE AS VARCHAR)    AS stock_code,
        CAST(DESCRIPTION AS VARCHAR)  AS description,

        -- Quantities & Prices
        TRY_CAST(QUANTITY AS INT)      AS quantity,
        TRY_CAST(UNITPRICE AS FLOAT)   AS unit_price,
        TRY_CAST(DISCOUNT AS FLOAT)    AS discount,
        TRY_CAST(SHIPPINGCOST AS FLOAT) AS shipping_cost,
        TRY_CAST(CUSTOMERID AS FLOAT)  AS customer_id,

        -- Convert once
        TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS') AS invoice_date,

        TRIM(UPPER(COUNTRY))            AS country,
        TRIM(UPPER(PAYMENTMETHOD))      AS payment_method,
        TRIM(UPPER(CATEGORY))           AS category,
        TRIM(UPPER(SALESCHANNEL))       AS sales_channel,
        TRIM(UPPER(RETURNSTATUS))       AS return_status,
        TRIM(UPPER(SHIPMENTPROVIDER))   AS shipment_provider,
        TRIM(UPPER(WAREHOUSELOCATION))  AS warehouse_location,
        TRIM(UPPER(ORDERPRIORITY))      AS order_priority,

        CURRENT_TIMESTAMP()             AS stg_loaded_at

    FROM raw_data
),

validated AS (
    SELECT
        *,
        YEAR(invoice_date)  AS invoice_year,
        MONTH(invoice_date) AS invoice_month,

        CASE
            WHEN invoice_date IS NULL           THEN 'INVALID_DATE'
            WHEN quantity IS NULL               THEN 'INVALID_QUANTITY'
            WHEN unit_price IS NULL             THEN 'INVALID_PRICE'
            WHEN shipping_cost < 0              THEN 'NEGATIVE_SHIPPING'
            WHEN discount < 0 OR discount > 1   THEN 'INVALID_DISCOUNT'
            WHEN invoice_no IS NULL             THEN 'NULL_INVOICE'
            ELSE 'VALID'
        END AS data_quality_flag
    FROM staged
)

SELECT *
FROM validated


WHERE NOT EXISTS (
    SELECT 1
    FROM INCREMENTALETL._staging.stg_sales t
    WHERE t.invoice_year  = validated.invoice_year
      AND t.invoice_month = validated.invoice_month
)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:52:27.533282 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m10:52:28.630675 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 1.097 seconds
[0m10:52:28.660607 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:52:28.660607 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:52:28.820748 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.153 seconds
[0m10:52:28.836803 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:52:28.836803 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:52:28.991073 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.142 seconds
[0m10:52:29.028633 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:52:29.028633 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:52:29.216600 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.176 seconds
[0m10:52:29.332690 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:52:29.332690 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table "INCREMENTALETL"."_STAGING"."STG_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:52:29.468882 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.138 seconds
[0m10:52:29.508267 [debug] [Thread-2 (]: 
    In "INCREMENTALETL"."_STAGING"."STG_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m10:52:29.539812 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.stg_sales"
[0m10:52:29.555518 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:52:29.555518 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:52:29.699586 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.155 seconds
[0m10:52:29.715754 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:52:29.715754 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: merge into INCREMENTALETL._staging.stg_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._staging.stg_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","DATA_QUALITY_FLAG" = DBT_INTERNAL_SOURCE."DATA_QUALITY_FLAG"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "INVOICE_YEAR", "INVOICE_MONTH", "DATA_QUALITY_FLAG")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "INVOICE_YEAR", "INVOICE_MONTH", "DATA_QUALITY_FLAG")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:52:30.625154 [debug] [Thread-2 (]: SQL status: SUCCESS 0 in 0.903 seconds
[0m10:52:30.625154 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:52:30.625154 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:52:30.877007 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.250 seconds
[0m10:52:30.908736 [debug] [Thread-2 (]: Applying DROP to: INCREMENTALETL._staging.stg_sales__dbt_tmp
[0m10:52:30.929484 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:52:30.929484 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: drop view if exists INCREMENTALETL._staging.stg_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:52:31.134933 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.206 seconds
[0m10:52:31.181138 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cd956227-bbf0-4854-b570-64eb315f217d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002801A734F40>]}
[0m10:52:31.181138 [info ] [Thread-2 (]: 1 of 2 OK created sql incremental model _staging.stg_sales ..................... [[32mSUCCESS 0[0m in 3.77s]
[0m10:52:31.181138 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.stg_sales
[0m10:52:31.196944 [debug] [Thread-4 (]: Began running node model.sales_pipelines.processed_sales
[0m10:52:31.198133 [info ] [Thread-4 (]: 2 of 2 START sql incremental model _processed.processed_sales .................. [RUN]
[0m10:52:31.198133 [debug] [Thread-4 (]: Acquiring new snowflake connection 'model.sales_pipelines.processed_sales'
[0m10:52:31.200254 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.processed_sales
[0m10:52:31.200254 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.processed_sales"
[0m10:52:31.200254 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.processed_sales
[0m10:52:31.215163 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:52:31.215163 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: create or replace  temporary view INCREMENTALETL._processed.processed_sales__dbt_tmp
  
  
  
  
  as (
    

WITH staging AS (
    SELECT * 
    FROM INCREMENTALETL._staging.stg_sales
),

deduped AS (
    SELECT *,
        ROW_NUMBER() OVER (
            PARTITION BY invoice_no, stock_code
            ORDER BY stg_loaded_at DESC
        ) AS row_num
    FROM staging
),

cleaned AS (
    SELECT
        invoice_no,
        stock_code,
        description,
        quantity,
        unit_price,
        discount,
        shipping_cost,
        customer_id,
        invoice_date,
        invoice_year,
        invoice_month,
        country,
        payment_method,
        category,
        sales_channel,
        return_status,
        shipment_provider,
        warehouse_location,
        order_priority,

        -- Derived metrics
        ROUND(quantity * unit_price, 2) AS gross_amount,
        ROUND(quantity * unit_price * (1 - discount), 2) AS net_amount,
        ROUND(
            quantity * unit_price * (1 - discount)
            + COALESCE(shipping_cost, 0),
        2) AS total_amount,

        CASE WHEN return_status = 'RETURNED' THEN TRUE ELSE FALSE END AS is_returned,
        CASE WHEN customer_id IS NULL THEN TRUE ELSE FALSE END AS is_guest_customer,

        stg_loaded_at,
        CURRENT_TIMESTAMP() AS processed_at

    FROM deduped
    WHERE
        row_num = 1
        AND data_quality_flag = 'VALID'
        AND invoice_date IS NOT NULL
)

SELECT *
FROM cleaned


WHERE NOT EXISTS (
    SELECT 1
    FROM INCREMENTALETL._processed.processed_sales t
    WHERE t.invoice_year  = cleaned.invoice_year
      AND t.invoice_month = cleaned.invoice_month
)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:52:31.215163 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m10:52:32.290121 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 1.070 seconds
[0m10:52:32.290121 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:52:32.290121 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:52:32.476958 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.184 seconds
[0m10:52:32.497271 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:52:32.497271 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:52:32.881578 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.373 seconds
[0m10:52:32.897634 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:52:32.897634 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:52:33.107402 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.198 seconds
[0m10:52:33.121583 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:52:33.121583 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:52:33.309442 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.176 seconds
[0m10:52:33.330967 [debug] [Thread-4 (]: 
    In "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m10:52:33.347094 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.processed_sales"
[0m10:52:33.347094 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:52:33.347094 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:52:33.559300 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.206 seconds
[0m10:52:33.559300 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:52:33.559300 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: merge into INCREMENTALETL._processed.processed_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._processed.processed_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","GROSS_AMOUNT" = DBT_INTERNAL_SOURCE."GROSS_AMOUNT","NET_AMOUNT" = DBT_INTERNAL_SOURCE."NET_AMOUNT","TOTAL_AMOUNT" = DBT_INTERNAL_SOURCE."TOTAL_AMOUNT","IS_RETURNED" = DBT_INTERNAL_SOURCE."IS_RETURNED","IS_GUEST_CUSTOMER" = DBT_INTERNAL_SOURCE."IS_GUEST_CUSTOMER","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","PROCESSED_AT" = DBT_INTERNAL_SOURCE."PROCESSED_AT"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:52:34.543104 [debug] [Thread-4 (]: SQL status: SUCCESS 0 in 0.975 seconds
[0m10:52:34.543104 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:52:34.543104 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:52:35.433731 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.896 seconds
[0m10:52:35.449801 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._processed.processed_sales__dbt_tmp
[0m10:52:35.449801 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:52:35.449801 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: drop view if exists INCREMENTALETL._processed.processed_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:52:35.840573 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.389 seconds
[0m10:52:35.856447 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cd956227-bbf0-4854-b570-64eb315f217d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002802D5CAC50>]}
[0m10:52:35.856447 [info ] [Thread-4 (]: 2 of 2 OK created sql incremental model _processed.processed_sales ............. [[32mSUCCESS 0[0m in 4.66s]
[0m10:52:35.856447 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.processed_sales
[0m10:52:35.877006 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:52:35.877006 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m10:52:35.877006 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m10:52:36.243344 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m10:52:36.243344 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m10:52:36.566050 [debug] [MainThread]: Connection 'list_INCREMENTALETL__staging' was left open.
[0m10:52:36.566050 [debug] [MainThread]: On list_INCREMENTALETL__staging: Close
[0m10:52:36.855516 [debug] [MainThread]: Connection 'list_INCREMENTALETL__processed' was left open.
[0m10:52:36.855516 [debug] [MainThread]: On list_INCREMENTALETL__processed: Close
[0m10:52:37.271951 [debug] [MainThread]: Connection 'model.sales_pipelines.stg_sales' was left open.
[0m10:52:37.288118 [debug] [MainThread]: On model.sales_pipelines.stg_sales: Close
[0m10:52:37.543695 [debug] [MainThread]: Connection 'model.sales_pipelines.processed_sales' was left open.
[0m10:52:37.543695 [debug] [MainThread]: On model.sales_pipelines.processed_sales: Close
[0m10:52:38.103968 [info ] [MainThread]: 
[0m10:52:38.103968 [info ] [MainThread]: Finished running 2 incremental models in 0 hours 0 minutes and 13.27 seconds (13.27s).
[0m10:52:38.120088 [debug] [MainThread]: Command end result
[0m10:52:38.185429 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m10:52:38.187438 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m10:52:38.197473 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\snowflake-incremental-pipeline\dbt_project\target\run_results.json
[0m10:52:38.199223 [info ] [MainThread]: 
[0m10:52:38.199223 [info ] [MainThread]: [32mCompleted successfully[0m
[0m10:52:38.199223 [info ] [MainThread]: 
[0m10:52:38.202417 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=2
[0m10:52:38.202417 [debug] [MainThread]: Command `dbt run` succeeded at 10:52:38.202417 after 15.90 seconds
[0m10:52:38.202417 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000280193F7700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028017E13730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028019EB9780>]}
[0m10:52:38.202417 [debug] [MainThread]: Flushing usage events
[0m10:52:39.530533 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:24:33.774864 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF73693670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF75913340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF759129B0>]}


============================== 11:24:33.778335 | ea726f85-525a-4883-8e72-348096481b0d ==============================
[0m11:24:33.778335 [info ] [MainThread]: Running with dbt=1.11.6
[0m11:24:33.778335 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'D:\\snowflake-incremental-pipeline\\dbt_project', 'indirect_selection': 'eager', 'invocation_command': 'dbt run', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'D:\\snowflake-incremental-pipeline\\dbt_project\\logs'}
[0m11:24:34.769458 [debug] [MainThread]: Snowflake adapter: Setting snowflake.connector to ERROR (file logging only)
[0m11:24:34.769458 [debug] [MainThread]: Snowflake adapter: Setting botocore to ERROR (file logging only)
[0m11:24:34.769458 [debug] [MainThread]: Snowflake adapter: Setting boto3 to ERROR (file logging only)
[0m11:24:35.067822 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ea726f85-525a-4883-8e72-348096481b0d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF06BB52A0>]}
[0m11:24:35.146485 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ea726f85-525a-4883-8e72-348096481b0d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF7576E140>]}
[0m11:24:35.162336 [info ] [MainThread]: Registered adapter: snowflake=1.11.2
[0m11:24:35.678508 [debug] [MainThread]: checksum: ec27062d18352e9cfca161c166fcc73d2201661a23a3cf3c89196a27f32cfcb8, vars: {}, profile: , target: , version: 1.11.6
[0m11:24:35.851010 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m11:24:35.851010 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'ea726f85-525a-4883-8e72-348096481b0d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF7512B880>]}
[0m11:24:38.088342 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ea726f85-525a-4883-8e72-348096481b0d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF06A61600>]}
[0m11:24:38.216287 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m11:24:38.232239 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m11:24:38.248244 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ea726f85-525a-4883-8e72-348096481b0d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF75910FA0>]}
[0m11:24:38.248244 [info ] [MainThread]: Found 6 models, 6 data tests, 1 source, 522 macros
[0m11:24:38.248244 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ea726f85-525a-4883-8e72-348096481b0d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF74747700>]}
[0m11:24:38.248244 [info ] [MainThread]: 
[0m11:24:38.248244 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m11:24:38.248244 [info ] [MainThread]: 
[0m11:24:38.260433 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m11:24:38.270616 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m11:24:38.286760 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m11:24:38.297770 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m11:24:38.378450 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m11:24:38.378450 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m11:24:38.378450 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m11:24:38.378450 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m11:24:38.378450 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m11:24:38.378450 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m11:24:38.378450 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:24:38.378450 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:24:38.385596 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:24:40.715563 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 2.331 seconds
[0m11:24:40.721192 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 2.338 seconds
[0m11:24:40.723201 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 2.339 seconds
[0m11:24:40.729228 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_INCREMENTALETL, now create_INCREMENTALETL__marts)
[0m11:24:40.729228 [debug] [ThreadPool]: Creating schema "database: "INCREMENTALETL"
schema: "_marts"
"
[0m11:24:40.737116 [debug] [ThreadPool]: Using snowflake connection "create_INCREMENTALETL__marts"
[0m11:24:40.740112 [debug] [ThreadPool]: On create_INCREMENTALETL__marts: create schema if not exists INCREMENTALETL._marts
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "create_INCREMENTALETL__marts"} */
[0m11:24:40.961947 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.221 seconds
[0m11:24:40.963957 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__marts'
[0m11:24:40.963957 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__processed'
[0m11:24:40.998805 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__staging'
[0m11:24:41.001274 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__marts"
[0m11:24:41.008281 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__processed"
[0m11:24:41.012307 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__staging"
[0m11:24:41.012307 [debug] [ThreadPool]: On list_INCREMENTALETL__marts: show objects in INCREMENTALETL._marts
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__marts"} */;
[0m11:24:41.017654 [debug] [ThreadPool]: On list_INCREMENTALETL__processed: show objects in INCREMENTALETL._processed
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__processed"} */;
[0m11:24:41.017654 [debug] [ThreadPool]: On list_INCREMENTALETL__staging: show objects in INCREMENTALETL._staging
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__staging"} */;
[0m11:24:41.019396 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:24:41.019396 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:24:41.022141 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:24:42.253332 [debug] [ThreadPool]: SQL status: SUCCESS 0 in 1.233 seconds
[0m11:24:42.406407 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 1.388 seconds
[0m11:24:49.045390 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 8.032 seconds
[0m11:24:49.061296 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ea726f85-525a-4883-8e72-348096481b0d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF74747700>]}
[0m11:24:49.077170 [debug] [Thread-2 (]: Began running node model.sales_pipelines.stg_sales
[0m11:24:49.077170 [info ] [Thread-2 (]: 1 of 6 START sql incremental model _staging.stg_sales .......................... [RUN]
[0m11:24:49.077170 [debug] [Thread-2 (]: Acquiring new snowflake connection 'model.sales_pipelines.stg_sales'
[0m11:24:49.093324 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.stg_sales
[0m11:24:49.140105 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.stg_sales"
[0m11:24:49.142532 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.stg_sales
[0m11:24:49.223487 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:24:49.223487 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: create or replace  temporary view INCREMENTALETL._staging.stg_sales__dbt_tmp
  
  
  
  
  as (
    

WITH raw_data AS (
    SELECT * 
    FROM INCREMENTALETL.LANDINGZONE.RAW_SALES
),

staged AS (
    SELECT
        -- IDs
        CAST(INVOICENO AS VARCHAR)    AS invoice_no,
        CAST(STOCKCODE AS VARCHAR)    AS stock_code,
        CAST(DESCRIPTION AS VARCHAR)  AS description,

        -- Quantities & Prices
        TRY_CAST(QUANTITY AS INT)      AS quantity,
        TRY_CAST(UNITPRICE AS FLOAT)   AS unit_price,
        TRY_CAST(DISCOUNT AS FLOAT)    AS discount,
        TRY_CAST(SHIPPINGCOST AS FLOAT) AS shipping_cost,
        TRY_CAST(CUSTOMERID AS FLOAT)  AS customer_id,

        -- Convert once
        TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS') AS invoice_date,

        TRIM(UPPER(COUNTRY))            AS country,
        TRIM(UPPER(PAYMENTMETHOD))      AS payment_method,
        TRIM(UPPER(CATEGORY))           AS category,
        TRIM(UPPER(SALESCHANNEL))       AS sales_channel,
        TRIM(UPPER(RETURNSTATUS))       AS return_status,
        TRIM(UPPER(SHIPMENTPROVIDER))   AS shipment_provider,
        TRIM(UPPER(WAREHOUSELOCATION))  AS warehouse_location,
        TRIM(UPPER(ORDERPRIORITY))      AS order_priority,

        CURRENT_TIMESTAMP()             AS stg_loaded_at

    FROM raw_data
),

validated AS (
    SELECT
        *,
        YEAR(invoice_date)  AS invoice_year,
        MONTH(invoice_date) AS invoice_month,

        CASE
            WHEN invoice_date IS NULL           THEN 'INVALID_DATE'
            WHEN quantity IS NULL               THEN 'INVALID_QUANTITY'
            WHEN unit_price IS NULL             THEN 'INVALID_PRICE'
            WHEN shipping_cost < 0              THEN 'NEGATIVE_SHIPPING'
            WHEN discount < 0 OR discount > 1   THEN 'INVALID_DISCOUNT'
            WHEN invoice_no IS NULL             THEN 'NULL_INVOICE'
            ELSE 'VALID'
        END AS data_quality_flag
    FROM staged
)

SELECT *
FROM validated


WHERE NOT EXISTS (
    SELECT 1
    FROM INCREMENTALETL._staging.stg_sales t
    WHERE t.invoice_year  = validated.invoice_year
      AND t.invoice_month = validated.invoice_month
)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m11:24:49.223487 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m11:24:50.258376 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 1.040 seconds
[0m11:24:50.295543 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:24:50.306577 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:24:50.451626 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.146 seconds
[0m11:24:50.467679 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:24:50.467679 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:24:50.680163 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.203 seconds
[0m11:24:50.724218 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:24:50.724218 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:24:50.890983 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.158 seconds
[0m11:24:50.907605 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:24:50.917210 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table "INCREMENTALETL"."_STAGING"."STG_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:24:51.107365 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.190 seconds
[0m11:24:51.142474 [debug] [Thread-2 (]: 
    In "INCREMENTALETL"."_STAGING"."STG_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m11:24:51.174258 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.stg_sales"
[0m11:24:51.190015 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:24:51.190015 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m11:24:51.399558 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.207 seconds
[0m11:24:51.399558 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:24:51.399558 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: merge into INCREMENTALETL._staging.stg_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._staging.stg_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","DATA_QUALITY_FLAG" = DBT_INTERNAL_SOURCE."DATA_QUALITY_FLAG"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "INVOICE_YEAR", "INVOICE_MONTH", "DATA_QUALITY_FLAG")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "INVOICE_YEAR", "INVOICE_MONTH", "DATA_QUALITY_FLAG")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m11:24:52.807252 [debug] [Thread-2 (]: SQL status: SUCCESS 0 in 1.412 seconds
[0m11:24:52.823098 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:24:52.823098 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:24:53.075633 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.249 seconds
[0m11:24:53.084651 [debug] [Thread-2 (]: Applying DROP to: INCREMENTALETL._staging.stg_sales__dbt_tmp
[0m11:24:53.084651 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:24:53.084651 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: drop view if exists INCREMENTALETL._staging.stg_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:24:53.294368 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.193 seconds
[0m11:24:53.336739 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ea726f85-525a-4883-8e72-348096481b0d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF759A8E50>]}
[0m11:24:53.336739 [info ] [Thread-2 (]: 1 of 6 OK created sql incremental model _staging.stg_sales ..................... [[32mSUCCESS 0[0m in 4.26s]
[0m11:24:53.336739 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.stg_sales
[0m11:24:53.336739 [debug] [Thread-4 (]: Began running node model.sales_pipelines.processed_sales
[0m11:24:53.336739 [info ] [Thread-4 (]: 2 of 6 START sql incremental model _processed.processed_sales .................. [RUN]
[0m11:24:53.336739 [debug] [Thread-4 (]: Acquiring new snowflake connection 'model.sales_pipelines.processed_sales'
[0m11:24:53.336739 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.processed_sales
[0m11:24:53.355846 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.processed_sales"
[0m11:24:53.358465 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.processed_sales
[0m11:24:53.369423 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:24:53.370079 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: create or replace  temporary view INCREMENTALETL._processed.processed_sales__dbt_tmp
  
  
  
  
  as (
    

WITH staging AS (
    SELECT * 
    FROM INCREMENTALETL._staging.stg_sales
),

deduped AS (
    SELECT *,
        ROW_NUMBER() OVER (
            PARTITION BY invoice_no, stock_code
            ORDER BY stg_loaded_at DESC
        ) AS row_num
    FROM staging
),

cleaned AS (
    SELECT
        invoice_no,
        stock_code,
        description,
        quantity,
        unit_price,
        discount,
        shipping_cost,
        customer_id,
        invoice_date,
        invoice_year,
        invoice_month,
        country,
        payment_method,
        category,
        sales_channel,
        return_status,
        shipment_provider,
        warehouse_location,
        order_priority,

        -- Derived metrics
        ROUND(quantity * unit_price, 2) AS gross_amount,
        ROUND(quantity * unit_price * (1 - discount), 2) AS net_amount,
        ROUND(
            quantity * unit_price * (1 - discount)
            + COALESCE(shipping_cost, 0),
        2) AS total_amount,

        CASE WHEN return_status = 'RETURNED' THEN TRUE ELSE FALSE END AS is_returned,
        CASE WHEN customer_id IS NULL THEN TRUE ELSE FALSE END AS is_guest_customer,

        stg_loaded_at,
        CURRENT_TIMESTAMP() AS processed_at

    FROM deduped
    WHERE
        row_num = 1
        AND data_quality_flag = 'VALID'
        AND invoice_date IS NOT NULL
)

SELECT *
FROM cleaned


WHERE NOT EXISTS (
    SELECT 1
    FROM INCREMENTALETL._processed.processed_sales t
    WHERE t.invoice_year  = cleaned.invoice_year
      AND t.invoice_month = cleaned.invoice_month
)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m11:24:53.372965 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m11:24:54.565587 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 1.201 seconds
[0m11:24:54.581594 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:24:54.581594 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:24:54.773511 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.184 seconds
[0m11:24:54.789389 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:24:54.805223 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:24:54.982585 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.175 seconds
[0m11:24:54.982585 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:24:54.982585 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:24:55.147571 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.155 seconds
[0m11:24:55.157234 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:24:55.157234 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:24:55.397595 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.232 seconds
[0m11:24:55.429147 [debug] [Thread-4 (]: 
    In "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m11:24:55.429147 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.processed_sales"
[0m11:24:55.429147 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:24:55.429147 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m11:24:55.622671 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.180 seconds
[0m11:24:55.623943 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:24:55.623943 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: merge into INCREMENTALETL._processed.processed_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._processed.processed_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","GROSS_AMOUNT" = DBT_INTERNAL_SOURCE."GROSS_AMOUNT","NET_AMOUNT" = DBT_INTERNAL_SOURCE."NET_AMOUNT","TOTAL_AMOUNT" = DBT_INTERNAL_SOURCE."TOTAL_AMOUNT","IS_RETURNED" = DBT_INTERNAL_SOURCE."IS_RETURNED","IS_GUEST_CUSTOMER" = DBT_INTERNAL_SOURCE."IS_GUEST_CUSTOMER","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","PROCESSED_AT" = DBT_INTERNAL_SOURCE."PROCESSED_AT"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m11:24:56.820557 [debug] [Thread-4 (]: SQL status: SUCCESS 0 in 1.199 seconds
[0m11:24:56.820557 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:24:56.836291 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:24:57.117311 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.283 seconds
[0m11:24:57.133339 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._processed.processed_sales__dbt_tmp
[0m11:24:57.133339 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:24:57.133339 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: drop view if exists INCREMENTALETL._processed.processed_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:24:57.365863 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.219 seconds
[0m11:24:57.365863 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ea726f85-525a-4883-8e72-348096481b0d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF08D34910>]}
[0m11:24:57.381956 [info ] [Thread-4 (]: 2 of 6 OK created sql incremental model _processed.processed_sales ............. [[32mSUCCESS 0[0m in 4.03s]
[0m11:24:57.381956 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.processed_sales
[0m11:24:57.394794 [debug] [Thread-3 (]: Began running node model.sales_pipelines.dim_customers
[0m11:24:57.394794 [debug] [Thread-2 (]: Began running node model.sales_pipelines.dim_date
[0m11:24:57.397808 [debug] [Thread-5 (]: Began running node model.sales_pipelines.dim_products
[0m11:24:57.402066 [debug] [Thread-4 (]: Began running node model.sales_pipelines.fact_sales
[0m11:24:57.406170 [info ] [Thread-3 (]: 3 of 6 START sql table model _marts.dim_customers .............................. [RUN]
[0m11:24:57.410014 [info ] [Thread-2 (]: 4 of 6 START sql table model _marts.dim_date ................................... [RUN]
[0m11:24:57.414661 [info ] [Thread-5 (]: 5 of 6 START sql table model _marts.dim_products ............................... [RUN]
[0m11:24:57.419637 [info ] [Thread-4 (]: 6 of 6 START sql incremental model _marts.fact_sales ........................... [RUN]
[0m11:24:57.424628 [debug] [Thread-3 (]: Acquiring new snowflake connection 'model.sales_pipelines.dim_customers'
[0m11:24:57.428351 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.sales_pipelines.stg_sales, now model.sales_pipelines.dim_date)
[0m11:24:57.434511 [debug] [Thread-5 (]: Acquiring new snowflake connection 'model.sales_pipelines.dim_products'
[0m11:24:57.436524 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.sales_pipelines.processed_sales, now model.sales_pipelines.fact_sales)
[0m11:24:57.436524 [debug] [Thread-3 (]: Began compiling node model.sales_pipelines.dim_customers
[0m11:24:57.439996 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.dim_date
[0m11:24:57.442684 [debug] [Thread-5 (]: Began compiling node model.sales_pipelines.dim_products
[0m11:24:57.442684 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.fact_sales
[0m11:24:57.449822 [debug] [Thread-3 (]: Writing injected SQL for node "model.sales_pipelines.dim_customers"
[0m11:24:57.456782 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.dim_date"
[0m11:24:57.464386 [debug] [Thread-5 (]: Writing injected SQL for node "model.sales_pipelines.dim_products"
[0m11:24:57.464386 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.fact_sales"
[0m11:24:57.478035 [debug] [Thread-3 (]: Began executing node model.sales_pipelines.dim_customers
[0m11:24:57.478035 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.dim_date
[0m11:24:57.488637 [debug] [Thread-5 (]: Began executing node model.sales_pipelines.dim_products
[0m11:24:57.513145 [debug] [Thread-3 (]: Writing runtime sql for node "model.sales_pipelines.dim_customers"
[0m11:24:57.513145 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.fact_sales
[0m11:24:57.521227 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.dim_date"
[0m11:24:57.529390 [debug] [Thread-5 (]: Writing runtime sql for node "model.sales_pipelines.dim_products"
[0m11:24:57.529390 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.fact_sales"
[0m11:24:57.546335 [debug] [Thread-3 (]: Using snowflake connection "model.sales_pipelines.dim_customers"
[0m11:24:57.548608 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.dim_date"
[0m11:24:57.548608 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:24:57.548608 [debug] [Thread-5 (]: Using snowflake connection "model.sales_pipelines.dim_products"
[0m11:24:57.548608 [debug] [Thread-3 (]: On model.sales_pipelines.dim_customers: create or replace transient table INCREMENTALETL._marts.dim_customers
    
    
    
    as (

SELECT DISTINCT
    customer_id,
    country,
    payment_method,
    sales_channel,
    is_guest_customer,
    MIN(invoice_date) AS first_purchase_date,
    MAX(invoice_date) AS last_purchase_date,
    COUNT(DISTINCT invoice_no) AS total_orders
FROM INCREMENTALETL._processed.processed_sales
WHERE customer_id IS NOT NULL
GROUP BY
    customer_id,
    country,
    payment_method,
    sales_channel,
    is_guest_customer
    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.dim_customers"} */;
[0m11:24:57.548608 [debug] [Thread-2 (]: On model.sales_pipelines.dim_date: create or replace transient table INCREMENTALETL._marts.dim_date
    
    
    
    as (

SELECT DISTINCT
    invoice_date::DATE AS date_key,
    YEAR(invoice_date)  AS year,
    MONTH(invoice_date) AS month,
    DAY(invoice_date)   AS day,
    DAYNAME(invoice_date) AS day_name,
    WEEK(invoice_date)  AS week_number,
    QUARTER(invoice_date) AS quarter
FROM INCREMENTALETL._processed.processed_sales
    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.dim_date"} */;
[0m11:24:57.548608 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: create or replace transient table INCREMENTALETL._marts.fact_sales
    
    
    
    as (

SELECT
    invoice_no,
    stock_code,
    customer_id,
    invoice_date::DATE AS date_key,

    quantity,
    unit_price,
    discount,
    shipping_cost,

    gross_amount,
    net_amount,
    total_amount,

    is_returned,
    processed_at

FROM INCREMENTALETL._processed.processed_sales


    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */;
[0m11:24:57.548608 [debug] [Thread-5 (]: On model.sales_pipelines.dim_products: create or replace transient table INCREMENTALETL._marts.dim_products
    
    
    
    as (

SELECT DISTINCT
    stock_code,
    description,
    category
FROM INCREMENTALETL._processed.processed_sales
    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.dim_products"} */;
[0m11:24:57.548608 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m11:24:57.566715 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m11:24:58.201546 [debug] [Thread-2 (]: SQL status: SUCCESS 731 in 0.643 seconds
[0m11:24:58.207020 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ea726f85-525a-4883-8e72-348096481b0d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF09D8B700>]}
[0m11:24:58.210073 [info ] [Thread-2 (]: 4 of 6 OK created sql table model _marts.dim_date .............................. [[32mSUCCESS 731[0m in 0.78s]
[0m11:24:58.210073 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.dim_date
[0m11:24:58.238952 [debug] [Thread-4 (]: SQL status: SUCCESS 17205 in 0.676 seconds
[0m11:24:58.242724 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._marts.fact_sales__dbt_tmp
[0m11:24:58.257003 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:24:58.259203 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: drop view if exists INCREMENTALETL._marts.fact_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:24:58.412660 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.151 seconds
[0m11:24:58.424562 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ea726f85-525a-4883-8e72-348096481b0d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF08D3D570>]}
[0m11:24:58.430789 [info ] [Thread-4 (]: 6 of 6 OK created sql incremental model _marts.fact_sales ...................... [[32mSUCCESS 17205[0m in 0.99s]
[0m11:24:58.443892 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.fact_sales
[0m11:24:58.889874 [debug] [Thread-3 (]: SQL status: SUCCESS 15835 in 1.335 seconds
[0m11:24:58.898802 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ea726f85-525a-4883-8e72-348096481b0d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF09DB25F0>]}
[0m11:24:58.898802 [info ] [Thread-3 (]: 3 of 6 OK created sql table model _marts.dim_customers ......................... [[32mSUCCESS 15835[0m in 1.48s]
[0m11:24:58.914727 [debug] [Thread-3 (]: Finished running node model.sales_pipelines.dim_customers
[0m11:24:59.283870 [debug] [Thread-5 (]: SQL status: SUCCESS 14696 in 1.724 seconds
[0m11:24:59.299966 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ea726f85-525a-4883-8e72-348096481b0d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF08D3EB90>]}
[0m11:24:59.299966 [info ] [Thread-5 (]: 5 of 6 OK created sql table model _marts.dim_products .......................... [[32mSUCCESS 14696[0m in 1.87s]
[0m11:24:59.299966 [debug] [Thread-5 (]: Finished running node model.sales_pipelines.dim_products
[0m11:24:59.318979 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:24:59.322540 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m11:24:59.324377 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m11:24:59.777765 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m11:24:59.777765 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m11:25:00.051248 [debug] [MainThread]: Connection 'create_INCREMENTALETL__marts' was left open.
[0m11:25:00.051248 [debug] [MainThread]: On create_INCREMENTALETL__marts: Close
[0m11:25:00.404167 [debug] [MainThread]: Connection 'list_INCREMENTALETL__marts' was left open.
[0m11:25:00.421663 [debug] [MainThread]: On list_INCREMENTALETL__marts: Close
[0m11:25:00.649018 [debug] [MainThread]: Connection 'list_INCREMENTALETL__processed' was left open.
[0m11:25:00.664939 [debug] [MainThread]: On list_INCREMENTALETL__processed: Close
[0m11:25:01.020764 [debug] [MainThread]: Connection 'list_INCREMENTALETL__staging' was left open.
[0m11:25:01.020764 [debug] [MainThread]: On list_INCREMENTALETL__staging: Close
[0m11:25:01.333661 [debug] [MainThread]: Connection 'model.sales_pipelines.dim_date' was left open.
[0m11:25:01.335684 [debug] [MainThread]: On model.sales_pipelines.dim_date: Close
[0m11:25:01.605226 [debug] [MainThread]: Connection 'model.sales_pipelines.fact_sales' was left open.
[0m11:25:01.623508 [debug] [MainThread]: On model.sales_pipelines.fact_sales: Close
[0m11:25:01.946530 [debug] [MainThread]: Connection 'model.sales_pipelines.dim_customers' was left open.
[0m11:25:01.946530 [debug] [MainThread]: On model.sales_pipelines.dim_customers: Close
[0m11:25:02.301550 [debug] [MainThread]: Connection 'model.sales_pipelines.dim_products' was left open.
[0m11:25:02.301550 [debug] [MainThread]: On model.sales_pipelines.dim_products: Close
[0m11:25:02.597823 [info ] [MainThread]: 
[0m11:25:02.597823 [info ] [MainThread]: Finished running 3 incremental models, 3 table models in 0 hours 0 minutes and 24.34 seconds (24.34s).
[0m11:25:02.616938 [debug] [MainThread]: Command end result
[0m11:25:02.674600 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m11:25:02.674600 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m11:25:02.690251 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\snowflake-incremental-pipeline\dbt_project\target\run_results.json
[0m11:25:02.690251 [info ] [MainThread]: 
[0m11:25:02.696275 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:25:02.696275 [info ] [MainThread]: 
[0m11:25:02.698645 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=6
[0m11:25:02.698645 [debug] [MainThread]: Command `dbt run` succeeded at 11:25:02.698645 after 29.05 seconds
[0m11:25:02.701506 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF73693670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF06C254E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF74A2D4E0>]}
[0m11:25:02.701506 [debug] [MainThread]: Flushing usage events
[0m11:25:05.114155 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:37:04.981929 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017BFA2176D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017BFB4C2620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017BFB4C30D0>]}


============================== 11:37:04.981929 | 310c6510-6fd5-449f-97a8-37be64e5c767 ==============================
[0m11:37:04.981929 [info ] [MainThread]: Running with dbt=1.11.6
[0m11:37:04.981929 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'D:\\snowflake-incremental-pipeline\\dbt_project', 'indirect_selection': 'eager', 'invocation_command': 'dbt run', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'D:\\snowflake-incremental-pipeline\\dbt_project\\logs'}
[0m11:37:05.913384 [debug] [MainThread]: Snowflake adapter: Setting snowflake.connector to ERROR (file logging only)
[0m11:37:05.915396 [debug] [MainThread]: Snowflake adapter: Setting botocore to ERROR (file logging only)
[0m11:37:05.915396 [debug] [MainThread]: Snowflake adapter: Setting boto3 to ERROR (file logging only)
[0m11:37:06.212706 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '310c6510-6fd5-449f-97a8-37be64e5c767', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017BF77BEB00>]}
[0m11:37:06.291592 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '310c6510-6fd5-449f-97a8-37be64e5c767', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017BFAC871C0>]}
[0m11:37:06.291592 [info ] [MainThread]: Registered adapter: snowflake=1.11.2
[0m11:37:06.805380 [debug] [MainThread]: checksum: ec27062d18352e9cfca161c166fcc73d2201661a23a3cf3c89196a27f32cfcb8, vars: {}, profile: , target: , version: 1.11.6
[0m11:37:07.073525 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:37:07.073525 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m11:37:07.073525 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:37:07.154643 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '310c6510-6fd5-449f-97a8-37be64e5c767', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017B8E118A60>]}
[0m11:37:07.296145 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m11:37:07.296145 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m11:37:07.312223 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '310c6510-6fd5-449f-97a8-37be64e5c767', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017B8E091600>]}
[0m11:37:07.322372 [info ] [MainThread]: Found 6 models, 6 data tests, 1 source, 522 macros
[0m11:37:07.322372 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '310c6510-6fd5-449f-97a8-37be64e5c767', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017B8E1189A0>]}
[0m11:37:07.324627 [info ] [MainThread]: 
[0m11:37:07.324627 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m11:37:07.328150 [info ] [MainThread]: 
[0m11:37:07.328150 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m11:37:07.338298 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m11:37:07.352476 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m11:37:07.362483 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m11:37:07.442971 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m11:37:07.442971 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m11:37:07.442971 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m11:37:07.442971 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m11:37:07.442971 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m11:37:07.442971 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m11:37:07.442971 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:37:07.442971 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:37:07.442971 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:37:08.882374 [debug] [ThreadPool]: SQL status: SUCCESS 5 in 1.435 seconds
[0m11:37:09.477247 [debug] [ThreadPool]: SQL status: SUCCESS 5 in 2.030 seconds
[0m11:37:09.560085 [debug] [ThreadPool]: SQL status: SUCCESS 5 in 2.107 seconds
[0m11:37:09.573120 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__staging'
[0m11:37:09.587216 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__processed'
[0m11:37:09.619406 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__marts'
[0m11:37:09.623955 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__staging"
[0m11:37:09.623955 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__processed"
[0m11:37:09.635203 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__marts"
[0m11:37:09.635203 [debug] [ThreadPool]: On list_INCREMENTALETL__staging: show objects in INCREMENTALETL._staging
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__staging"} */;
[0m11:37:09.635203 [debug] [ThreadPool]: On list_INCREMENTALETL__processed: show objects in INCREMENTALETL._processed
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__processed"} */;
[0m11:37:09.635203 [debug] [ThreadPool]: On list_INCREMENTALETL__marts: show objects in INCREMENTALETL._marts
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__marts"} */;
[0m11:37:09.647186 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:37:09.648394 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:37:09.648394 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:37:10.418801 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 0.778 seconds
[0m11:37:10.476549 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.828 seconds
[0m11:37:10.582292 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.942 seconds
[0m11:37:10.594606 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '310c6510-6fd5-449f-97a8-37be64e5c767', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017B8E176E90>]}
[0m11:37:10.598628 [debug] [Thread-2 (]: Began running node model.sales_pipelines.stg_sales
[0m11:37:10.598628 [info ] [Thread-2 (]: 1 of 6 START sql incremental model _staging.stg_sales .......................... [RUN]
[0m11:37:10.598628 [debug] [Thread-2 (]: Acquiring new snowflake connection 'model.sales_pipelines.stg_sales'
[0m11:37:10.605467 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.stg_sales
[0m11:37:10.622433 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.stg_sales"
[0m11:37:10.622433 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.stg_sales
[0m11:37:10.716737 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:37:10.716737 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: create or replace  temporary view INCREMENTALETL._staging.stg_sales__dbt_tmp
  
  
  
  
  as (
    

WITH raw_data AS (
    SELECT * 
    FROM INCREMENTALETL.LANDINGZONE.RAW_SALES
),

staged AS (
    SELECT
        -- IDs
        CAST(INVOICENO AS VARCHAR)    AS invoice_no,
        CAST(STOCKCODE AS VARCHAR)    AS stock_code,
        CAST(DESCRIPTION AS VARCHAR)  AS description,

        -- Quantities & Prices
        TRY_CAST(QUANTITY AS INT)      AS quantity,
        TRY_CAST(UNITPRICE AS FLOAT)   AS unit_price,
        TRY_CAST(DISCOUNT AS FLOAT)    AS discount,
        TRY_CAST(SHIPPINGCOST AS FLOAT) AS shipping_cost,
        TRY_CAST(CUSTOMERID AS FLOAT)  AS customer_id,

        -- Convert once
        TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS') AS invoice_date,

        TRIM(UPPER(COUNTRY))            AS country,
        TRIM(UPPER(PAYMENTMETHOD))      AS payment_method,
        TRIM(UPPER(CATEGORY))           AS category,
        TRIM(UPPER(SALESCHANNEL))       AS sales_channel,
        TRIM(UPPER(RETURNSTATUS))       AS return_status,
        TRIM(UPPER(SHIPMENTPROVIDER))   AS shipment_provider,
        TRIM(UPPER(WAREHOUSELOCATION))  AS warehouse_location,
        TRIM(UPPER(ORDERPRIORITY))      AS order_priority,

        CURRENT_TIMESTAMP()             AS stg_loaded_at

    FROM raw_data
),

validated AS (
    SELECT
        *,
        YEAR(invoice_date)  AS invoice_year,
        MONTH(invoice_date) AS invoice_month,

        CASE
            WHEN invoice_date IS NULL           THEN 'INVALID_DATE'
            WHEN quantity IS NULL               THEN 'INVALID_QUANTITY'
            WHEN unit_price IS NULL             THEN 'INVALID_PRICE'
            WHEN shipping_cost < 0              THEN 'NEGATIVE_SHIPPING'
            WHEN discount < 0 OR discount > 1   THEN 'INVALID_DISCOUNT'
            WHEN invoice_no IS NULL             THEN 'NULL_INVOICE'
            ELSE 'VALID'
        END AS data_quality_flag
    FROM staged
)

SELECT *
FROM validated


WHERE NOT EXISTS (
    SELECT 1
    FROM INCREMENTALETL._staging.stg_sales t
    WHERE t.invoice_year  = validated.invoice_year
      AND t.invoice_month = validated.invoice_month
)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m11:37:10.716737 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m11:37:11.549141 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.830 seconds
[0m11:37:11.677003 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:37:11.677003 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:37:11.820874 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.141 seconds
[0m11:37:11.832960 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:37:11.834975 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:37:11.961605 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.128 seconds
[0m11:37:11.993310 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:37:11.993310 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:37:12.138079 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.131 seconds
[0m11:37:12.159138 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:37:12.159138 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table "INCREMENTALETL"."_STAGING"."STG_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:37:12.298470 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.136 seconds
[0m11:37:12.346292 [debug] [Thread-2 (]: 
    In "INCREMENTALETL"."_STAGING"."STG_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m11:37:12.378015 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.stg_sales"
[0m11:37:12.388271 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:37:12.388271 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m11:37:12.662136 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.284 seconds
[0m11:37:12.678277 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:37:12.680309 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: merge into INCREMENTALETL._staging.stg_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._staging.stg_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","DATA_QUALITY_FLAG" = DBT_INTERNAL_SOURCE."DATA_QUALITY_FLAG"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "INVOICE_YEAR", "INVOICE_MONTH", "DATA_QUALITY_FLAG")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "INVOICE_YEAR", "INVOICE_MONTH", "DATA_QUALITY_FLAG")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m11:37:13.896165 [debug] [Thread-2 (]: SQL status: SUCCESS 744 in 1.219 seconds
[0m11:37:13.907963 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:37:13.907963 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:37:14.219769 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.311 seconds
[0m11:37:14.256265 [debug] [Thread-2 (]: Applying DROP to: INCREMENTALETL._staging.stg_sales__dbt_tmp
[0m11:37:14.271593 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:37:14.271593 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: drop view if exists INCREMENTALETL._staging.stg_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:37:14.447567 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.174 seconds
[0m11:37:14.506326 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '310c6510-6fd5-449f-97a8-37be64e5c767', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017B8E999360>]}
[0m11:37:14.506326 [info ] [Thread-2 (]: 1 of 6 OK created sql incremental model _staging.stg_sales ..................... [[32mSUCCESS 744[0m in 3.91s]
[0m11:37:14.506326 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.stg_sales
[0m11:37:14.511508 [debug] [Thread-4 (]: Began running node model.sales_pipelines.processed_sales
[0m11:37:14.513836 [info ] [Thread-4 (]: 2 of 6 START sql incremental model _processed.processed_sales .................. [RUN]
[0m11:37:14.513836 [debug] [Thread-4 (]: Acquiring new snowflake connection 'model.sales_pipelines.processed_sales'
[0m11:37:14.517416 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.processed_sales
[0m11:37:14.530444 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.processed_sales"
[0m11:37:14.532883 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.processed_sales
[0m11:37:14.548175 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:37:14.549418 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: create or replace  temporary view INCREMENTALETL._processed.processed_sales__dbt_tmp
  
  
  
  
  as (
    

WITH staging AS (
    SELECT * 
    FROM INCREMENTALETL._staging.stg_sales
),

deduped AS (
    SELECT *,
        ROW_NUMBER() OVER (
            PARTITION BY invoice_no, stock_code
            ORDER BY stg_loaded_at DESC
        ) AS row_num
    FROM staging
),

cleaned AS (
    SELECT
        invoice_no,
        stock_code,
        description,
        quantity,
        unit_price,
        discount,
        shipping_cost,
        customer_id,
        invoice_date,
        invoice_year,
        invoice_month,
        country,
        payment_method,
        category,
        sales_channel,
        return_status,
        shipment_provider,
        warehouse_location,
        order_priority,

        -- Derived metrics
        ROUND(quantity * unit_price, 2) AS gross_amount,
        ROUND(quantity * unit_price * (1 - discount), 2) AS net_amount,
        ROUND(
            quantity * unit_price * (1 - discount)
            + COALESCE(shipping_cost, 0),
        2) AS total_amount,

        CASE WHEN return_status = 'RETURNED' THEN TRUE ELSE FALSE END AS is_returned,
        CASE WHEN customer_id IS NULL THEN TRUE ELSE FALSE END AS is_guest_customer,

        stg_loaded_at,
        CURRENT_TIMESTAMP() AS processed_at

    FROM deduped
    WHERE
        row_num = 1
        AND data_quality_flag = 'VALID'
        AND invoice_date IS NOT NULL
)

SELECT *
FROM cleaned


WHERE NOT EXISTS (
    SELECT 1
    FROM INCREMENTALETL._processed.processed_sales t
    WHERE t.invoice_year  = cleaned.invoice_year
      AND t.invoice_month = cleaned.invoice_month
)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m11:37:14.551066 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m11:37:15.446101 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.900 seconds
[0m11:37:15.453200 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:37:15.453200 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:37:15.629481 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.164 seconds
[0m11:37:15.645308 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:37:15.645308 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:37:15.774214 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.128 seconds
[0m11:37:15.806459 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:37:15.807867 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:37:16.064225 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.259 seconds
[0m11:37:16.080277 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:37:16.080277 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:37:16.229095 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.134 seconds
[0m11:37:16.246493 [debug] [Thread-4 (]: 
    In "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m11:37:16.250572 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.processed_sales"
[0m11:37:16.259447 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:37:16.259447 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m11:37:16.493058 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.241 seconds
[0m11:37:16.493058 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:37:16.509991 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: merge into INCREMENTALETL._processed.processed_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._processed.processed_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","GROSS_AMOUNT" = DBT_INTERNAL_SOURCE."GROSS_AMOUNT","NET_AMOUNT" = DBT_INTERNAL_SOURCE."NET_AMOUNT","TOTAL_AMOUNT" = DBT_INTERNAL_SOURCE."TOTAL_AMOUNT","IS_RETURNED" = DBT_INTERNAL_SOURCE."IS_RETURNED","IS_GUEST_CUSTOMER" = DBT_INTERNAL_SOURCE."IS_GUEST_CUSTOMER","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","PROCESSED_AT" = DBT_INTERNAL_SOURCE."PROCESSED_AT"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m11:37:17.682716 [debug] [Thread-4 (]: SQL status: SUCCESS 724 in 1.176 seconds
[0m11:37:17.682716 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:37:17.682716 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:37:17.947462 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.258 seconds
[0m11:37:17.958696 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._processed.processed_sales__dbt_tmp
[0m11:37:17.974452 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:37:17.974452 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: drop view if exists INCREMENTALETL._processed.processed_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:37:18.150967 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.175 seconds
[0m11:37:18.150967 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '310c6510-6fd5-449f-97a8-37be64e5c767', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017BF9099720>]}
[0m11:37:18.150967 [info ] [Thread-4 (]: 2 of 6 OK created sql incremental model _processed.processed_sales ............. [[32mSUCCESS 724[0m in 3.64s]
[0m11:37:18.150967 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.processed_sales
[0m11:37:18.150967 [debug] [Thread-3 (]: Began running node model.sales_pipelines.dim_customers
[0m11:37:18.150967 [debug] [Thread-2 (]: Began running node model.sales_pipelines.dim_date
[0m11:37:18.166889 [debug] [Thread-5 (]: Began running node model.sales_pipelines.dim_products
[0m11:37:18.166889 [debug] [Thread-4 (]: Began running node model.sales_pipelines.fact_sales
[0m11:37:18.168074 [info ] [Thread-3 (]: 3 of 6 START sql table model _marts.dim_customers .............................. [RUN]
[0m11:37:18.168074 [info ] [Thread-2 (]: 4 of 6 START sql table model _marts.dim_date ................................... [RUN]
[0m11:37:18.171467 [info ] [Thread-5 (]: 5 of 6 START sql table model _marts.dim_products ............................... [RUN]
[0m11:37:18.172535 [info ] [Thread-4 (]: 6 of 6 START sql incremental model _marts.fact_sales ........................... [RUN]
[0m11:37:18.175318 [debug] [Thread-3 (]: Acquiring new snowflake connection 'model.sales_pipelines.dim_customers'
[0m11:37:18.177235 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.sales_pipelines.stg_sales, now model.sales_pipelines.dim_date)
[0m11:37:18.177235 [debug] [Thread-5 (]: Acquiring new snowflake connection 'model.sales_pipelines.dim_products'
[0m11:37:18.179887 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.sales_pipelines.processed_sales, now model.sales_pipelines.fact_sales)
[0m11:37:18.179887 [debug] [Thread-3 (]: Began compiling node model.sales_pipelines.dim_customers
[0m11:37:18.179887 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.dim_date
[0m11:37:18.185412 [debug] [Thread-5 (]: Began compiling node model.sales_pipelines.dim_products
[0m11:37:18.185412 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.fact_sales
[0m11:37:18.192994 [debug] [Thread-3 (]: Writing injected SQL for node "model.sales_pipelines.dim_customers"
[0m11:37:18.207870 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.dim_date"
[0m11:37:18.207870 [debug] [Thread-5 (]: Writing injected SQL for node "model.sales_pipelines.dim_products"
[0m11:37:18.221494 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.fact_sales"
[0m11:37:18.225388 [debug] [Thread-3 (]: Began executing node model.sales_pipelines.dim_customers
[0m11:37:18.225388 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.dim_date
[0m11:37:18.227902 [debug] [Thread-5 (]: Began executing node model.sales_pipelines.dim_products
[0m11:37:18.260786 [debug] [Thread-3 (]: Writing runtime sql for node "model.sales_pipelines.dim_customers"
[0m11:37:18.260786 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.fact_sales
[0m11:37:18.260786 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.dim_date"
[0m11:37:18.279102 [debug] [Thread-5 (]: Writing runtime sql for node "model.sales_pipelines.dim_products"
[0m11:37:18.287175 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:37:18.287175 [debug] [Thread-3 (]: Using snowflake connection "model.sales_pipelines.dim_customers"
[0m11:37:18.295247 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.dim_date"
[0m11:37:18.295247 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: create or replace  temporary view INCREMENTALETL._marts.fact_sales__dbt_tmp
  
  
  
  
  as (
    

SELECT
    invoice_no,
    stock_code,
    customer_id,
    invoice_date::DATE AS date_key,

    quantity,
    unit_price,
    discount,
    shipping_cost,

    gross_amount,
    net_amount,
    total_amount,

    is_returned,
    processed_at

FROM INCREMENTALETL._processed.processed_sales


WHERE processed_at > (SELECT MAX(processed_at) FROM INCREMENTALETL._marts.fact_sales)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */;
[0m11:37:18.295247 [debug] [Thread-3 (]: On model.sales_pipelines.dim_customers: create or replace transient table INCREMENTALETL._marts.dim_customers
    
    
    
    as (

SELECT DISTINCT
    customer_id,
    country,
    payment_method,
    sales_channel,
    is_guest_customer,
    MIN(invoice_date) AS first_purchase_date,
    MAX(invoice_date) AS last_purchase_date,
    COUNT(DISTINCT invoice_no) AS total_orders
FROM INCREMENTALETL._processed.processed_sales
WHERE customer_id IS NOT NULL
GROUP BY
    customer_id,
    country,
    payment_method,
    sales_channel,
    is_guest_customer
    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.dim_customers"} */;
[0m11:37:18.295247 [debug] [Thread-5 (]: Using snowflake connection "model.sales_pipelines.dim_products"
[0m11:37:18.295247 [debug] [Thread-2 (]: On model.sales_pipelines.dim_date: create or replace transient table INCREMENTALETL._marts.dim_date
    
    
    
    as (

SELECT DISTINCT
    invoice_date::DATE AS date_key,
    YEAR(invoice_date)  AS year,
    MONTH(invoice_date) AS month,
    DAY(invoice_date)   AS day,
    DAYNAME(invoice_date) AS day_name,
    WEEK(invoice_date)  AS week_number,
    QUARTER(invoice_date) AS quarter
FROM INCREMENTALETL._processed.processed_sales
    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.dim_date"} */;
[0m11:37:18.303783 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m11:37:18.304943 [debug] [Thread-5 (]: On model.sales_pipelines.dim_products: create or replace transient table INCREMENTALETL._marts.dim_products
    
    
    
    as (

SELECT DISTINCT
    stock_code,
    description,
    category
FROM INCREMENTALETL._processed.processed_sales
    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.dim_products"} */;
[0m11:37:18.304943 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m11:37:18.543628 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.248 seconds
[0m11:37:18.559465 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:37:18.559465 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: describe table INCREMENTALETL._marts.fact_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:37:18.811070 [debug] [Thread-4 (]: SQL status: SUCCESS 13 in 0.248 seconds
[0m11:37:18.821827 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:37:18.833884 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: describe table INCREMENTALETL._marts.fact_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:37:18.913433 [debug] [Thread-2 (]: SQL status: SUCCESS 762 in 0.620 seconds
[0m11:37:18.945176 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '310c6510-6fd5-449f-97a8-37be64e5c767', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017BFB4C30D0>]}
[0m11:37:18.945176 [info ] [Thread-2 (]: 4 of 6 OK created sql table model _marts.dim_date .............................. [[32mSUCCESS 762[0m in 0.77s]
[0m11:37:18.955997 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.dim_date
[0m11:37:18.973818 [debug] [Thread-4 (]: SQL status: SUCCESS 13 in 0.138 seconds
[0m11:37:18.988051 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:37:18.997673 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: describe table "INCREMENTALETL"."_MARTS"."FACT_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:37:19.129421 [debug] [Thread-4 (]: SQL status: SUCCESS 13 in 0.131 seconds
[0m11:37:19.141074 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.fact_sales"
[0m11:37:19.158012 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:37:19.160027 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */;
[0m11:37:19.308577 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.145 seconds
[0m11:37:19.314167 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:37:19.320257 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: merge into INCREMENTALETL._marts.fact_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._marts.fact_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","DATE_KEY" = DBT_INTERNAL_SOURCE."DATE_KEY","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","GROSS_AMOUNT" = DBT_INTERNAL_SOURCE."GROSS_AMOUNT","NET_AMOUNT" = DBT_INTERNAL_SOURCE."NET_AMOUNT","TOTAL_AMOUNT" = DBT_INTERNAL_SOURCE."TOTAL_AMOUNT","IS_RETURNED" = DBT_INTERNAL_SOURCE."IS_RETURNED","PROCESSED_AT" = DBT_INTERNAL_SOURCE."PROCESSED_AT"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "CUSTOMER_ID", "DATE_KEY", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "PROCESSED_AT")
    values
        ("INVOICE_NO", "STOCK_CODE", "CUSTOMER_ID", "DATE_KEY", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "PROCESSED_AT")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */;
[0m11:37:19.641494 [debug] [Thread-5 (]: SQL status: SUCCESS 15211 in 1.341 seconds
[0m11:37:19.657535 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '310c6510-6fd5-449f-97a8-37be64e5c767', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017B8E94AE00>]}
[0m11:37:19.657535 [info ] [Thread-5 (]: 5 of 6 OK created sql table model _marts.dim_products .......................... [[32mSUCCESS 15211[0m in 1.48s]
[0m11:37:19.672150 [debug] [Thread-5 (]: Finished running node model.sales_pipelines.dim_products
[0m11:37:19.785431 [debug] [Thread-3 (]: SQL status: SUCCESS 16507 in 1.482 seconds
[0m11:37:19.785431 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '310c6510-6fd5-449f-97a8-37be64e5c767', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017B8E998BB0>]}
[0m11:37:19.785431 [info ] [Thread-3 (]: 3 of 6 OK created sql table model _marts.dim_customers ......................... [[32mSUCCESS 16507[0m in 1.61s]
[0m11:37:19.800154 [debug] [Thread-3 (]: Finished running node model.sales_pipelines.dim_customers
[0m11:37:20.158101 [debug] [Thread-4 (]: SQL status: SUCCESS 724 in 0.834 seconds
[0m11:37:20.158101 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:37:20.163727 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:37:20.420922 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.253 seconds
[0m11:37:20.424421 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._marts.fact_sales__dbt_tmp
[0m11:37:20.424421 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:37:20.424421 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: drop view if exists INCREMENTALETL._marts.fact_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:37:20.612542 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.178 seconds
[0m11:37:20.614611 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '310c6510-6fd5-449f-97a8-37be64e5c767', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017B8EACBBB0>]}
[0m11:37:20.614611 [info ] [Thread-4 (]: 6 of 6 OK created sql incremental model _marts.fact_sales ...................... [[32mSUCCESS 724[0m in 2.44s]
[0m11:37:20.631121 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.fact_sales
[0m11:37:20.631121 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:37:20.637918 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m11:37:20.637918 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m11:37:20.970126 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m11:37:20.970126 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m11:37:21.379272 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m11:37:21.379272 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m11:37:21.783045 [debug] [MainThread]: Connection 'list_INCREMENTALETL__staging' was left open.
[0m11:37:21.783045 [debug] [MainThread]: On list_INCREMENTALETL__staging: Close
[0m11:37:22.082699 [debug] [MainThread]: Connection 'list_INCREMENTALETL__processed' was left open.
[0m11:37:22.082699 [debug] [MainThread]: On list_INCREMENTALETL__processed: Close
[0m11:37:22.522206 [debug] [MainThread]: Connection 'list_INCREMENTALETL__marts' was left open.
[0m11:37:22.522206 [debug] [MainThread]: On list_INCREMENTALETL__marts: Close
[0m11:37:22.779307 [debug] [MainThread]: Connection 'model.sales_pipelines.dim_date' was left open.
[0m11:37:22.779307 [debug] [MainThread]: On model.sales_pipelines.dim_date: Close
[0m11:37:23.036874 [debug] [MainThread]: Connection 'model.sales_pipelines.fact_sales' was left open.
[0m11:37:23.038856 [debug] [MainThread]: On model.sales_pipelines.fact_sales: Close
[0m11:37:23.287587 [debug] [MainThread]: Connection 'model.sales_pipelines.dim_customers' was left open.
[0m11:37:23.289610 [debug] [MainThread]: On model.sales_pipelines.dim_customers: Close
[0m11:37:23.524513 [debug] [MainThread]: Connection 'model.sales_pipelines.dim_products' was left open.
[0m11:37:23.524513 [debug] [MainThread]: On model.sales_pipelines.dim_products: Close
[0m11:37:23.946341 [info ] [MainThread]: 
[0m11:37:23.957924 [info ] [MainThread]: Finished running 3 incremental models, 3 table models in 0 hours 0 minutes and 16.62 seconds (16.62s).
[0m11:37:23.963781 [debug] [MainThread]: Command end result
[0m11:37:24.028403 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m11:37:24.028403 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m11:37:24.038455 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\snowflake-incremental-pipeline\dbt_project\target\run_results.json
[0m11:37:24.038455 [info ] [MainThread]: 
[0m11:37:24.045033 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:37:24.045033 [info ] [MainThread]: 
[0m11:37:24.047580 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=6
[0m11:37:24.049707 [debug] [MainThread]: Command `dbt run` succeeded at 11:37:24.047580 after 19.19 seconds
[0m11:37:24.049707 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017BFA2176D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017B8E1021D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017B8CA23FA0>]}
[0m11:37:24.049707 [debug] [MainThread]: Flushing usage events
[0m11:37:25.878250 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:37:52.360552 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E273377700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E2755F2230>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E2755F0AF0>]}


============================== 11:37:52.366261 | a7f22ac4-9872-4640-8bfd-23233d0a8a53 ==============================
[0m11:37:52.366261 [info ] [MainThread]: Running with dbt=1.11.6
[0m11:37:52.366261 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'D:\\snowflake-incremental-pipeline\\dbt_project', 'indirect_selection': 'eager', 'invocation_command': 'dbt run', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'D:\\snowflake-incremental-pipeline\\dbt_project\\logs'}
[0m11:37:53.296851 [debug] [MainThread]: Snowflake adapter: Setting snowflake.connector to ERROR (file logging only)
[0m11:37:53.296851 [debug] [MainThread]: Snowflake adapter: Setting botocore to ERROR (file logging only)
[0m11:37:53.298858 [debug] [MainThread]: Snowflake adapter: Setting boto3 to ERROR (file logging only)
[0m11:37:53.610812 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a7f22ac4-9872-4640-8bfd-23233d0a8a53', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E274DE4A90>]}
[0m11:37:53.691769 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a7f22ac4-9872-4640-8bfd-23233d0a8a53', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E27568A8F0>]}
[0m11:37:53.691769 [info ] [MainThread]: Registered adapter: snowflake=1.11.2
[0m11:37:54.188088 [debug] [MainThread]: checksum: ec27062d18352e9cfca161c166fcc73d2201661a23a3cf3c89196a27f32cfcb8, vars: {}, profile: , target: , version: 1.11.6
[0m11:37:54.435929 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:37:54.435929 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m11:37:54.435929 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:37:54.500913 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a7f22ac4-9872-4640-8bfd-23233d0a8a53', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E20812CA90>]}
[0m11:37:54.657514 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m11:37:54.657514 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m11:37:54.673200 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a7f22ac4-9872-4640-8bfd-23233d0a8a53', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E206BE0190>]}
[0m11:37:54.673200 [info ] [MainThread]: Found 6 models, 6 data tests, 1 source, 522 macros
[0m11:37:54.673200 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a7f22ac4-9872-4640-8bfd-23233d0a8a53', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E20812C370>]}
[0m11:37:54.684937 [info ] [MainThread]: 
[0m11:37:54.684937 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m11:37:54.684937 [info ] [MainThread]: 
[0m11:37:54.688983 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m11:37:54.696794 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m11:37:54.721168 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m11:37:54.725377 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m11:37:54.810820 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m11:37:54.810820 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m11:37:54.810820 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m11:37:54.810820 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m11:37:54.810820 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m11:37:54.810820 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m11:37:54.810820 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:37:54.810820 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:37:54.810820 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:37:56.082206 [debug] [ThreadPool]: SQL status: SUCCESS 5 in 1.258 seconds
[0m11:37:56.102874 [debug] [ThreadPool]: SQL status: SUCCESS 5 in 1.279 seconds
[0m11:37:56.105850 [debug] [ThreadPool]: SQL status: SUCCESS 5 in 1.283 seconds
[0m11:37:56.112298 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__marts'
[0m11:37:56.131077 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__processed'
[0m11:37:56.156805 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__marts"
[0m11:37:56.156805 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__staging'
[0m11:37:56.165080 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__processed"
[0m11:37:56.166275 [debug] [ThreadPool]: On list_INCREMENTALETL__marts: show objects in INCREMENTALETL._marts
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__marts"} */;
[0m11:37:56.173557 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__staging"
[0m11:37:56.173557 [debug] [ThreadPool]: On list_INCREMENTALETL__processed: show objects in INCREMENTALETL._processed
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__processed"} */;
[0m11:37:56.175566 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:37:56.177576 [debug] [ThreadPool]: On list_INCREMENTALETL__staging: show objects in INCREMENTALETL._staging
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__staging"} */;
[0m11:37:56.177576 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:37:56.179083 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:37:57.046758 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.868 seconds
[0m11:37:57.164213 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.991 seconds
[0m11:37:57.611815 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 1.437 seconds
[0m11:37:57.619609 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a7f22ac4-9872-4640-8bfd-23233d0a8a53', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E208217520>]}
[0m11:37:57.635771 [debug] [Thread-2 (]: Began running node model.sales_pipelines.stg_sales
[0m11:37:57.635771 [info ] [Thread-2 (]: 1 of 6 START sql incremental model _staging.stg_sales .......................... [RUN]
[0m11:37:57.653171 [debug] [Thread-2 (]: Acquiring new snowflake connection 'model.sales_pipelines.stg_sales'
[0m11:37:57.653171 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.stg_sales
[0m11:37:57.690272 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.stg_sales"
[0m11:37:57.690272 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.stg_sales
[0m11:37:57.772522 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:37:57.772522 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: create or replace  temporary view INCREMENTALETL._staging.stg_sales__dbt_tmp
  
  
  
  
  as (
    

WITH raw_data AS (
    SELECT * 
    FROM INCREMENTALETL.LANDINGZONE.RAW_SALES
),

staged AS (
    SELECT
        -- IDs
        CAST(INVOICENO AS VARCHAR)    AS invoice_no,
        CAST(STOCKCODE AS VARCHAR)    AS stock_code,
        CAST(DESCRIPTION AS VARCHAR)  AS description,

        -- Quantities & Prices
        TRY_CAST(QUANTITY AS INT)      AS quantity,
        TRY_CAST(UNITPRICE AS FLOAT)   AS unit_price,
        TRY_CAST(DISCOUNT AS FLOAT)    AS discount,
        TRY_CAST(SHIPPINGCOST AS FLOAT) AS shipping_cost,
        TRY_CAST(CUSTOMERID AS FLOAT)  AS customer_id,

        -- Convert once
        TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS') AS invoice_date,

        TRIM(UPPER(COUNTRY))            AS country,
        TRIM(UPPER(PAYMENTMETHOD))      AS payment_method,
        TRIM(UPPER(CATEGORY))           AS category,
        TRIM(UPPER(SALESCHANNEL))       AS sales_channel,
        TRIM(UPPER(RETURNSTATUS))       AS return_status,
        TRIM(UPPER(SHIPMENTPROVIDER))   AS shipment_provider,
        TRIM(UPPER(WAREHOUSELOCATION))  AS warehouse_location,
        TRIM(UPPER(ORDERPRIORITY))      AS order_priority,

        CURRENT_TIMESTAMP()             AS stg_loaded_at

    FROM raw_data
),

validated AS (
    SELECT
        *,
        YEAR(invoice_date)  AS invoice_year,
        MONTH(invoice_date) AS invoice_month,

        CASE
            WHEN invoice_date IS NULL           THEN 'INVALID_DATE'
            WHEN quantity IS NULL               THEN 'INVALID_QUANTITY'
            WHEN unit_price IS NULL             THEN 'INVALID_PRICE'
            WHEN shipping_cost < 0              THEN 'NEGATIVE_SHIPPING'
            WHEN discount < 0 OR discount > 1   THEN 'INVALID_DISCOUNT'
            WHEN invoice_no IS NULL             THEN 'NULL_INVOICE'
            ELSE 'VALID'
        END AS data_quality_flag
    FROM staged
)

SELECT *
FROM validated


WHERE NOT EXISTS (
    SELECT 1
    FROM INCREMENTALETL._staging.stg_sales t
    WHERE t.invoice_year  = validated.invoice_year
      AND t.invoice_month = validated.invoice_month
)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m11:37:57.772522 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m11:37:58.849778 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 1.079 seconds
[0m11:37:58.865905 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:37:58.865905 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:37:59.040152 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.165 seconds
[0m11:37:59.060350 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:37:59.060350 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:37:59.262644 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.199 seconds
[0m11:37:59.310028 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:37:59.310028 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:37:59.470527 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.155 seconds
[0m11:37:59.486319 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:37:59.486319 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table "INCREMENTALETL"."_STAGING"."STG_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:37:59.657774 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.163 seconds
[0m11:37:59.696463 [debug] [Thread-2 (]: 
    In "INCREMENTALETL"."_STAGING"."STG_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m11:37:59.736444 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.stg_sales"
[0m11:37:59.736444 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:37:59.736444 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m11:37:59.929092 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.189 seconds
[0m11:37:59.929092 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:37:59.929092 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: merge into INCREMENTALETL._staging.stg_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._staging.stg_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","DATA_QUALITY_FLAG" = DBT_INTERNAL_SOURCE."DATA_QUALITY_FLAG"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "INVOICE_YEAR", "INVOICE_MONTH", "DATA_QUALITY_FLAG")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "INVOICE_YEAR", "INVOICE_MONTH", "DATA_QUALITY_FLAG")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m11:38:01.204803 [debug] [Thread-2 (]: SQL status: SUCCESS 672 in 1.265 seconds
[0m11:38:01.208854 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:38:01.210878 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:38:01.490168 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.280 seconds
[0m11:38:01.522153 [debug] [Thread-2 (]: Applying DROP to: INCREMENTALETL._staging.stg_sales__dbt_tmp
[0m11:38:01.539896 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:38:01.539896 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: drop view if exists INCREMENTALETL._staging.stg_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:38:01.735358 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.194 seconds
[0m11:38:01.778864 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a7f22ac4-9872-4640-8bfd-23233d0a8a53', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E2731C9810>]}
[0m11:38:01.778864 [info ] [Thread-2 (]: 1 of 6 OK created sql incremental model _staging.stg_sales ..................... [[32mSUCCESS 672[0m in 4.14s]
[0m11:38:01.778864 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.stg_sales
[0m11:38:01.794987 [debug] [Thread-4 (]: Began running node model.sales_pipelines.processed_sales
[0m11:38:01.796223 [info ] [Thread-4 (]: 2 of 6 START sql incremental model _processed.processed_sales .................. [RUN]
[0m11:38:01.799565 [debug] [Thread-4 (]: Acquiring new snowflake connection 'model.sales_pipelines.processed_sales'
[0m11:38:01.799872 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.processed_sales
[0m11:38:01.809967 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.processed_sales"
[0m11:38:01.811414 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.processed_sales
[0m11:38:01.823487 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:38:01.825182 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: create or replace  temporary view INCREMENTALETL._processed.processed_sales__dbt_tmp
  
  
  
  
  as (
    

WITH staging AS (
    SELECT * 
    FROM INCREMENTALETL._staging.stg_sales
),

deduped AS (
    SELECT *,
        ROW_NUMBER() OVER (
            PARTITION BY invoice_no, stock_code
            ORDER BY stg_loaded_at DESC
        ) AS row_num
    FROM staging
),

cleaned AS (
    SELECT
        invoice_no,
        stock_code,
        description,
        quantity,
        unit_price,
        discount,
        shipping_cost,
        customer_id,
        invoice_date,
        invoice_year,
        invoice_month,
        country,
        payment_method,
        category,
        sales_channel,
        return_status,
        shipment_provider,
        warehouse_location,
        order_priority,

        -- Derived metrics
        ROUND(quantity * unit_price, 2) AS gross_amount,
        ROUND(quantity * unit_price * (1 - discount), 2) AS net_amount,
        ROUND(
            quantity * unit_price * (1 - discount)
            + COALESCE(shipping_cost, 0),
        2) AS total_amount,

        CASE WHEN return_status = 'RETURNED' THEN TRUE ELSE FALSE END AS is_returned,
        CASE WHEN customer_id IS NULL THEN TRUE ELSE FALSE END AS is_guest_customer,

        stg_loaded_at,
        CURRENT_TIMESTAMP() AS processed_at

    FROM deduped
    WHERE
        row_num = 1
        AND data_quality_flag = 'VALID'
        AND invoice_date IS NOT NULL
)

SELECT *
FROM cleaned


WHERE NOT EXISTS (
    SELECT 1
    FROM INCREMENTALETL._processed.processed_sales t
    WHERE t.invoice_year  = cleaned.invoice_year
      AND t.invoice_month = cleaned.invoice_month
)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m11:38:01.825182 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m11:38:02.638319 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.815 seconds
[0m11:38:02.638319 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:38:02.638319 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:38:02.783908 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.147 seconds
[0m11:38:02.815382 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:38:02.815382 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:38:02.955562 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.129 seconds
[0m11:38:02.973946 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:38:02.984055 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:38:03.170316 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.183 seconds
[0m11:38:03.190257 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:38:03.192268 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:38:03.359189 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.179 seconds
[0m11:38:03.393309 [debug] [Thread-4 (]: 
    In "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m11:38:03.405002 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.processed_sales"
[0m11:38:03.413143 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:38:03.413143 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m11:38:03.585198 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.167 seconds
[0m11:38:03.585198 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:38:03.585198 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: merge into INCREMENTALETL._processed.processed_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._processed.processed_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","GROSS_AMOUNT" = DBT_INTERNAL_SOURCE."GROSS_AMOUNT","NET_AMOUNT" = DBT_INTERNAL_SOURCE."NET_AMOUNT","TOTAL_AMOUNT" = DBT_INTERNAL_SOURCE."TOTAL_AMOUNT","IS_RETURNED" = DBT_INTERNAL_SOURCE."IS_RETURNED","IS_GUEST_CUSTOMER" = DBT_INTERNAL_SOURCE."IS_GUEST_CUSTOMER","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","PROCESSED_AT" = DBT_INTERNAL_SOURCE."PROCESSED_AT"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m11:38:04.790363 [debug] [Thread-4 (]: SQL status: SUCCESS 660 in 1.202 seconds
[0m11:38:04.790363 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:38:04.790363 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:38:05.085090 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.296 seconds
[0m11:38:05.117020 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._processed.processed_sales__dbt_tmp
[0m11:38:05.117020 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:38:05.117020 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: drop view if exists INCREMENTALETL._processed.processed_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:38:05.298408 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.169 seconds
[0m11:38:05.310054 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a7f22ac4-9872-4640-8bfd-23233d0a8a53', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E206A9CEB0>]}
[0m11:38:05.314107 [info ] [Thread-4 (]: 2 of 6 OK created sql incremental model _processed.processed_sales ............. [[32mSUCCESS 660[0m in 3.51s]
[0m11:38:05.318392 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.processed_sales
[0m11:38:05.320417 [debug] [Thread-3 (]: Began running node model.sales_pipelines.dim_customers
[0m11:38:05.325800 [debug] [Thread-2 (]: Began running node model.sales_pipelines.dim_date
[0m11:38:05.328575 [debug] [Thread-5 (]: Began running node model.sales_pipelines.dim_products
[0m11:38:05.331593 [debug] [Thread-4 (]: Began running node model.sales_pipelines.fact_sales
[0m11:38:05.334146 [info ] [Thread-3 (]: 3 of 6 START sql table model _marts.dim_customers .............................. [RUN]
[0m11:38:05.338350 [info ] [Thread-2 (]: 4 of 6 START sql table model _marts.dim_date ................................... [RUN]
[0m11:38:05.338350 [info ] [Thread-5 (]: 5 of 6 START sql table model _marts.dim_products ............................... [RUN]
[0m11:38:05.342167 [info ] [Thread-4 (]: 6 of 6 START sql incremental model _marts.fact_sales ........................... [RUN]
[0m11:38:05.344872 [debug] [Thread-3 (]: Acquiring new snowflake connection 'model.sales_pipelines.dim_customers'
[0m11:38:05.348020 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.sales_pipelines.stg_sales, now model.sales_pipelines.dim_date)
[0m11:38:05.349759 [debug] [Thread-5 (]: Acquiring new snowflake connection 'model.sales_pipelines.dim_products'
[0m11:38:05.351487 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.sales_pipelines.processed_sales, now model.sales_pipelines.fact_sales)
[0m11:38:05.351487 [debug] [Thread-3 (]: Began compiling node model.sales_pipelines.dim_customers
[0m11:38:05.354147 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.dim_date
[0m11:38:05.355295 [debug] [Thread-5 (]: Began compiling node model.sales_pipelines.dim_products
[0m11:38:05.355295 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.fact_sales
[0m11:38:05.365441 [debug] [Thread-3 (]: Writing injected SQL for node "model.sales_pipelines.dim_customers"
[0m11:38:05.372316 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.dim_date"
[0m11:38:05.372316 [debug] [Thread-5 (]: Writing injected SQL for node "model.sales_pipelines.dim_products"
[0m11:38:05.382683 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.fact_sales"
[0m11:38:05.388965 [debug] [Thread-3 (]: Began executing node model.sales_pipelines.dim_customers
[0m11:38:05.391044 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.dim_date
[0m11:38:05.392520 [debug] [Thread-5 (]: Began executing node model.sales_pipelines.dim_products
[0m11:38:05.392520 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.fact_sales
[0m11:38:05.429667 [debug] [Thread-3 (]: Writing runtime sql for node "model.sales_pipelines.dim_customers"
[0m11:38:05.439458 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.dim_date"
[0m11:38:05.446193 [debug] [Thread-5 (]: Writing runtime sql for node "model.sales_pipelines.dim_products"
[0m11:38:05.454262 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:38:05.456919 [debug] [Thread-3 (]: Using snowflake connection "model.sales_pipelines.dim_customers"
[0m11:38:05.456919 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.dim_date"
[0m11:38:05.456919 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: create or replace  temporary view INCREMENTALETL._marts.fact_sales__dbt_tmp
  
  
  
  
  as (
    

SELECT
    invoice_no,
    stock_code,
    customer_id,
    invoice_date::DATE AS date_key,

    quantity,
    unit_price,
    discount,
    shipping_cost,

    gross_amount,
    net_amount,
    total_amount,

    is_returned,
    processed_at

FROM INCREMENTALETL._processed.processed_sales


WHERE processed_at > (SELECT MAX(processed_at) FROM INCREMENTALETL._marts.fact_sales)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */;
[0m11:38:05.456919 [debug] [Thread-5 (]: Using snowflake connection "model.sales_pipelines.dim_products"
[0m11:38:05.456919 [debug] [Thread-3 (]: On model.sales_pipelines.dim_customers: create or replace transient table INCREMENTALETL._marts.dim_customers
    
    
    
    as (

SELECT DISTINCT
    customer_id,
    country,
    payment_method,
    sales_channel,
    is_guest_customer,
    MIN(invoice_date) AS first_purchase_date,
    MAX(invoice_date) AS last_purchase_date,
    COUNT(DISTINCT invoice_no) AS total_orders
FROM INCREMENTALETL._processed.processed_sales
WHERE customer_id IS NOT NULL
GROUP BY
    customer_id,
    country,
    payment_method,
    sales_channel,
    is_guest_customer
    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.dim_customers"} */;
[0m11:38:05.456919 [debug] [Thread-2 (]: On model.sales_pipelines.dim_date: create or replace transient table INCREMENTALETL._marts.dim_date
    
    
    
    as (

SELECT DISTINCT
    invoice_date::DATE AS date_key,
    YEAR(invoice_date)  AS year,
    MONTH(invoice_date) AS month,
    DAY(invoice_date)   AS day,
    DAYNAME(invoice_date) AS day_name,
    WEEK(invoice_date)  AS week_number,
    QUARTER(invoice_date) AS quarter
FROM INCREMENTALETL._processed.processed_sales
    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.dim_date"} */;
[0m11:38:05.467979 [debug] [Thread-5 (]: On model.sales_pipelines.dim_products: create or replace transient table INCREMENTALETL._marts.dim_products
    
    
    
    as (

SELECT DISTINCT
    stock_code,
    description,
    category
FROM INCREMENTALETL._processed.processed_sales
    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.dim_products"} */;
[0m11:38:05.470090 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m11:38:05.472663 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m11:38:05.765758 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.304 seconds
[0m11:38:05.778287 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:38:05.778287 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: describe table INCREMENTALETL._marts.fact_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:38:05.942936 [debug] [Thread-4 (]: SQL status: SUCCESS 13 in 0.152 seconds
[0m11:38:05.962647 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:38:05.964687 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: describe table INCREMENTALETL._marts.fact_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:38:06.099915 [debug] [Thread-4 (]: SQL status: SUCCESS 13 in 0.132 seconds
[0m11:38:06.113034 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:38:06.113034 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: describe table "INCREMENTALETL"."_MARTS"."FACT_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:38:06.128713 [debug] [Thread-2 (]: SQL status: SUCCESS 790 in 0.657 seconds
[0m11:38:06.149048 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a7f22ac4-9872-4640-8bfd-23233d0a8a53', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E20898E200>]}
[0m11:38:06.149048 [info ] [Thread-2 (]: 4 of 6 OK created sql table model _marts.dim_date .............................. [[32mSUCCESS 790[0m in 0.80s]
[0m11:38:06.156477 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.dim_date
[0m11:38:06.291664 [debug] [Thread-4 (]: SQL status: SUCCESS 13 in 0.165 seconds
[0m11:38:06.318340 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.fact_sales"
[0m11:38:06.318340 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:38:06.318340 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */;
[0m11:38:06.528686 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.206 seconds
[0m11:38:06.528686 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:38:06.528686 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: merge into INCREMENTALETL._marts.fact_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._marts.fact_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","DATE_KEY" = DBT_INTERNAL_SOURCE."DATE_KEY","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","GROSS_AMOUNT" = DBT_INTERNAL_SOURCE."GROSS_AMOUNT","NET_AMOUNT" = DBT_INTERNAL_SOURCE."NET_AMOUNT","TOTAL_AMOUNT" = DBT_INTERNAL_SOURCE."TOTAL_AMOUNT","IS_RETURNED" = DBT_INTERNAL_SOURCE."IS_RETURNED","PROCESSED_AT" = DBT_INTERNAL_SOURCE."PROCESSED_AT"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "CUSTOMER_ID", "DATE_KEY", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "PROCESSED_AT")
    values
        ("INVOICE_NO", "STOCK_CODE", "CUSTOMER_ID", "DATE_KEY", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "PROCESSED_AT")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */;
[0m11:38:06.605175 [debug] [Thread-5 (]: SQL status: SUCCESS 15689 in 1.137 seconds
[0m11:38:06.615096 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a7f22ac4-9872-4640-8bfd-23233d0a8a53', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E2080C7E80>]}
[0m11:38:06.615096 [info ] [Thread-5 (]: 5 of 6 OK created sql table model _marts.dim_products .......................... [[32mSUCCESS 15689[0m in 1.27s]
[0m11:38:06.626491 [debug] [Thread-5 (]: Finished running node model.sales_pipelines.dim_products
[0m11:38:06.834262 [debug] [Thread-3 (]: SQL status: SUCCESS 17114 in 1.368 seconds
[0m11:38:06.839663 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a7f22ac4-9872-4640-8bfd-23233d0a8a53', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E208A5FC40>]}
[0m11:38:06.839663 [info ] [Thread-3 (]: 3 of 6 OK created sql table model _marts.dim_customers ......................... [[32mSUCCESS 17114[0m in 1.49s]
[0m11:38:06.839663 [debug] [Thread-3 (]: Finished running node model.sales_pipelines.dim_customers
[0m11:38:07.218599 [debug] [Thread-4 (]: SQL status: SUCCESS 660 in 0.688 seconds
[0m11:38:07.234730 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:38:07.234730 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:38:07.542233 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.301 seconds
[0m11:38:07.560174 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._marts.fact_sales__dbt_tmp
[0m11:38:07.564220 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:38:07.568259 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: drop view if exists INCREMENTALETL._marts.fact_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:38:07.750836 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.187 seconds
[0m11:38:07.766651 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a7f22ac4-9872-4640-8bfd-23233d0a8a53', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E2087F7D90>]}
[0m11:38:07.766651 [info ] [Thread-4 (]: 6 of 6 OK created sql incremental model _marts.fact_sales ...................... [[32mSUCCESS 660[0m in 2.42s]
[0m11:38:07.766651 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.fact_sales
[0m11:38:07.784389 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:38:07.786986 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m11:38:07.788812 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m11:38:08.047142 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m11:38:08.047142 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m11:38:08.377515 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m11:38:08.393440 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m11:38:08.768220 [debug] [MainThread]: Connection 'list_INCREMENTALETL__marts' was left open.
[0m11:38:08.783959 [debug] [MainThread]: On list_INCREMENTALETL__marts: Close
[0m11:38:09.188057 [debug] [MainThread]: Connection 'list_INCREMENTALETL__processed' was left open.
[0m11:38:09.188057 [debug] [MainThread]: On list_INCREMENTALETL__processed: Close
[0m11:38:12.470011 [debug] [MainThread]: Connection 'list_INCREMENTALETL__staging' was left open.
[0m11:38:12.470011 [debug] [MainThread]: On list_INCREMENTALETL__staging: Close
[0m11:38:12.729234 [debug] [MainThread]: Connection 'model.sales_pipelines.dim_date' was left open.
[0m11:38:12.729234 [debug] [MainThread]: On model.sales_pipelines.dim_date: Close
[0m11:38:13.024175 [debug] [MainThread]: Connection 'model.sales_pipelines.fact_sales' was left open.
[0m11:38:13.024175 [debug] [MainThread]: On model.sales_pipelines.fact_sales: Close
[0m11:38:13.292695 [debug] [MainThread]: Connection 'model.sales_pipelines.dim_customers' was left open.
[0m11:38:13.292695 [debug] [MainThread]: On model.sales_pipelines.dim_customers: Close
[0m11:38:13.598472 [debug] [MainThread]: Connection 'model.sales_pipelines.dim_products' was left open.
[0m11:38:13.602661 [debug] [MainThread]: On model.sales_pipelines.dim_products: Close
[0m11:38:13.909514 [info ] [MainThread]: 
[0m11:38:13.910914 [info ] [MainThread]: Finished running 3 incremental models, 3 table models in 0 hours 0 minutes and 19.22 seconds (19.22s).
[0m11:38:13.913527 [debug] [MainThread]: Command end result
[0m11:38:14.029127 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m11:38:14.037175 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m11:38:14.072332 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\snowflake-incremental-pipeline\dbt_project\target\run_results.json
[0m11:38:14.076077 [info ] [MainThread]: 
[0m11:38:14.077814 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:38:14.084992 [info ] [MainThread]: 
[0m11:38:14.087418 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=6
[0m11:38:14.089562 [debug] [MainThread]: Command `dbt run` succeeded at 11:38:14.089562 after 21.86 seconds
[0m11:38:14.089562 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E273377700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E208112140>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E27470FB50>]}
[0m11:38:14.089562 [debug] [MainThread]: Flushing usage events
[0m11:38:15.435756 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:40:34.163204 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF0A497730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF0C6F5C30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF0C6F79D0>]}


============================== 11:40:34.166130 | 6a1c17c2-a680-4673-b172-60c40d4d138b ==============================
[0m11:40:34.166130 [info ] [MainThread]: Running with dbt=1.11.6
[0m11:40:34.166130 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'profiles_dir': 'D:\\snowflake-incremental-pipeline\\dbt_project', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'indirect_selection': 'eager', 'use_colors': 'True', 'invocation_command': 'dbt run', 'use_experimental_parser': 'False', 'log_path': 'D:\\snowflake-incremental-pipeline\\dbt_project\\logs'}
[0m11:40:35.109301 [debug] [MainThread]: Snowflake adapter: Setting snowflake.connector to ERROR (file logging only)
[0m11:40:35.109301 [debug] [MainThread]: Snowflake adapter: Setting botocore to ERROR (file logging only)
[0m11:40:35.109301 [debug] [MainThread]: Snowflake adapter: Setting boto3 to ERROR (file logging only)
[0m11:40:35.447885 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6a1c17c2-a680-4673-b172-60c40d4d138b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF08A0EB60>]}
[0m11:40:35.548198 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6a1c17c2-a680-4673-b172-60c40d4d138b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF0C7B8BE0>]}
[0m11:40:35.548198 [info ] [MainThread]: Registered adapter: snowflake=1.11.2
[0m11:40:36.080291 [debug] [MainThread]: checksum: ec27062d18352e9cfca161c166fcc73d2201661a23a3cf3c89196a27f32cfcb8, vars: {}, profile: , target: , version: 1.11.6
[0m11:40:36.300047 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:40:36.300047 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m11:40:36.300047 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:40:36.378813 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6a1c17c2-a680-4673-b172-60c40d4d138b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF1EE2CAC0>]}
[0m11:40:36.533791 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m11:40:36.537545 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m11:40:36.557353 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6a1c17c2-a680-4673-b172-60c40d4d138b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF0A30F460>]}
[0m11:40:36.557353 [info ] [MainThread]: Found 6 models, 6 data tests, 1 source, 522 macros
[0m11:40:36.557353 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6a1c17c2-a680-4673-b172-60c40d4d138b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF1EE2C2E0>]}
[0m11:40:36.557353 [info ] [MainThread]: 
[0m11:40:36.557353 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m11:40:36.557353 [info ] [MainThread]: 
[0m11:40:36.567002 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m11:40:36.576574 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m11:40:36.582537 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m11:40:36.591598 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m11:40:36.695916 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m11:40:36.695916 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m11:40:36.695916 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m11:40:36.695916 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m11:40:36.695916 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m11:40:36.695916 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m11:40:36.695916 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:40:36.704073 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:40:36.704073 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:40:38.039327 [debug] [ThreadPool]: SQL status: SUCCESS 5 in 1.341 seconds
[0m11:40:38.061390 [debug] [ThreadPool]: SQL status: SUCCESS 5 in 1.362 seconds
[0m11:40:38.083120 [debug] [ThreadPool]: SQL status: SUCCESS 5 in 1.383 seconds
[0m11:40:38.089860 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__staging'
[0m11:40:38.098887 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__processed'
[0m11:40:38.114638 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__staging"
[0m11:40:38.122408 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__processed"
[0m11:40:38.122408 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__marts'
[0m11:40:38.122408 [debug] [ThreadPool]: On list_INCREMENTALETL__staging: show objects in INCREMENTALETL._staging
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__staging"} */;
[0m11:40:38.126037 [debug] [ThreadPool]: On list_INCREMENTALETL__processed: show objects in INCREMENTALETL._processed
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__processed"} */;
[0m11:40:38.130757 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__marts"
[0m11:40:38.133147 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:40:38.134467 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:40:38.134467 [debug] [ThreadPool]: On list_INCREMENTALETL__marts: show objects in INCREMENTALETL._marts
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__marts"} */;
[0m11:40:38.136457 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:40:39.088445 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 0.950 seconds
[0m11:40:39.117862 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.985 seconds
[0m11:40:39.120908 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.988 seconds
[0m11:40:39.139919 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6a1c17c2-a680-4673-b172-60c40d4d138b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF1EF13100>]}
[0m11:40:39.157818 [debug] [Thread-2 (]: Began running node model.sales_pipelines.stg_sales
[0m11:40:39.162345 [info ] [Thread-2 (]: 1 of 6 START sql incremental model _staging.stg_sales .......................... [RUN]
[0m11:40:39.162345 [debug] [Thread-2 (]: Acquiring new snowflake connection 'model.sales_pipelines.stg_sales'
[0m11:40:39.168884 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.stg_sales
[0m11:40:39.224986 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.stg_sales"
[0m11:40:39.224986 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.stg_sales
[0m11:40:39.292272 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:40:39.292272 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: create or replace  temporary view INCREMENTALETL._staging.stg_sales__dbt_tmp
  
  
  
  
  as (
    

WITH raw_data AS (
    SELECT * 
    FROM INCREMENTALETL.LANDINGZONE.RAW_SALES
),

staged AS (
    SELECT
        -- IDs
        CAST(INVOICENO AS VARCHAR)    AS invoice_no,
        CAST(STOCKCODE AS VARCHAR)    AS stock_code,
        CAST(DESCRIPTION AS VARCHAR)  AS description,

        -- Quantities & Prices
        TRY_CAST(QUANTITY AS INT)      AS quantity,
        TRY_CAST(UNITPRICE AS FLOAT)   AS unit_price,
        TRY_CAST(DISCOUNT AS FLOAT)    AS discount,
        TRY_CAST(SHIPPINGCOST AS FLOAT) AS shipping_cost,
        TRY_CAST(CUSTOMERID AS FLOAT)  AS customer_id,

        -- Convert once
        TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS') AS invoice_date,

        TRIM(UPPER(COUNTRY))            AS country,
        TRIM(UPPER(PAYMENTMETHOD))      AS payment_method,
        TRIM(UPPER(CATEGORY))           AS category,
        TRIM(UPPER(SALESCHANNEL))       AS sales_channel,
        TRIM(UPPER(RETURNSTATUS))       AS return_status,
        TRIM(UPPER(SHIPMENTPROVIDER))   AS shipment_provider,
        TRIM(UPPER(WAREHOUSELOCATION))  AS warehouse_location,
        TRIM(UPPER(ORDERPRIORITY))      AS order_priority,

        CURRENT_TIMESTAMP()             AS stg_loaded_at

    FROM raw_data
),

validated AS (
    SELECT
        *,
        YEAR(invoice_date)  AS invoice_year,
        MONTH(invoice_date) AS invoice_month,

        CASE
            WHEN invoice_date IS NULL           THEN 'INVALID_DATE'
            WHEN quantity IS NULL               THEN 'INVALID_QUANTITY'
            WHEN unit_price IS NULL             THEN 'INVALID_PRICE'
            WHEN shipping_cost < 0              THEN 'NEGATIVE_SHIPPING'
            WHEN discount < 0 OR discount > 1   THEN 'INVALID_DISCOUNT'
            WHEN invoice_no IS NULL             THEN 'NULL_INVOICE'
            ELSE 'VALID'
        END AS data_quality_flag
    FROM staged
)

SELECT *
FROM validated


WHERE NOT EXISTS (
    SELECT 1
    FROM INCREMENTALETL._staging.stg_sales t
    WHERE t.invoice_year  = validated.invoice_year
      AND t.invoice_month = validated.invoice_month
)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m11:40:39.306828 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m11:40:40.083238 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.778 seconds
[0m11:40:40.190832 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:40:40.204678 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:40:40.367293 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.164 seconds
[0m11:40:40.383044 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:40:40.383044 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:40:40.541730 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.148 seconds
[0m11:40:40.589352 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:40:40.589352 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:40:40.746509 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.152 seconds
[0m11:40:40.767972 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:40:40.769994 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table "INCREMENTALETL"."_STAGING"."STG_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:40:40.952608 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.182 seconds
[0m11:40:40.993341 [debug] [Thread-2 (]: 
    In "INCREMENTALETL"."_STAGING"."STG_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m11:40:41.023448 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.stg_sales"
[0m11:40:41.039183 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:40:41.039183 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m11:40:41.257494 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.217 seconds
[0m11:40:41.257494 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:40:41.257494 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: merge into INCREMENTALETL._staging.stg_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._staging.stg_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","DATA_QUALITY_FLAG" = DBT_INTERNAL_SOURCE."DATA_QUALITY_FLAG"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "INVOICE_YEAR", "INVOICE_MONTH", "DATA_QUALITY_FLAG")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "INVOICE_YEAR", "INVOICE_MONTH", "DATA_QUALITY_FLAG")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m11:40:42.392940 [debug] [Thread-2 (]: SQL status: SUCCESS 2928 in 1.127 seconds
[0m11:40:42.392940 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:40:42.397514 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:40:42.636912 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.240 seconds
[0m11:40:42.658525 [debug] [Thread-2 (]: Applying DROP to: INCREMENTALETL._staging.stg_sales__dbt_tmp
[0m11:40:42.674281 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:40:42.674281 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: drop view if exists INCREMENTALETL._staging.stg_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:40:43.036566 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.349 seconds
[0m11:40:43.084197 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6a1c17c2-a680-4673-b172-60c40d4d138b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF1F6A93C0>]}
[0m11:40:43.084197 [info ] [Thread-2 (]: 1 of 6 OK created sql incremental model _staging.stg_sales ..................... [[32mSUCCESS 2928[0m in 3.92s]
[0m11:40:43.084197 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.stg_sales
[0m11:40:43.096406 [debug] [Thread-4 (]: Began running node model.sales_pipelines.processed_sales
[0m11:40:43.098068 [info ] [Thread-4 (]: 2 of 6 START sql incremental model _processed.processed_sales .................. [RUN]
[0m11:40:43.100082 [debug] [Thread-4 (]: Acquiring new snowflake connection 'model.sales_pipelines.processed_sales'
[0m11:40:43.101222 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.processed_sales
[0m11:40:43.108706 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.processed_sales"
[0m11:40:43.110654 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.processed_sales
[0m11:40:43.113355 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:40:43.113355 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: create or replace  temporary view INCREMENTALETL._processed.processed_sales__dbt_tmp
  
  
  
  
  as (
    

WITH staging AS (
    SELECT * 
    FROM INCREMENTALETL._staging.stg_sales
),

deduped AS (
    SELECT *,
        ROW_NUMBER() OVER (
            PARTITION BY invoice_no, stock_code
            ORDER BY stg_loaded_at DESC
        ) AS row_num
    FROM staging
),

cleaned AS (
    SELECT
        invoice_no,
        stock_code,
        description,
        quantity,
        unit_price,
        discount,
        shipping_cost,
        customer_id,
        invoice_date,
        invoice_year,
        invoice_month,
        country,
        payment_method,
        category,
        sales_channel,
        return_status,
        shipment_provider,
        warehouse_location,
        order_priority,

        -- Derived metrics
        ROUND(quantity * unit_price, 2) AS gross_amount,
        ROUND(quantity * unit_price * (1 - discount), 2) AS net_amount,
        ROUND(
            quantity * unit_price * (1 - discount)
            + COALESCE(shipping_cost, 0),
        2) AS total_amount,

        CASE WHEN return_status = 'RETURNED' THEN TRUE ELSE FALSE END AS is_returned,
        CASE WHEN customer_id IS NULL THEN TRUE ELSE FALSE END AS is_guest_customer,

        stg_loaded_at,
        CURRENT_TIMESTAMP() AS processed_at

    FROM deduped
    WHERE
        row_num = 1
        AND data_quality_flag = 'VALID'
        AND invoice_date IS NOT NULL
)

SELECT *
FROM cleaned


WHERE NOT EXISTS (
    SELECT 1
    FROM INCREMENTALETL._processed.processed_sales t
    WHERE t.invoice_year  = cleaned.invoice_year
      AND t.invoice_month = cleaned.invoice_month
)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m11:40:43.113355 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m11:40:44.132812 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 1.023 seconds
[0m11:40:44.148698 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:40:44.148698 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:40:44.334264 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.168 seconds
[0m11:40:44.356956 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:40:44.356956 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:40:44.537536 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.172 seconds
[0m11:40:44.555061 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:40:44.569122 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:40:44.697878 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.140 seconds
[0m11:40:44.729969 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:40:44.729969 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:40:44.859372 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.135 seconds
[0m11:40:44.899566 [debug] [Thread-4 (]: 
    In "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m11:40:44.909324 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.processed_sales"
[0m11:40:44.914698 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:40:44.914698 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m11:40:45.067566 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.154 seconds
[0m11:40:45.067566 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:40:45.067566 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: merge into INCREMENTALETL._processed.processed_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._processed.processed_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","GROSS_AMOUNT" = DBT_INTERNAL_SOURCE."GROSS_AMOUNT","NET_AMOUNT" = DBT_INTERNAL_SOURCE."NET_AMOUNT","TOTAL_AMOUNT" = DBT_INTERNAL_SOURCE."TOTAL_AMOUNT","IS_RETURNED" = DBT_INTERNAL_SOURCE."IS_RETURNED","IS_GUEST_CUSTOMER" = DBT_INTERNAL_SOURCE."IS_GUEST_CUSTOMER","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","PROCESSED_AT" = DBT_INTERNAL_SOURCE."PROCESSED_AT"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m11:40:46.170403 [debug] [Thread-4 (]: SQL status: SUCCESS 2880 in 1.093 seconds
[0m11:40:46.170403 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:40:46.170403 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:40:46.482241 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.304 seconds
[0m11:40:46.482241 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._processed.processed_sales__dbt_tmp
[0m11:40:46.491457 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:40:46.492519 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: drop view if exists INCREMENTALETL._processed.processed_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:40:46.679267 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.192 seconds
[0m11:40:46.695134 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6a1c17c2-a680-4673-b172-60c40d4d138b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF0A2EBF40>]}
[0m11:40:46.695134 [info ] [Thread-4 (]: 2 of 6 OK created sql incremental model _processed.processed_sales ............. [[32mSUCCESS 2880[0m in 3.60s]
[0m11:40:46.695134 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.processed_sales
[0m11:40:46.711404 [debug] [Thread-2 (]: Began running node model.sales_pipelines.dim_date
[0m11:40:46.715481 [debug] [Thread-5 (]: Began running node model.sales_pipelines.dim_products
[0m11:40:46.695134 [debug] [Thread-3 (]: Began running node model.sales_pipelines.dim_customers
[0m11:40:46.716523 [debug] [Thread-4 (]: Began running node model.sales_pipelines.fact_sales
[0m11:40:46.716523 [info ] [Thread-2 (]: 4 of 6 START sql table model _marts.dim_date ................................... [RUN]
[0m11:40:46.720086 [info ] [Thread-5 (]: 5 of 6 START sql table model _marts.dim_products ............................... [RUN]
[0m11:40:46.721214 [info ] [Thread-3 (]: 3 of 6 START sql table model _marts.dim_customers .............................. [RUN]
[0m11:40:46.724111 [info ] [Thread-4 (]: 6 of 6 START sql incremental model _marts.fact_sales ........................... [RUN]
[0m11:40:46.726397 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.sales_pipelines.stg_sales, now model.sales_pipelines.dim_date)
[0m11:40:46.727815 [debug] [Thread-5 (]: Acquiring new snowflake connection 'model.sales_pipelines.dim_products'
[0m11:40:46.729831 [debug] [Thread-3 (]: Acquiring new snowflake connection 'model.sales_pipelines.dim_customers'
[0m11:40:46.732258 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.sales_pipelines.processed_sales, now model.sales_pipelines.fact_sales)
[0m11:40:46.733331 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.dim_date
[0m11:40:46.733331 [debug] [Thread-5 (]: Began compiling node model.sales_pipelines.dim_products
[0m11:40:46.736090 [debug] [Thread-3 (]: Began compiling node model.sales_pipelines.dim_customers
[0m11:40:46.736090 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.fact_sales
[0m11:40:46.743533 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.dim_date"
[0m11:40:46.753198 [debug] [Thread-5 (]: Writing injected SQL for node "model.sales_pipelines.dim_products"
[0m11:40:46.758851 [debug] [Thread-3 (]: Writing injected SQL for node "model.sales_pipelines.dim_customers"
[0m11:40:46.767343 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.fact_sales"
[0m11:40:46.769099 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.dim_date
[0m11:40:46.771391 [debug] [Thread-5 (]: Began executing node model.sales_pipelines.dim_products
[0m11:40:46.773237 [debug] [Thread-3 (]: Began executing node model.sales_pipelines.dim_customers
[0m11:40:46.809461 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.dim_date"
[0m11:40:46.817484 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.fact_sales
[0m11:40:46.822772 [debug] [Thread-5 (]: Writing runtime sql for node "model.sales_pipelines.dim_products"
[0m11:40:46.826045 [debug] [Thread-3 (]: Writing runtime sql for node "model.sales_pipelines.dim_customers"
[0m11:40:46.835586 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:40:46.835586 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.dim_date"
[0m11:40:46.842643 [debug] [Thread-5 (]: Using snowflake connection "model.sales_pipelines.dim_products"
[0m11:40:46.845835 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: create or replace  temporary view INCREMENTALETL._marts.fact_sales__dbt_tmp
  
  
  
  
  as (
    

SELECT
    invoice_no,
    stock_code,
    customer_id,
    invoice_date::DATE AS date_key,

    quantity,
    unit_price,
    discount,
    shipping_cost,

    gross_amount,
    net_amount,
    total_amount,

    is_returned,
    processed_at

FROM INCREMENTALETL._processed.processed_sales


WHERE processed_at > (SELECT MAX(processed_at) FROM INCREMENTALETL._marts.fact_sales)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */;
[0m11:40:46.845835 [debug] [Thread-3 (]: Using snowflake connection "model.sales_pipelines.dim_customers"
[0m11:40:46.845835 [debug] [Thread-2 (]: On model.sales_pipelines.dim_date: create or replace transient table INCREMENTALETL._marts.dim_date
    
    
    
    as (

SELECT DISTINCT
    invoice_date::DATE AS date_key,
    YEAR(invoice_date)  AS year,
    MONTH(invoice_date) AS month,
    DAY(invoice_date)   AS day,
    DAYNAME(invoice_date) AS day_name,
    WEEK(invoice_date)  AS week_number,
    QUARTER(invoice_date) AS quarter
FROM INCREMENTALETL._processed.processed_sales
    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.dim_date"} */;
[0m11:40:46.845835 [debug] [Thread-5 (]: On model.sales_pipelines.dim_products: create or replace transient table INCREMENTALETL._marts.dim_products
    
    
    
    as (

SELECT DISTINCT
    stock_code,
    description,
    category
FROM INCREMENTALETL._processed.processed_sales
    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.dim_products"} */;
[0m11:40:46.855154 [debug] [Thread-3 (]: On model.sales_pipelines.dim_customers: create or replace transient table INCREMENTALETL._marts.dim_customers
    
    
    
    as (

SELECT DISTINCT
    customer_id,
    country,
    payment_method,
    sales_channel,
    is_guest_customer,
    MIN(invoice_date) AS first_purchase_date,
    MAX(invoice_date) AS last_purchase_date,
    COUNT(DISTINCT invoice_no) AS total_orders
FROM INCREMENTALETL._processed.processed_sales
WHERE customer_id IS NOT NULL
GROUP BY
    customer_id,
    country,
    payment_method,
    sales_channel,
    is_guest_customer
    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.dim_customers"} */;
[0m11:40:46.858723 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m11:40:46.862136 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m11:40:47.088337 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.239 seconds
[0m11:40:47.088337 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:40:47.088337 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: describe table INCREMENTALETL._marts.fact_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:40:47.444589 [debug] [Thread-2 (]: SQL status: SUCCESS 912 in 0.596 seconds
[0m11:40:47.476229 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6a1c17c2-a680-4673-b172-60c40d4d138b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF1EE87AF0>]}
[0m11:40:47.476229 [info ] [Thread-2 (]: 4 of 6 OK created sql table model _marts.dim_date .............................. [[32mSUCCESS 912[0m in 0.75s]
[0m11:40:47.476229 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.dim_date
[0m11:40:48.251346 [debug] [Thread-3 (]: SQL status: SUCCESS 19760 in 1.389 seconds
[0m11:40:48.257401 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6a1c17c2-a680-4673-b172-60c40d4d138b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF1F655F90>]}
[0m11:40:48.257401 [info ] [Thread-3 (]: 3 of 6 OK created sql table model _marts.dim_customers ......................... [[32mSUCCESS 19760[0m in 1.53s]
[0m11:40:48.259411 [debug] [Thread-3 (]: Finished running node model.sales_pipelines.dim_customers
[0m11:40:48.954889 [debug] [Thread-4 (]: SQL status: SUCCESS 13 in 1.858 seconds
[0m11:40:48.954889 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:40:48.970986 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: describe table INCREMENTALETL._marts.fact_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:40:49.115720 [debug] [Thread-4 (]: SQL status: SUCCESS 13 in 0.153 seconds
[0m11:40:49.143622 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:40:49.150193 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: describe table "INCREMENTALETL"."_MARTS"."FACT_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:40:49.301651 [debug] [Thread-4 (]: SQL status: SUCCESS 13 in 0.152 seconds
[0m11:40:49.311899 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.fact_sales"
[0m11:40:49.327806 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:40:49.327806 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */;
[0m11:40:49.459659 [debug] [Thread-5 (]: SQL status: SUCCESS 17702 in 2.606 seconds
[0m11:40:49.478525 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6a1c17c2-a680-4673-b172-60c40d4d138b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF1EF40460>]}
[0m11:40:49.478525 [info ] [Thread-5 (]: 5 of 6 OK created sql table model _marts.dim_products .......................... [[32mSUCCESS 17702[0m in 2.75s]
[0m11:40:49.488713 [debug] [Thread-5 (]: Finished running node model.sales_pipelines.dim_products
[0m11:40:49.488713 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.164 seconds
[0m11:40:49.488713 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:40:49.488713 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: merge into INCREMENTALETL._marts.fact_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._marts.fact_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","DATE_KEY" = DBT_INTERNAL_SOURCE."DATE_KEY","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","GROSS_AMOUNT" = DBT_INTERNAL_SOURCE."GROSS_AMOUNT","NET_AMOUNT" = DBT_INTERNAL_SOURCE."NET_AMOUNT","TOTAL_AMOUNT" = DBT_INTERNAL_SOURCE."TOTAL_AMOUNT","IS_RETURNED" = DBT_INTERNAL_SOURCE."IS_RETURNED","PROCESSED_AT" = DBT_INTERNAL_SOURCE."PROCESSED_AT"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "CUSTOMER_ID", "DATE_KEY", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "PROCESSED_AT")
    values
        ("INVOICE_NO", "STOCK_CODE", "CUSTOMER_ID", "DATE_KEY", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "PROCESSED_AT")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */;
[0m11:40:50.257876 [debug] [Thread-4 (]: SQL status: SUCCESS 2880 in 0.765 seconds
[0m11:40:50.272119 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:40:50.272119 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:40:50.552166 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.273 seconds
[0m11:40:50.572126 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._marts.fact_sales__dbt_tmp
[0m11:40:50.578190 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:40:50.579949 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: drop view if exists INCREMENTALETL._marts.fact_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:40:50.767529 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.200 seconds
[0m11:40:50.783233 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6a1c17c2-a680-4673-b172-60c40d4d138b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF1F7BE6E0>]}
[0m11:40:50.783233 [info ] [Thread-4 (]: 6 of 6 OK created sql incremental model _marts.fact_sales ...................... [[32mSUCCESS 2880[0m in 4.05s]
[0m11:40:50.800778 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.fact_sales
[0m11:40:50.807510 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:40:50.807510 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m11:40:50.807510 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m11:40:51.145321 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m11:40:51.147344 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m11:40:51.386536 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m11:40:51.386536 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m11:40:51.640548 [debug] [MainThread]: Connection 'list_INCREMENTALETL__staging' was left open.
[0m11:40:51.640548 [debug] [MainThread]: On list_INCREMENTALETL__staging: Close
[0m11:40:52.091029 [debug] [MainThread]: Connection 'list_INCREMENTALETL__processed' was left open.
[0m11:40:52.091029 [debug] [MainThread]: On list_INCREMENTALETL__processed: Close
[0m11:40:52.527345 [debug] [MainThread]: Connection 'list_INCREMENTALETL__marts' was left open.
[0m11:40:52.527345 [debug] [MainThread]: On list_INCREMENTALETL__marts: Close
[0m11:40:52.935914 [debug] [MainThread]: Connection 'model.sales_pipelines.dim_date' was left open.
[0m11:40:52.939740 [debug] [MainThread]: On model.sales_pipelines.dim_date: Close
[0m11:40:53.455610 [debug] [MainThread]: Connection 'model.sales_pipelines.fact_sales' was left open.
[0m11:40:53.455610 [debug] [MainThread]: On model.sales_pipelines.fact_sales: Close
[0m11:40:53.764762 [debug] [MainThread]: Connection 'model.sales_pipelines.dim_products' was left open.
[0m11:40:53.768804 [debug] [MainThread]: On model.sales_pipelines.dim_products: Close
[0m11:40:54.059579 [debug] [MainThread]: Connection 'model.sales_pipelines.dim_customers' was left open.
[0m11:40:54.059579 [debug] [MainThread]: On model.sales_pipelines.dim_customers: Close
[0m11:40:54.377347 [info ] [MainThread]: 
[0m11:40:54.377347 [info ] [MainThread]: Finished running 3 incremental models, 3 table models in 0 hours 0 minutes and 17.79 seconds (17.79s).
[0m11:40:54.377347 [debug] [MainThread]: Command end result
[0m11:40:54.428959 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m11:40:54.428959 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m11:40:54.444153 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\snowflake-incremental-pipeline\dbt_project\target\run_results.json
[0m11:40:54.444153 [info ] [MainThread]: 
[0m11:40:54.444153 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:40:54.444153 [info ] [MainThread]: 
[0m11:40:54.444153 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=6
[0m11:40:54.456093 [debug] [MainThread]: Command `dbt run` succeeded at 11:40:54.456093 after 20.41 seconds
[0m11:40:54.458149 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF0A497730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF0B7D28F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF0A30F460>]}
[0m11:40:54.459185 [debug] [MainThread]: Flushing usage events
[0m11:40:56.515813 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:43:28.182326 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000214BB233700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000214BC4E2230>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000214BC4E0AF0>]}


============================== 11:43:28.190122 | 30480710-c9b4-45d7-ac66-3edfab51fe14 ==============================
[0m11:43:28.190122 [info ] [MainThread]: Running with dbt=1.11.6
[0m11:43:28.193540 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'profiles_dir': 'D:\\snowflake-incremental-pipeline\\dbt_project', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'use_colors': 'True', 'invocation_command': 'dbt run', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'D:\\snowflake-incremental-pipeline\\dbt_project\\logs'}
[0m11:43:29.174738 [debug] [MainThread]: Snowflake adapter: Setting snowflake.connector to ERROR (file logging only)
[0m11:43:29.176745 [debug] [MainThread]: Snowflake adapter: Setting botocore to ERROR (file logging only)
[0m11:43:29.176745 [debug] [MainThread]: Snowflake adapter: Setting boto3 to ERROR (file logging only)
[0m11:43:29.474708 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '30480710-c9b4-45d7-ac66-3edfab51fe14', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000214BBCD4A90>]}
[0m11:43:29.551661 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '30480710-c9b4-45d7-ac66-3edfab51fe14', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000214BC57E8F0>]}
[0m11:43:29.551661 [info ] [MainThread]: Registered adapter: snowflake=1.11.2
[0m11:43:30.086356 [debug] [MainThread]: checksum: ec27062d18352e9cfca161c166fcc73d2201661a23a3cf3c89196a27f32cfcb8, vars: {}, profile: , target: , version: 1.11.6
[0m11:43:30.318765 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:43:30.318765 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m11:43:30.318765 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:43:30.398044 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '30480710-c9b4-45d7-ac66-3edfab51fe14', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000214CEBE8A90>]}
[0m11:43:30.540774 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m11:43:30.540774 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m11:43:30.556766 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '30480710-c9b4-45d7-ac66-3edfab51fe14', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000214CD6A4190>]}
[0m11:43:30.568508 [info ] [MainThread]: Found 6 models, 6 data tests, 1 source, 522 macros
[0m11:43:30.568508 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '30480710-c9b4-45d7-ac66-3edfab51fe14', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000214CEBE8370>]}
[0m11:43:30.570260 [info ] [MainThread]: 
[0m11:43:30.572783 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m11:43:30.574543 [info ] [MainThread]: 
[0m11:43:30.575561 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m11:43:30.586424 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m11:43:30.586424 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m11:43:30.611190 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m11:43:30.700919 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m11:43:30.700919 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m11:43:30.700919 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m11:43:30.700919 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m11:43:30.700919 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m11:43:30.700919 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m11:43:30.700919 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:43:30.700919 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:43:30.700919 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:43:31.947079 [debug] [ThreadPool]: SQL status: SUCCESS 5 in 1.240 seconds
[0m11:43:31.956482 [debug] [ThreadPool]: SQL status: SUCCESS 5 in 1.248 seconds
[0m11:43:31.962886 [debug] [ThreadPool]: SQL status: SUCCESS 5 in 1.252 seconds
[0m11:43:31.980222 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__staging'
[0m11:43:31.980222 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__marts'
[0m11:43:32.004393 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__processed'
[0m11:43:32.013483 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__staging"
[0m11:43:32.014717 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__marts"
[0m11:43:32.020242 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__processed"
[0m11:43:32.020242 [debug] [ThreadPool]: On list_INCREMENTALETL__staging: show objects in INCREMENTALETL._staging
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__staging"} */;
[0m11:43:32.020242 [debug] [ThreadPool]: On list_INCREMENTALETL__marts: show objects in INCREMENTALETL._marts
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__marts"} */;
[0m11:43:32.020242 [debug] [ThreadPool]: On list_INCREMENTALETL__processed: show objects in INCREMENTALETL._processed
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__processed"} */;
[0m11:43:32.020242 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:43:32.020242 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:43:32.020242 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:43:32.877030 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 0.853 seconds
[0m11:43:32.992323 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.965 seconds
[0m11:43:33.001450 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.978 seconds
[0m11:43:33.012327 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '30480710-c9b4-45d7-ac66-3edfab51fe14', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000214CD5A5DE0>]}
[0m11:43:33.034981 [debug] [Thread-2 (]: Began running node model.sales_pipelines.stg_sales
[0m11:43:33.039226 [info ] [Thread-2 (]: 1 of 6 START sql incremental model _staging.stg_sales .......................... [RUN]
[0m11:43:33.043376 [debug] [Thread-2 (]: Acquiring new snowflake connection 'model.sales_pipelines.stg_sales'
[0m11:43:33.045397 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.stg_sales
[0m11:43:33.091485 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.stg_sales"
[0m11:43:33.094760 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.stg_sales
[0m11:43:33.169634 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:43:33.169634 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: create or replace  temporary view INCREMENTALETL._staging.stg_sales__dbt_tmp
  
  
  
  
  as (
    

WITH raw_data AS (
    SELECT * 
    FROM INCREMENTALETL.LANDINGZONE.RAW_SALES
),

staged AS (
    SELECT
        -- IDs
        CAST(INVOICENO AS VARCHAR)    AS invoice_no,
        CAST(STOCKCODE AS VARCHAR)    AS stock_code,
        CAST(DESCRIPTION AS VARCHAR)  AS description,

        -- Quantities & Prices
        TRY_CAST(QUANTITY AS INT)      AS quantity,
        TRY_CAST(UNITPRICE AS FLOAT)   AS unit_price,
        TRY_CAST(DISCOUNT AS FLOAT)    AS discount,
        TRY_CAST(SHIPPINGCOST AS FLOAT) AS shipping_cost,
        TRY_CAST(CUSTOMERID AS FLOAT)  AS customer_id,

        -- Convert once
        TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS') AS invoice_date,

        TRIM(UPPER(COUNTRY))            AS country,
        TRIM(UPPER(PAYMENTMETHOD))      AS payment_method,
        TRIM(UPPER(CATEGORY))           AS category,
        TRIM(UPPER(SALESCHANNEL))       AS sales_channel,
        TRIM(UPPER(RETURNSTATUS))       AS return_status,
        TRIM(UPPER(SHIPMENTPROVIDER))   AS shipment_provider,
        TRIM(UPPER(WAREHOUSELOCATION))  AS warehouse_location,
        TRIM(UPPER(ORDERPRIORITY))      AS order_priority,

        CURRENT_TIMESTAMP()             AS stg_loaded_at

    FROM raw_data
),

validated AS (
    SELECT
        *,
        YEAR(invoice_date)  AS invoice_year,
        MONTH(invoice_date) AS invoice_month,

        CASE
            WHEN invoice_date IS NULL           THEN 'INVALID_DATE'
            WHEN quantity IS NULL               THEN 'INVALID_QUANTITY'
            WHEN unit_price IS NULL             THEN 'INVALID_PRICE'
            WHEN shipping_cost < 0              THEN 'NEGATIVE_SHIPPING'
            WHEN discount < 0 OR discount > 1   THEN 'INVALID_DISCOUNT'
            WHEN invoice_no IS NULL             THEN 'NULL_INVOICE'
            ELSE 'VALID'
        END AS data_quality_flag
    FROM staged
)

SELECT *
FROM validated


WHERE NOT EXISTS (
    SELECT 1
    FROM INCREMENTALETL._staging.stg_sales t
    WHERE t.invoice_year  = validated.invoice_year
      AND t.invoice_month = validated.invoice_month
)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m11:43:33.169634 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m11:43:34.319719 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 1.147 seconds
[0m11:43:34.347580 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:43:34.347580 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:43:34.508481 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.164 seconds
[0m11:43:34.524277 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:43:34.524277 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:43:34.735630 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.203 seconds
[0m11:43:34.785254 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:43:34.785254 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:43:34.959206 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.173 seconds
[0m11:43:34.975312 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:43:34.975312 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table "INCREMENTALETL"."_STAGING"."STG_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:43:35.141228 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.155 seconds
[0m11:43:35.182467 [debug] [Thread-2 (]: 
    In "INCREMENTALETL"."_STAGING"."STG_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m11:43:35.220937 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.stg_sales"
[0m11:43:35.228936 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:43:35.228936 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m11:43:35.440699 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.212 seconds
[0m11:43:35.445128 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:43:35.446605 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: merge into INCREMENTALETL._staging.stg_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._staging.stg_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","DATA_QUALITY_FLAG" = DBT_INTERNAL_SOURCE."DATA_QUALITY_FLAG"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "INVOICE_YEAR", "INVOICE_MONTH", "DATA_QUALITY_FLAG")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "INVOICE_YEAR", "INVOICE_MONTH", "DATA_QUALITY_FLAG")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m11:43:36.567073 [debug] [Thread-2 (]: SQL status: SUCCESS 5832 in 1.117 seconds
[0m11:43:36.567073 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:43:36.567073 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:43:36.882515 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.312 seconds
[0m11:43:36.898331 [debug] [Thread-2 (]: Applying DROP to: INCREMENTALETL._staging.stg_sales__dbt_tmp
[0m11:43:36.898331 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:43:36.898331 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: drop view if exists INCREMENTALETL._staging.stg_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:43:37.076502 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.179 seconds
[0m11:43:37.140006 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '30480710-c9b4-45d7-ac66-3edfab51fe14', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000214BA0B9810>]}
[0m11:43:37.140006 [info ] [Thread-2 (]: 1 of 6 OK created sql incremental model _staging.stg_sales ..................... [[32mSUCCESS 5832[0m in 4.10s]
[0m11:43:37.140006 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.stg_sales
[0m11:43:37.140006 [debug] [Thread-4 (]: Began running node model.sales_pipelines.processed_sales
[0m11:43:37.140006 [info ] [Thread-4 (]: 2 of 6 START sql incremental model _processed.processed_sales .................. [RUN]
[0m11:43:37.140006 [debug] [Thread-4 (]: Acquiring new snowflake connection 'model.sales_pipelines.processed_sales'
[0m11:43:37.140006 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.processed_sales
[0m11:43:37.158130 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.processed_sales"
[0m11:43:37.161504 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.processed_sales
[0m11:43:37.167979 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:43:37.172030 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: create or replace  temporary view INCREMENTALETL._processed.processed_sales__dbt_tmp
  
  
  
  
  as (
    

WITH staging AS (
    SELECT * 
    FROM INCREMENTALETL._staging.stg_sales
),

deduped AS (
    SELECT *,
        ROW_NUMBER() OVER (
            PARTITION BY invoice_no, stock_code
            ORDER BY stg_loaded_at DESC
        ) AS row_num
    FROM staging
),

cleaned AS (
    SELECT
        invoice_no,
        stock_code,
        description,
        quantity,
        unit_price,
        discount,
        shipping_cost,
        customer_id,
        invoice_date,
        invoice_year,
        invoice_month,
        country,
        payment_method,
        category,
        sales_channel,
        return_status,
        shipment_provider,
        warehouse_location,
        order_priority,

        -- Derived metrics
        ROUND(quantity * unit_price, 2) AS gross_amount,
        ROUND(quantity * unit_price * (1 - discount), 2) AS net_amount,
        ROUND(
            quantity * unit_price * (1 - discount)
            + COALESCE(shipping_cost, 0),
        2) AS total_amount,

        CASE WHEN return_status = 'RETURNED' THEN TRUE ELSE FALSE END AS is_returned,
        CASE WHEN customer_id IS NULL THEN TRUE ELSE FALSE END AS is_guest_customer,

        stg_loaded_at,
        CURRENT_TIMESTAMP() AS processed_at

    FROM deduped
    WHERE
        row_num = 1
        AND data_quality_flag = 'VALID'
        AND invoice_date IS NOT NULL
)

SELECT *
FROM cleaned


WHERE NOT EXISTS (
    SELECT 1
    FROM INCREMENTALETL._processed.processed_sales t
    WHERE t.invoice_year  = cleaned.invoice_year
      AND t.invoice_month = cleaned.invoice_month
)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m11:43:37.172030 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m11:43:38.107474 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.934 seconds
[0m11:43:38.114512 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:43:38.116520 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:43:38.311039 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.193 seconds
[0m11:43:38.332991 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:43:38.335007 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:43:38.512447 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.177 seconds
[0m11:43:38.535750 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:43:38.535750 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:43:38.716419 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.176 seconds
[0m11:43:38.743278 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:43:38.743278 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:43:38.920393 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.178 seconds
[0m11:43:38.933828 [debug] [Thread-4 (]: 
    In "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m11:43:38.933828 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.processed_sales"
[0m11:43:38.943697 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:43:38.943697 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m11:43:39.128895 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.188 seconds
[0m11:43:39.128895 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:43:39.128895 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: merge into INCREMENTALETL._processed.processed_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._processed.processed_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","GROSS_AMOUNT" = DBT_INTERNAL_SOURCE."GROSS_AMOUNT","NET_AMOUNT" = DBT_INTERNAL_SOURCE."NET_AMOUNT","TOTAL_AMOUNT" = DBT_INTERNAL_SOURCE."TOTAL_AMOUNT","IS_RETURNED" = DBT_INTERNAL_SOURCE."IS_RETURNED","IS_GUEST_CUSTOMER" = DBT_INTERNAL_SOURCE."IS_GUEST_CUSTOMER","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","PROCESSED_AT" = DBT_INTERNAL_SOURCE."PROCESSED_AT"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m11:43:40.253365 [debug] [Thread-4 (]: SQL status: SUCCESS 5705 in 1.112 seconds
[0m11:43:40.253365 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:43:40.253365 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:43:40.481732 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.235 seconds
[0m11:43:40.497794 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._processed.processed_sales__dbt_tmp
[0m11:43:40.513981 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:43:40.513981 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: drop view if exists INCREMENTALETL._processed.processed_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:43:40.690734 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.173 seconds
[0m11:43:40.690734 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '30480710-c9b4-45d7-ac66-3edfab51fe14', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000214CEC2E170>]}
[0m11:43:40.706703 [info ] [Thread-4 (]: 2 of 6 OK created sql incremental model _processed.processed_sales ............. [[32mSUCCESS 5705[0m in 3.55s]
[0m11:43:40.706703 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.processed_sales
[0m11:43:40.718408 [debug] [Thread-3 (]: Began running node model.sales_pipelines.dim_customers
[0m11:43:40.719875 [debug] [Thread-2 (]: Began running node model.sales_pipelines.dim_date
[0m11:43:40.724403 [debug] [Thread-5 (]: Began running node model.sales_pipelines.dim_products
[0m11:43:40.726666 [debug] [Thread-4 (]: Began running node model.sales_pipelines.fact_sales
[0m11:43:40.730449 [info ] [Thread-3 (]: 3 of 6 START sql table model _marts.dim_customers .............................. [RUN]
[0m11:43:40.734493 [info ] [Thread-2 (]: 4 of 6 START sql table model _marts.dim_date ................................... [RUN]
[0m11:43:40.740106 [info ] [Thread-5 (]: 5 of 6 START sql table model _marts.dim_products ............................... [RUN]
[0m11:43:40.740106 [info ] [Thread-4 (]: 6 of 6 START sql incremental model _marts.fact_sales ........................... [RUN]
[0m11:43:40.748704 [debug] [Thread-3 (]: Acquiring new snowflake connection 'model.sales_pipelines.dim_customers'
[0m11:43:40.754606 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.sales_pipelines.stg_sales, now model.sales_pipelines.dim_date)
[0m11:43:40.756627 [debug] [Thread-5 (]: Acquiring new snowflake connection 'model.sales_pipelines.dim_products'
[0m11:43:40.756627 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.sales_pipelines.processed_sales, now model.sales_pipelines.fact_sales)
[0m11:43:40.756627 [debug] [Thread-3 (]: Began compiling node model.sales_pipelines.dim_customers
[0m11:43:40.762563 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.dim_date
[0m11:43:40.764860 [debug] [Thread-5 (]: Began compiling node model.sales_pipelines.dim_products
[0m11:43:40.767367 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.fact_sales
[0m11:43:40.773756 [debug] [Thread-3 (]: Writing injected SQL for node "model.sales_pipelines.dim_customers"
[0m11:43:40.781053 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.dim_date"
[0m11:43:40.790132 [debug] [Thread-5 (]: Writing injected SQL for node "model.sales_pipelines.dim_products"
[0m11:43:40.795672 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.fact_sales"
[0m11:43:40.795672 [debug] [Thread-3 (]: Began executing node model.sales_pipelines.dim_customers
[0m11:43:40.795672 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.dim_date
[0m11:43:40.804175 [debug] [Thread-5 (]: Began executing node model.sales_pipelines.dim_products
[0m11:43:40.848202 [debug] [Thread-3 (]: Writing runtime sql for node "model.sales_pipelines.dim_customers"
[0m11:43:40.850125 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.fact_sales
[0m11:43:40.852731 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.dim_date"
[0m11:43:40.861102 [debug] [Thread-5 (]: Writing runtime sql for node "model.sales_pipelines.dim_products"
[0m11:43:40.869942 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:43:40.869942 [debug] [Thread-3 (]: Using snowflake connection "model.sales_pipelines.dim_customers"
[0m11:43:40.880900 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.dim_date"
[0m11:43:40.880900 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: create or replace  temporary view INCREMENTALETL._marts.fact_sales__dbt_tmp
  
  
  
  
  as (
    

SELECT
    invoice_no,
    stock_code,
    customer_id,
    invoice_date::DATE AS date_key,

    quantity,
    unit_price,
    discount,
    shipping_cost,

    gross_amount,
    net_amount,
    total_amount,

    is_returned,
    processed_at

FROM INCREMENTALETL._processed.processed_sales


WHERE processed_at > (SELECT MAX(processed_at) FROM INCREMENTALETL._marts.fact_sales)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */;
[0m11:43:40.884663 [debug] [Thread-5 (]: Using snowflake connection "model.sales_pipelines.dim_products"
[0m11:43:40.884663 [debug] [Thread-3 (]: On model.sales_pipelines.dim_customers: create or replace transient table INCREMENTALETL._marts.dim_customers
    
    
    
    as (

SELECT DISTINCT
    customer_id,
    country,
    payment_method,
    sales_channel,
    is_guest_customer,
    MIN(invoice_date) AS first_purchase_date,
    MAX(invoice_date) AS last_purchase_date,
    COUNT(DISTINCT invoice_no) AS total_orders
FROM INCREMENTALETL._processed.processed_sales
WHERE customer_id IS NOT NULL
GROUP BY
    customer_id,
    country,
    payment_method,
    sales_channel,
    is_guest_customer
    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.dim_customers"} */;
[0m11:43:40.884663 [debug] [Thread-2 (]: On model.sales_pipelines.dim_date: create or replace transient table INCREMENTALETL._marts.dim_date
    
    
    
    as (

SELECT DISTINCT
    invoice_date::DATE AS date_key,
    YEAR(invoice_date)  AS year,
    MONTH(invoice_date) AS month,
    DAY(invoice_date)   AS day,
    DAYNAME(invoice_date) AS day_name,
    WEEK(invoice_date)  AS week_number,
    QUARTER(invoice_date) AS quarter
FROM INCREMENTALETL._processed.processed_sales
    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.dim_date"} */;
[0m11:43:40.891748 [debug] [Thread-5 (]: On model.sales_pipelines.dim_products: create or replace transient table INCREMENTALETL._marts.dim_products
    
    
    
    as (

SELECT DISTINCT
    stock_code,
    description,
    category
FROM INCREMENTALETL._processed.processed_sales
    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.dim_products"} */;
[0m11:43:40.894343 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m11:43:40.900379 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m11:43:41.118708 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.230 seconds
[0m11:43:41.126542 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:43:41.126542 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: describe table INCREMENTALETL._marts.fact_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:43:41.280305 [debug] [Thread-4 (]: SQL status: SUCCESS 13 in 0.152 seconds
[0m11:43:41.303587 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:43:41.303587 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: describe table INCREMENTALETL._marts.fact_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:43:41.488335 [debug] [Thread-4 (]: SQL status: SUCCESS 13 in 0.181 seconds
[0m11:43:41.495800 [debug] [Thread-2 (]: SQL status: SUCCESS 1155 in 0.602 seconds
[0m11:43:41.511635 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:43:41.543996 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '30480710-c9b4-45d7-ac66-3edfab51fe14', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000214CF464850>]}
[0m11:43:41.560440 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: describe table "INCREMENTALETL"."_MARTS"."FACT_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:43:41.560440 [info ] [Thread-2 (]: 4 of 6 OK created sql table model _marts.dim_date .............................. [[32mSUCCESS 1155[0m in 0.79s]
[0m11:43:41.578378 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.dim_date
[0m11:43:41.722831 [debug] [Thread-4 (]: SQL status: SUCCESS 13 in 0.152 seconds
[0m11:43:41.738165 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.fact_sales"
[0m11:43:41.748529 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:43:41.750546 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */;
[0m11:43:41.923968 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.172 seconds
[0m11:43:41.923968 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:43:41.923968 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: merge into INCREMENTALETL._marts.fact_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._marts.fact_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","DATE_KEY" = DBT_INTERNAL_SOURCE."DATE_KEY","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","GROSS_AMOUNT" = DBT_INTERNAL_SOURCE."GROSS_AMOUNT","NET_AMOUNT" = DBT_INTERNAL_SOURCE."NET_AMOUNT","TOTAL_AMOUNT" = DBT_INTERNAL_SOURCE."TOTAL_AMOUNT","IS_RETURNED" = DBT_INTERNAL_SOURCE."IS_RETURNED","PROCESSED_AT" = DBT_INTERNAL_SOURCE."PROCESSED_AT"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "CUSTOMER_ID", "DATE_KEY", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "PROCESSED_AT")
    values
        ("INVOICE_NO", "STOCK_CODE", "CUSTOMER_ID", "DATE_KEY", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "PROCESSED_AT")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */;
[0m11:43:42.182080 [debug] [Thread-5 (]: SQL status: SUCCESS 21378 in 1.289 seconds
[0m11:43:42.197823 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '30480710-c9b4-45d7-ac66-3edfab51fe14', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000214CF465270>]}
[0m11:43:42.197823 [info ] [Thread-5 (]: 5 of 6 OK created sql table model _marts.dim_products .......................... [[32mSUCCESS 21378[0m in 1.44s]
[0m11:43:42.197823 [debug] [Thread-5 (]: Finished running node model.sales_pipelines.dim_products
[0m11:43:42.239691 [debug] [Thread-3 (]: SQL status: SUCCESS 24971 in 1.344 seconds
[0m11:43:42.251003 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '30480710-c9b4-45d7-ac66-3edfab51fe14', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000214CD6E7C10>]}
[0m11:43:42.251003 [info ] [Thread-3 (]: 3 of 6 OK created sql table model _marts.dim_customers ......................... [[32mSUCCESS 24971[0m in 1.50s]
[0m11:43:42.251003 [debug] [Thread-3 (]: Finished running node model.sales_pipelines.dim_customers
[0m11:43:42.652656 [debug] [Thread-4 (]: SQL status: SUCCESS 5705 in 0.730 seconds
[0m11:43:42.667491 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:43:42.667491 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:43:43.033851 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.361 seconds
[0m11:43:43.041632 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._marts.fact_sales__dbt_tmp
[0m11:43:43.043641 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:43:43.043641 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: drop view if exists INCREMENTALETL._marts.fact_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:43:43.224706 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.179 seconds
[0m11:43:43.228728 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '30480710-c9b4-45d7-ac66-3edfab51fe14', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000214CF2B32E0>]}
[0m11:43:43.230736 [info ] [Thread-4 (]: 6 of 6 OK created sql incremental model _marts.fact_sales ...................... [[32mSUCCESS 5705[0m in 2.47s]
[0m11:43:43.230736 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.fact_sales
[0m11:43:43.233849 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:43:43.236010 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m11:43:43.236010 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m11:43:43.466568 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m11:43:43.477448 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m11:43:43.719623 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m11:43:43.719623 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m11:43:44.042994 [debug] [MainThread]: Connection 'list_INCREMENTALETL__staging' was left open.
[0m11:43:44.052165 [debug] [MainThread]: On list_INCREMENTALETL__staging: Close
[0m11:43:44.460613 [debug] [MainThread]: Connection 'list_INCREMENTALETL__marts' was left open.
[0m11:43:44.460613 [debug] [MainThread]: On list_INCREMENTALETL__marts: Close
[0m11:43:44.779134 [debug] [MainThread]: Connection 'list_INCREMENTALETL__processed' was left open.
[0m11:43:44.779134 [debug] [MainThread]: On list_INCREMENTALETL__processed: Close
[0m11:43:45.776220 [debug] [MainThread]: Connection 'model.sales_pipelines.dim_date' was left open.
[0m11:43:45.776220 [debug] [MainThread]: On model.sales_pipelines.dim_date: Close
[0m11:43:46.126307 [debug] [MainThread]: Connection 'model.sales_pipelines.fact_sales' was left open.
[0m11:43:46.126307 [debug] [MainThread]: On model.sales_pipelines.fact_sales: Close
[0m11:43:46.505001 [debug] [MainThread]: Connection 'model.sales_pipelines.dim_customers' was left open.
[0m11:43:46.505001 [debug] [MainThread]: On model.sales_pipelines.dim_customers: Close
[0m11:43:46.808127 [debug] [MainThread]: Connection 'model.sales_pipelines.dim_products' was left open.
[0m11:43:46.813830 [debug] [MainThread]: On model.sales_pipelines.dim_products: Close
[0m11:43:47.126932 [info ] [MainThread]: 
[0m11:43:47.126932 [info ] [MainThread]: Finished running 3 incremental models, 3 table models in 0 hours 0 minutes and 16.55 seconds (16.55s).
[0m11:43:47.142984 [debug] [MainThread]: Command end result
[0m11:43:47.199428 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m11:43:47.207818 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m11:43:47.215951 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\snowflake-incremental-pipeline\dbt_project\target\run_results.json
[0m11:43:47.215951 [info ] [MainThread]: 
[0m11:43:47.220870 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:43:47.222527 [info ] [MainThread]: 
[0m11:43:47.222527 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=6
[0m11:43:47.224155 [debug] [MainThread]: Command `dbt run` succeeded at 11:43:47.224155 after 19.18 seconds
[0m11:43:47.225623 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000214BB233700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000214BBCD66E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000214BB5FC040>]}
[0m11:43:47.225623 [debug] [MainThread]: Flushing usage events
[0m11:43:49.560596 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:44:45.494067 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FFBF117670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FFC03C7340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FFC03C69B0>]}


============================== 11:44:45.495829 | d97619d0-9517-48b4-8af0-5b63f3ebd116 ==============================
[0m11:44:45.495829 [info ] [MainThread]: Running with dbt=1.11.6
[0m11:44:45.495829 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'D:\\snowflake-incremental-pipeline\\dbt_project', 'use_colors': 'True', 'indirect_selection': 'eager', 'invocation_command': 'dbt run', 'use_experimental_parser': 'False', 'log_path': 'D:\\snowflake-incremental-pipeline\\dbt_project\\logs'}
[0m11:44:46.462750 [debug] [MainThread]: Snowflake adapter: Setting snowflake.connector to ERROR (file logging only)
[0m11:44:46.462750 [debug] [MainThread]: Snowflake adapter: Setting botocore to ERROR (file logging only)
[0m11:44:46.464761 [debug] [MainThread]: Snowflake adapter: Setting boto3 to ERROR (file logging only)
[0m11:44:46.758048 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd97619d0-9517-48b4-8af0-5b63f3ebd116', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FFD15552A0>]}
[0m11:44:46.837203 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd97619d0-9517-48b4-8af0-5b63f3ebd116', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FFC021E140>]}
[0m11:44:46.837203 [info ] [MainThread]: Registered adapter: snowflake=1.11.2
[0m11:44:47.363920 [debug] [MainThread]: checksum: ec27062d18352e9cfca161c166fcc73d2201661a23a3cf3c89196a27f32cfcb8, vars: {}, profile: , target: , version: 1.11.6
[0m11:44:47.600242 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:44:47.600242 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m11:44:47.600242 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:44:47.679331 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd97619d0-9517-48b4-8af0-5b63f3ebd116', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FFD2ACCA00>]}
[0m11:44:47.822199 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m11:44:47.836365 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m11:44:47.854248 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd97619d0-9517-48b4-8af0-5b63f3ebd116', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FFD189B8B0>]}
[0m11:44:47.854248 [info ] [MainThread]: Found 6 models, 6 data tests, 1 source, 522 macros
[0m11:44:47.854248 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd97619d0-9517-48b4-8af0-5b63f3ebd116', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FFD2ACC400>]}
[0m11:44:47.860204 [info ] [MainThread]: 
[0m11:44:47.860204 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m11:44:47.864349 [info ] [MainThread]: 
[0m11:44:47.865306 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m11:44:47.873068 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m11:44:47.896228 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m11:44:47.904157 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m11:44:48.041832 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m11:44:48.041832 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m11:44:48.041832 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m11:44:48.041832 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m11:44:48.041832 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m11:44:48.041832 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m11:44:48.041832 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:44:48.041832 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:44:48.056357 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:44:49.247573 [debug] [ThreadPool]: SQL status: SUCCESS 5 in 1.196 seconds
[0m11:44:49.257555 [debug] [ThreadPool]: SQL status: SUCCESS 5 in 1.202 seconds
[0m11:44:49.266190 [debug] [ThreadPool]: SQL status: SUCCESS 5 in 1.211 seconds
[0m11:44:49.287176 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__processed'
[0m11:44:49.287176 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__staging'
[0m11:44:49.313497 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__marts'
[0m11:44:49.327795 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__processed"
[0m11:44:49.343500 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__staging"
[0m11:44:49.359520 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__marts"
[0m11:44:49.359520 [debug] [ThreadPool]: On list_INCREMENTALETL__processed: show objects in INCREMENTALETL._processed
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__processed"} */;
[0m11:44:49.363122 [debug] [ThreadPool]: On list_INCREMENTALETL__staging: show objects in INCREMENTALETL._staging
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__staging"} */;
[0m11:44:49.363122 [debug] [ThreadPool]: On list_INCREMENTALETL__marts: show objects in INCREMENTALETL._marts
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__marts"} */;
[0m11:44:49.363122 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:44:49.363122 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:44:49.363122 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:44:50.208701 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.846 seconds
[0m11:44:50.218823 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 0.850 seconds
[0m11:44:50.272510 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.907 seconds
[0m11:44:50.272510 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd97619d0-9517-48b4-8af0-5b63f3ebd116', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FFD2B27AF0>]}
[0m11:44:50.290791 [debug] [Thread-2 (]: Began running node model.sales_pipelines.stg_sales
[0m11:44:50.307579 [info ] [Thread-2 (]: 1 of 6 START sql incremental model _staging.stg_sales .......................... [RUN]
[0m11:44:50.311056 [debug] [Thread-2 (]: Acquiring new snowflake connection 'model.sales_pipelines.stg_sales'
[0m11:44:50.311056 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.stg_sales
[0m11:44:50.346933 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.stg_sales"
[0m11:44:50.361997 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.stg_sales
[0m11:44:50.434688 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:44:50.434688 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: create or replace  temporary view INCREMENTALETL._staging.stg_sales__dbt_tmp
  
  
  
  
  as (
    

WITH raw_data AS (
    SELECT * 
    FROM INCREMENTALETL.LANDINGZONE.RAW_SALES
),

staged AS (
    SELECT
        -- IDs
        CAST(INVOICENO AS VARCHAR)    AS invoice_no,
        CAST(STOCKCODE AS VARCHAR)    AS stock_code,
        CAST(DESCRIPTION AS VARCHAR)  AS description,

        -- Quantities & Prices
        TRY_CAST(QUANTITY AS INT)      AS quantity,
        TRY_CAST(UNITPRICE AS FLOAT)   AS unit_price,
        TRY_CAST(DISCOUNT AS FLOAT)    AS discount,
        TRY_CAST(SHIPPINGCOST AS FLOAT) AS shipping_cost,
        TRY_CAST(CUSTOMERID AS FLOAT)  AS customer_id,

        -- Convert once
        TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS') AS invoice_date,

        TRIM(UPPER(COUNTRY))            AS country,
        TRIM(UPPER(PAYMENTMETHOD))      AS payment_method,
        TRIM(UPPER(CATEGORY))           AS category,
        TRIM(UPPER(SALESCHANNEL))       AS sales_channel,
        TRIM(UPPER(RETURNSTATUS))       AS return_status,
        TRIM(UPPER(SHIPMENTPROVIDER))   AS shipment_provider,
        TRIM(UPPER(WAREHOUSELOCATION))  AS warehouse_location,
        TRIM(UPPER(ORDERPRIORITY))      AS order_priority,

        CURRENT_TIMESTAMP()             AS stg_loaded_at

    FROM raw_data
),

validated AS (
    SELECT
        *,
        YEAR(invoice_date)  AS invoice_year,
        MONTH(invoice_date) AS invoice_month,

        CASE
            WHEN invoice_date IS NULL           THEN 'INVALID_DATE'
            WHEN quantity IS NULL               THEN 'INVALID_QUANTITY'
            WHEN unit_price IS NULL             THEN 'INVALID_PRICE'
            WHEN shipping_cost < 0              THEN 'NEGATIVE_SHIPPING'
            WHEN discount < 0 OR discount > 1   THEN 'INVALID_DISCOUNT'
            WHEN invoice_no IS NULL             THEN 'NULL_INVOICE'
            ELSE 'VALID'
        END AS data_quality_flag
    FROM staged
)

SELECT *
FROM validated


WHERE NOT EXISTS (
    SELECT 1
    FROM INCREMENTALETL._staging.stg_sales t
    WHERE t.invoice_year  = validated.invoice_year
      AND t.invoice_month = validated.invoice_month
)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m11:44:50.434688 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m11:44:51.318400 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.879 seconds
[0m11:44:51.352581 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:44:51.352581 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:44:51.519190 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.160 seconds
[0m11:44:51.536613 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:44:51.536613 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:44:51.704710 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.161 seconds
[0m11:44:51.744008 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:44:51.744008 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:44:51.896374 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.142 seconds
[0m11:44:51.921140 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:44:51.921140 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table "INCREMENTALETL"."_STAGING"."STG_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:44:52.051671 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.135 seconds
[0m11:44:52.099112 [debug] [Thread-2 (]: 
    In "INCREMENTALETL"."_STAGING"."STG_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m11:44:52.132052 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.stg_sales"
[0m11:44:52.139584 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:44:52.139584 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m11:44:52.294723 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.165 seconds
[0m11:44:52.294723 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:44:52.310569 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: merge into INCREMENTALETL._staging.stg_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._staging.stg_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","DATA_QUALITY_FLAG" = DBT_INTERNAL_SOURCE."DATA_QUALITY_FLAG"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "INVOICE_YEAR", "INVOICE_MONTH", "DATA_QUALITY_FLAG")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "INVOICE_YEAR", "INVOICE_MONTH", "DATA_QUALITY_FLAG")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m11:44:53.467935 [debug] [Thread-2 (]: SQL status: SUCCESS 3672 in 1.154 seconds
[0m11:44:53.467935 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:44:53.467935 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:44:53.780803 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.307 seconds
[0m11:44:53.789799 [debug] [Thread-2 (]: Applying DROP to: INCREMENTALETL._staging.stg_sales__dbt_tmp
[0m11:44:53.805580 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:44:53.805580 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: drop view if exists INCREMENTALETL._staging.stg_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:44:53.985482 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.171 seconds
[0m11:44:54.032858 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd97619d0-9517-48b4-8af0-5b63f3ebd116', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FFD1398910>]}
[0m11:44:54.032858 [info ] [Thread-2 (]: 1 of 6 OK created sql incremental model _staging.stg_sales ..................... [[32mSUCCESS 3672[0m in 3.72s]
[0m11:44:54.032858 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.stg_sales
[0m11:44:54.032858 [debug] [Thread-4 (]: Began running node model.sales_pipelines.processed_sales
[0m11:44:54.049780 [info ] [Thread-4 (]: 2 of 6 START sql incremental model _processed.processed_sales .................. [RUN]
[0m11:44:54.049780 [debug] [Thread-4 (]: Acquiring new snowflake connection 'model.sales_pipelines.processed_sales'
[0m11:44:54.052113 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.processed_sales
[0m11:44:54.057244 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.processed_sales"
[0m11:44:54.057244 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.processed_sales
[0m11:44:54.071050 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:44:54.071050 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: create or replace  temporary view INCREMENTALETL._processed.processed_sales__dbt_tmp
  
  
  
  
  as (
    

WITH staging AS (
    SELECT * 
    FROM INCREMENTALETL._staging.stg_sales
),

deduped AS (
    SELECT *,
        ROW_NUMBER() OVER (
            PARTITION BY invoice_no, stock_code
            ORDER BY stg_loaded_at DESC
        ) AS row_num
    FROM staging
),

cleaned AS (
    SELECT
        invoice_no,
        stock_code,
        description,
        quantity,
        unit_price,
        discount,
        shipping_cost,
        customer_id,
        invoice_date,
        invoice_year,
        invoice_month,
        country,
        payment_method,
        category,
        sales_channel,
        return_status,
        shipment_provider,
        warehouse_location,
        order_priority,

        -- Derived metrics
        ROUND(quantity * unit_price, 2) AS gross_amount,
        ROUND(quantity * unit_price * (1 - discount), 2) AS net_amount,
        ROUND(
            quantity * unit_price * (1 - discount)
            + COALESCE(shipping_cost, 0),
        2) AS total_amount,

        CASE WHEN return_status = 'RETURNED' THEN TRUE ELSE FALSE END AS is_returned,
        CASE WHEN customer_id IS NULL THEN TRUE ELSE FALSE END AS is_guest_customer,

        stg_loaded_at,
        CURRENT_TIMESTAMP() AS processed_at

    FROM deduped
    WHERE
        row_num = 1
        AND data_quality_flag = 'VALID'
        AND invoice_date IS NOT NULL
)

SELECT *
FROM cleaned


WHERE NOT EXISTS (
    SELECT 1
    FROM INCREMENTALETL._processed.processed_sales t
    WHERE t.invoice_year  = cleaned.invoice_year
      AND t.invoice_month = cleaned.invoice_month
)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m11:44:54.073686 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m11:44:55.104779 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 1.042 seconds
[0m11:44:55.121226 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:44:55.121226 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:44:55.311669 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.189 seconds
[0m11:44:55.323741 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:44:55.339565 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:44:55.516031 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.175 seconds
[0m11:44:55.533940 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:44:55.533940 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:44:55.708742 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.161 seconds
[0m11:44:55.708742 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:44:55.708742 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:44:56.541420 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.818 seconds
[0m11:44:56.546034 [debug] [Thread-4 (]: 
    In "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m11:44:56.560253 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.processed_sales"
[0m11:44:56.564937 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:44:56.564937 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m11:44:57.368647 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.799 seconds
[0m11:44:57.368647 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:44:57.368647 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: merge into INCREMENTALETL._processed.processed_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._processed.processed_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","GROSS_AMOUNT" = DBT_INTERNAL_SOURCE."GROSS_AMOUNT","NET_AMOUNT" = DBT_INTERNAL_SOURCE."NET_AMOUNT","TOTAL_AMOUNT" = DBT_INTERNAL_SOURCE."TOTAL_AMOUNT","IS_RETURNED" = DBT_INTERNAL_SOURCE."IS_RETURNED","IS_GUEST_CUSTOMER" = DBT_INTERNAL_SOURCE."IS_GUEST_CUSTOMER","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","PROCESSED_AT" = DBT_INTERNAL_SOURCE."PROCESSED_AT"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m11:44:58.583508 [debug] [Thread-4 (]: SQL status: SUCCESS 3600 in 1.225 seconds
[0m11:44:58.599488 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:44:58.599488 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:44:58.895260 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.298 seconds
[0m11:44:58.911161 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._processed.processed_sales__dbt_tmp
[0m11:44:58.911161 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:44:58.911161 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: drop view if exists INCREMENTALETL._processed.processed_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:44:59.121274 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.200 seconds
[0m11:44:59.131390 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd97619d0-9517-48b4-8af0-5b63f3ebd116', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FFD1556AA0>]}
[0m11:44:59.136997 [info ] [Thread-4 (]: 2 of 6 OK created sql incremental model _processed.processed_sales ............. [[32mSUCCESS 3600[0m in 5.08s]
[0m11:44:59.136997 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.processed_sales
[0m11:44:59.139613 [debug] [Thread-3 (]: Began running node model.sales_pipelines.dim_customers
[0m11:44:59.139613 [debug] [Thread-2 (]: Began running node model.sales_pipelines.dim_date
[0m11:44:59.139613 [debug] [Thread-5 (]: Began running node model.sales_pipelines.dim_products
[0m11:44:59.139613 [debug] [Thread-4 (]: Began running node model.sales_pipelines.fact_sales
[0m11:44:59.139613 [info ] [Thread-3 (]: 3 of 6 START sql table model _marts.dim_customers .............................. [RUN]
[0m11:44:59.147836 [info ] [Thread-2 (]: 4 of 6 START sql table model _marts.dim_date ................................... [RUN]
[0m11:44:59.148638 [info ] [Thread-5 (]: 5 of 6 START sql table model _marts.dim_products ............................... [RUN]
[0m11:44:59.148638 [info ] [Thread-4 (]: 6 of 6 START sql incremental model _marts.fact_sales ........................... [RUN]
[0m11:44:59.153915 [debug] [Thread-3 (]: Acquiring new snowflake connection 'model.sales_pipelines.dim_customers'
[0m11:44:59.156994 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.sales_pipelines.stg_sales, now model.sales_pipelines.dim_date)
[0m11:44:59.160142 [debug] [Thread-5 (]: Acquiring new snowflake connection 'model.sales_pipelines.dim_products'
[0m11:44:59.161865 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.sales_pipelines.processed_sales, now model.sales_pipelines.fact_sales)
[0m11:44:59.163068 [debug] [Thread-3 (]: Began compiling node model.sales_pipelines.dim_customers
[0m11:44:59.165757 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.dim_date
[0m11:44:59.167620 [debug] [Thread-5 (]: Began compiling node model.sales_pipelines.dim_products
[0m11:44:59.167620 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.fact_sales
[0m11:44:59.181887 [debug] [Thread-3 (]: Writing injected SQL for node "model.sales_pipelines.dim_customers"
[0m11:44:59.191344 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.dim_date"
[0m11:44:59.200447 [debug] [Thread-5 (]: Writing injected SQL for node "model.sales_pipelines.dim_products"
[0m11:44:59.202479 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.fact_sales"
[0m11:44:59.202479 [debug] [Thread-3 (]: Began executing node model.sales_pipelines.dim_customers
[0m11:44:59.211265 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.dim_date
[0m11:44:59.213199 [debug] [Thread-5 (]: Began executing node model.sales_pipelines.dim_products
[0m11:44:59.254150 [debug] [Thread-3 (]: Writing runtime sql for node "model.sales_pipelines.dim_customers"
[0m11:44:59.254150 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.fact_sales
[0m11:44:59.257115 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.dim_date"
[0m11:44:59.257115 [debug] [Thread-5 (]: Writing runtime sql for node "model.sales_pipelines.dim_products"
[0m11:44:59.270278 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:44:59.270278 [debug] [Thread-3 (]: Using snowflake connection "model.sales_pipelines.dim_customers"
[0m11:44:59.283501 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.dim_date"
[0m11:44:59.283501 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: create or replace  temporary view INCREMENTALETL._marts.fact_sales__dbt_tmp
  
  
  
  
  as (
    

SELECT
    invoice_no,
    stock_code,
    customer_id,
    invoice_date::DATE AS date_key,

    quantity,
    unit_price,
    discount,
    shipping_cost,

    gross_amount,
    net_amount,
    total_amount,

    is_returned,
    processed_at

FROM INCREMENTALETL._processed.processed_sales


WHERE processed_at > (SELECT MAX(processed_at) FROM INCREMENTALETL._marts.fact_sales)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */;
[0m11:44:59.286145 [debug] [Thread-5 (]: Using snowflake connection "model.sales_pipelines.dim_products"
[0m11:44:59.287550 [debug] [Thread-3 (]: On model.sales_pipelines.dim_customers: create or replace transient table INCREMENTALETL._marts.dim_customers
    
    
    
    as (

SELECT DISTINCT
    customer_id,
    country,
    payment_method,
    sales_channel,
    is_guest_customer,
    MIN(invoice_date) AS first_purchase_date,
    MAX(invoice_date) AS last_purchase_date,
    COUNT(DISTINCT invoice_no) AS total_orders
FROM INCREMENTALETL._processed.processed_sales
WHERE customer_id IS NOT NULL
GROUP BY
    customer_id,
    country,
    payment_method,
    sales_channel,
    is_guest_customer
    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.dim_customers"} */;
[0m11:44:59.287550 [debug] [Thread-2 (]: On model.sales_pipelines.dim_date: create or replace transient table INCREMENTALETL._marts.dim_date
    
    
    
    as (

SELECT DISTINCT
    invoice_date::DATE AS date_key,
    YEAR(invoice_date)  AS year,
    MONTH(invoice_date) AS month,
    DAY(invoice_date)   AS day,
    DAYNAME(invoice_date) AS day_name,
    WEEK(invoice_date)  AS week_number,
    QUARTER(invoice_date) AS quarter
FROM INCREMENTALETL._processed.processed_sales
    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.dim_date"} */;
[0m11:44:59.293277 [debug] [Thread-5 (]: On model.sales_pipelines.dim_products: create or replace transient table INCREMENTALETL._marts.dim_products
    
    
    
    as (

SELECT DISTINCT
    stock_code,
    description,
    category
FROM INCREMENTALETL._processed.processed_sales
    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.dim_products"} */;
[0m11:44:59.295165 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m11:44:59.298247 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m11:44:59.533139 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.247 seconds
[0m11:44:59.548991 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:44:59.548991 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: describe table INCREMENTALETL._marts.fact_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:44:59.729915 [debug] [Thread-4 (]: SQL status: SUCCESS 13 in 0.172 seconds
[0m11:44:59.747862 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:44:59.749879 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: describe table INCREMENTALETL._marts.fact_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:44:59.905762 [debug] [Thread-2 (]: SQL status: SUCCESS 1308 in 0.610 seconds
[0m11:44:59.913630 [debug] [Thread-4 (]: SQL status: SUCCESS 13 in 0.161 seconds
[0m11:44:59.921477 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd97619d0-9517-48b4-8af0-5b63f3ebd116', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FFD1556860>]}
[0m11:44:59.937645 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:44:59.953596 [info ] [Thread-2 (]: 4 of 6 OK created sql table model _marts.dim_date .............................. [[32mSUCCESS 1308[0m in 0.77s]
[0m11:44:59.958811 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: describe table "INCREMENTALETL"."_MARTS"."FACT_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:44:59.960829 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.dim_date
[0m11:45:00.133738 [debug] [Thread-4 (]: SQL status: SUCCESS 13 in 0.172 seconds
[0m11:45:00.145021 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.fact_sales"
[0m11:45:00.160742 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:45:00.160742 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */;
[0m11:45:00.432114 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.242 seconds
[0m11:45:00.436573 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:45:00.439172 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: merge into INCREMENTALETL._marts.fact_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._marts.fact_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","DATE_KEY" = DBT_INTERNAL_SOURCE."DATE_KEY","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","GROSS_AMOUNT" = DBT_INTERNAL_SOURCE."GROSS_AMOUNT","NET_AMOUNT" = DBT_INTERNAL_SOURCE."NET_AMOUNT","TOTAL_AMOUNT" = DBT_INTERNAL_SOURCE."TOTAL_AMOUNT","IS_RETURNED" = DBT_INTERNAL_SOURCE."IS_RETURNED","PROCESSED_AT" = DBT_INTERNAL_SOURCE."PROCESSED_AT"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "CUSTOMER_ID", "DATE_KEY", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "PROCESSED_AT")
    values
        ("INVOICE_NO", "STOCK_CODE", "CUSTOMER_ID", "DATE_KEY", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "PROCESSED_AT")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */;
[0m11:45:01.260284 [debug] [Thread-4 (]: SQL status: SUCCESS 3600 in 0.817 seconds
[0m11:45:01.261614 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:45:01.261614 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:45:01.276390 [debug] [Thread-5 (]: SQL status: SUCCESS 23492 in 1.988 seconds
[0m11:45:01.292583 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd97619d0-9517-48b4-8af0-5b63f3ebd116', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FFD2B24520>]}
[0m11:45:01.292583 [info ] [Thread-5 (]: 5 of 6 OK created sql table model _marts.dim_products .......................... [[32mSUCCESS 23492[0m in 2.13s]
[0m11:45:01.303887 [debug] [Thread-5 (]: Finished running node model.sales_pipelines.dim_products
[0m11:45:01.567979 [debug] [Thread-3 (]: SQL status: SUCCESS 28239 in 2.277 seconds
[0m11:45:01.567979 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.312 seconds
[0m11:45:01.579685 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd97619d0-9517-48b4-8af0-5b63f3ebd116', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FFD33F7AF0>]}
[0m11:45:01.606585 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._marts.fact_sales__dbt_tmp
[0m11:45:01.611620 [info ] [Thread-3 (]: 3 of 6 OK created sql table model _marts.dim_customers ......................... [[32mSUCCESS 28239[0m in 2.43s]
[0m11:45:01.615661 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:45:01.615661 [debug] [Thread-3 (]: Finished running node model.sales_pipelines.dim_customers
[0m11:45:01.615661 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: drop view if exists INCREMENTALETL._marts.fact_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:45:01.862668 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.242 seconds
[0m11:45:01.878395 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd97619d0-9517-48b4-8af0-5b63f3ebd116', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FFD32AEE30>]}
[0m11:45:01.878395 [info ] [Thread-4 (]: 6 of 6 OK created sql incremental model _marts.fact_sales ...................... [[32mSUCCESS 3600[0m in 2.72s]
[0m11:45:01.878395 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.fact_sales
[0m11:45:01.899415 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:45:01.899415 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m11:45:01.903844 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m11:45:02.281987 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m11:45:02.281987 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m11:45:02.558921 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m11:45:02.558921 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m11:45:02.997925 [debug] [MainThread]: Connection 'list_INCREMENTALETL__processed' was left open.
[0m11:45:02.997925 [debug] [MainThread]: On list_INCREMENTALETL__processed: Close
[0m11:45:03.257418 [debug] [MainThread]: Connection 'list_INCREMENTALETL__staging' was left open.
[0m11:45:03.257418 [debug] [MainThread]: On list_INCREMENTALETL__staging: Close
[0m11:45:03.614737 [debug] [MainThread]: Connection 'list_INCREMENTALETL__marts' was left open.
[0m11:45:03.614737 [debug] [MainThread]: On list_INCREMENTALETL__marts: Close
[0m11:45:03.937572 [debug] [MainThread]: Connection 'model.sales_pipelines.dim_date' was left open.
[0m11:45:03.939595 [debug] [MainThread]: On model.sales_pipelines.dim_date: Close
[0m11:45:04.178779 [debug] [MainThread]: Connection 'model.sales_pipelines.fact_sales' was left open.
[0m11:45:04.178779 [debug] [MainThread]: On model.sales_pipelines.fact_sales: Close
[0m11:45:04.535896 [debug] [MainThread]: Connection 'model.sales_pipelines.dim_customers' was left open.
[0m11:45:04.535896 [debug] [MainThread]: On model.sales_pipelines.dim_customers: Close
[0m11:45:04.941928 [debug] [MainThread]: Connection 'model.sales_pipelines.dim_products' was left open.
[0m11:45:04.941928 [debug] [MainThread]: On model.sales_pipelines.dim_products: Close
[0m11:45:06.169770 [info ] [MainThread]: 
[0m11:45:06.169770 [info ] [MainThread]: Finished running 3 incremental models, 3 table models in 0 hours 0 minutes and 18.30 seconds (18.30s).
[0m11:45:06.176668 [debug] [MainThread]: Command end result
[0m11:45:06.229960 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m11:45:06.231559 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m11:45:06.245279 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\snowflake-incremental-pipeline\dbt_project\target\run_results.json
[0m11:45:06.246482 [info ] [MainThread]: 
[0m11:45:06.247433 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:45:06.248481 [info ] [MainThread]: 
[0m11:45:06.248481 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=6
[0m11:45:06.250589 [debug] [MainThread]: Command `dbt run` succeeded at 11:45:06.250589 after 20.89 seconds
[0m11:45:06.250589 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FFBF117670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FFD2AB2E00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FFBF4DDBA0>]}
[0m11:45:06.250589 [debug] [MainThread]: Flushing usage events
[0m11:45:08.318241 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:45:27.580206 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023447607730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023449855C30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000234498579D0>]}


============================== 11:45:27.580206 | 4190b719-4abf-4b15-b13d-7676893931e0 ==============================
[0m11:45:27.580206 [info ] [MainThread]: Running with dbt=1.11.6
[0m11:45:27.595925 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'profiles_dir': 'D:\\snowflake-incremental-pipeline\\dbt_project', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'indirect_selection': 'eager', 'use_colors': 'True', 'invocation_command': 'dbt run', 'use_experimental_parser': 'False', 'log_path': 'D:\\snowflake-incremental-pipeline\\dbt_project\\logs'}
[0m11:45:28.548162 [debug] [MainThread]: Snowflake adapter: Setting snowflake.connector to ERROR (file logging only)
[0m11:45:28.548162 [debug] [MainThread]: Snowflake adapter: Setting botocore to ERROR (file logging only)
[0m11:45:28.548162 [debug] [MainThread]: Snowflake adapter: Setting boto3 to ERROR (file logging only)
[0m11:45:28.868818 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4190b719-4abf-4b15-b13d-7676893931e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023445B7EB60>]}
[0m11:45:28.947661 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4190b719-4abf-4b15-b13d-7676893931e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023449918BE0>]}
[0m11:45:28.963353 [info ] [MainThread]: Registered adapter: snowflake=1.11.2
[0m11:45:29.450629 [debug] [MainThread]: checksum: ec27062d18352e9cfca161c166fcc73d2201661a23a3cf3c89196a27f32cfcb8, vars: {}, profile: , target: , version: 1.11.6
[0m11:45:29.703583 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:45:29.703583 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m11:45:29.703583 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:45:29.783927 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4190b719-4abf-4b15-b13d-7676893931e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002345BF8CAC0>]}
[0m11:45:29.927949 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m11:45:29.927949 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m11:45:29.942673 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4190b719-4abf-4b15-b13d-7676893931e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002344747F460>]}
[0m11:45:29.942673 [info ] [MainThread]: Found 6 models, 6 data tests, 1 source, 522 macros
[0m11:45:29.942673 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4190b719-4abf-4b15-b13d-7676893931e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002345BF8C2E0>]}
[0m11:45:29.958414 [info ] [MainThread]: 
[0m11:45:29.958414 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m11:45:29.958414 [info ] [MainThread]: 
[0m11:45:29.958414 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m11:45:29.958414 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m11:45:29.958414 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m11:45:29.989925 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m11:45:30.086776 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m11:45:30.086776 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m11:45:30.086776 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m11:45:30.086776 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m11:45:30.086776 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m11:45:30.086776 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m11:45:30.086776 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:45:30.086776 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:45:30.086776 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:45:31.290604 [debug] [ThreadPool]: SQL status: SUCCESS 5 in 1.199 seconds
[0m11:45:31.290604 [debug] [ThreadPool]: SQL status: SUCCESS 5 in 1.205 seconds
[0m11:45:31.306270 [debug] [ThreadPool]: SQL status: SUCCESS 5 in 1.212 seconds
[0m11:45:31.314347 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__processed'
[0m11:45:31.314347 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__marts'
[0m11:45:31.330044 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__staging'
[0m11:45:31.349360 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__processed"
[0m11:45:31.351455 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__marts"
[0m11:45:31.351455 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__staging"
[0m11:45:31.351455 [debug] [ThreadPool]: On list_INCREMENTALETL__processed: show objects in INCREMENTALETL._processed
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__processed"} */;
[0m11:45:31.362005 [debug] [ThreadPool]: On list_INCREMENTALETL__marts: show objects in INCREMENTALETL._marts
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__marts"} */;
[0m11:45:31.362005 [debug] [ThreadPool]: On list_INCREMENTALETL__staging: show objects in INCREMENTALETL._staging
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__staging"} */;
[0m11:45:31.363815 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:45:31.364996 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:45:31.364996 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:45:32.211520 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.852 seconds
[0m11:45:32.282018 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.922 seconds
[0m11:45:32.323552 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 0.966 seconds
[0m11:45:32.339696 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4190b719-4abf-4b15-b13d-7676893931e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002345BFE4910>]}
[0m11:45:32.355857 [debug] [Thread-2 (]: Began running node model.sales_pipelines.stg_sales
[0m11:45:32.355857 [info ] [Thread-2 (]: 1 of 6 START sql incremental model _staging.stg_sales .......................... [RUN]
[0m11:45:32.355857 [debug] [Thread-2 (]: Acquiring new snowflake connection 'model.sales_pipelines.stg_sales'
[0m11:45:32.371824 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.stg_sales
[0m11:45:32.416377 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.stg_sales"
[0m11:45:32.416377 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.stg_sales
[0m11:45:32.498484 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:45:32.498484 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: create or replace  temporary view INCREMENTALETL._staging.stg_sales__dbt_tmp
  
  
  
  
  as (
    

WITH raw_data AS (
    SELECT * 
    FROM INCREMENTALETL.LANDINGZONE.RAW_SALES
),

staged AS (
    SELECT
        -- IDs
        CAST(INVOICENO AS VARCHAR)    AS invoice_no,
        CAST(STOCKCODE AS VARCHAR)    AS stock_code,
        CAST(DESCRIPTION AS VARCHAR)  AS description,

        -- Quantities & Prices
        TRY_CAST(QUANTITY AS INT)      AS quantity,
        TRY_CAST(UNITPRICE AS FLOAT)   AS unit_price,
        TRY_CAST(DISCOUNT AS FLOAT)    AS discount,
        TRY_CAST(SHIPPINGCOST AS FLOAT) AS shipping_cost,
        TRY_CAST(CUSTOMERID AS FLOAT)  AS customer_id,

        -- Convert once
        TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS') AS invoice_date,

        TRIM(UPPER(COUNTRY))            AS country,
        TRIM(UPPER(PAYMENTMETHOD))      AS payment_method,
        TRIM(UPPER(CATEGORY))           AS category,
        TRIM(UPPER(SALESCHANNEL))       AS sales_channel,
        TRIM(UPPER(RETURNSTATUS))       AS return_status,
        TRIM(UPPER(SHIPMENTPROVIDER))   AS shipment_provider,
        TRIM(UPPER(WAREHOUSELOCATION))  AS warehouse_location,
        TRIM(UPPER(ORDERPRIORITY))      AS order_priority,

        CURRENT_TIMESTAMP()             AS stg_loaded_at

    FROM raw_data
),

validated AS (
    SELECT
        *,
        YEAR(invoice_date)  AS invoice_year,
        MONTH(invoice_date) AS invoice_month,

        CASE
            WHEN invoice_date IS NULL           THEN 'INVALID_DATE'
            WHEN quantity IS NULL               THEN 'INVALID_QUANTITY'
            WHEN unit_price IS NULL             THEN 'INVALID_PRICE'
            WHEN shipping_cost < 0              THEN 'NEGATIVE_SHIPPING'
            WHEN discount < 0 OR discount > 1   THEN 'INVALID_DISCOUNT'
            WHEN invoice_no IS NULL             THEN 'NULL_INVOICE'
            ELSE 'VALID'
        END AS data_quality_flag
    FROM staged
)

SELECT *
FROM validated


WHERE NOT EXISTS (
    SELECT 1
    FROM INCREMENTALETL._staging.stg_sales t
    WHERE t.invoice_year  = validated.invoice_year
      AND t.invoice_month = validated.invoice_month
)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m11:45:32.498484 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m11:45:33.817615 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 1.309 seconds
[0m11:45:33.928691 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:45:33.928691 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:45:34.082708 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.144 seconds
[0m11:45:34.104935 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:45:34.104935 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:45:34.233885 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.132 seconds
[0m11:45:34.282984 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:45:34.282984 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:45:34.442365 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.153 seconds
[0m11:45:34.458416 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:45:34.458416 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table "INCREMENTALETL"."_STAGING"."STG_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:45:34.599998 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.131 seconds
[0m11:45:34.629986 [debug] [Thread-2 (]: 
    In "INCREMENTALETL"."_STAGING"."STG_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m11:45:34.677414 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.stg_sales"
[0m11:45:34.677414 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:45:34.677414 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m11:45:34.823541 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.150 seconds
[0m11:45:34.837509 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:45:34.837509 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: merge into INCREMENTALETL._staging.stg_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._staging.stg_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","DATA_QUALITY_FLAG" = DBT_INTERNAL_SOURCE."DATA_QUALITY_FLAG"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "INVOICE_YEAR", "INVOICE_MONTH", "DATA_QUALITY_FLAG")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "INVOICE_YEAR", "INVOICE_MONTH", "DATA_QUALITY_FLAG")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m11:45:35.962843 [debug] [Thread-2 (]: SQL status: SUCCESS 744 in 1.120 seconds
[0m11:45:35.964852 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:45:35.964852 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:45:36.197388 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.231 seconds
[0m11:45:36.206912 [debug] [Thread-2 (]: Applying DROP to: INCREMENTALETL._staging.stg_sales__dbt_tmp
[0m11:45:36.220166 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:45:36.220166 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: drop view if exists INCREMENTALETL._staging.stg_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:45:36.397151 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.174 seconds
[0m11:45:36.430651 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4190b719-4abf-4b15-b13d-7676893931e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002345C809420>]}
[0m11:45:36.430651 [info ] [Thread-2 (]: 1 of 6 OK created sql incremental model _staging.stg_sales ..................... [[32mSUCCESS 744[0m in 4.07s]
[0m11:45:36.430651 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.stg_sales
[0m11:45:36.438273 [debug] [Thread-4 (]: Began running node model.sales_pipelines.processed_sales
[0m11:45:36.439525 [info ] [Thread-4 (]: 2 of 6 START sql incremental model _processed.processed_sales .................. [RUN]
[0m11:45:36.439525 [debug] [Thread-4 (]: Acquiring new snowflake connection 'model.sales_pipelines.processed_sales'
[0m11:45:36.439525 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.processed_sales
[0m11:45:36.450250 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.processed_sales"
[0m11:45:36.451326 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.processed_sales
[0m11:45:36.453426 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:45:36.462747 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: create or replace  temporary view INCREMENTALETL._processed.processed_sales__dbt_tmp
  
  
  
  
  as (
    

WITH staging AS (
    SELECT * 
    FROM INCREMENTALETL._staging.stg_sales
),

deduped AS (
    SELECT *,
        ROW_NUMBER() OVER (
            PARTITION BY invoice_no, stock_code
            ORDER BY stg_loaded_at DESC
        ) AS row_num
    FROM staging
),

cleaned AS (
    SELECT
        invoice_no,
        stock_code,
        description,
        quantity,
        unit_price,
        discount,
        shipping_cost,
        customer_id,
        invoice_date,
        invoice_year,
        invoice_month,
        country,
        payment_method,
        category,
        sales_channel,
        return_status,
        shipment_provider,
        warehouse_location,
        order_priority,

        -- Derived metrics
        ROUND(quantity * unit_price, 2) AS gross_amount,
        ROUND(quantity * unit_price * (1 - discount), 2) AS net_amount,
        ROUND(
            quantity * unit_price * (1 - discount)
            + COALESCE(shipping_cost, 0),
        2) AS total_amount,

        CASE WHEN return_status = 'RETURNED' THEN TRUE ELSE FALSE END AS is_returned,
        CASE WHEN customer_id IS NULL THEN TRUE ELSE FALSE END AS is_guest_customer,

        stg_loaded_at,
        CURRENT_TIMESTAMP() AS processed_at

    FROM deduped
    WHERE
        row_num = 1
        AND data_quality_flag = 'VALID'
        AND invoice_date IS NOT NULL
)

SELECT *
FROM cleaned


WHERE NOT EXISTS (
    SELECT 1
    FROM INCREMENTALETL._processed.processed_sales t
    WHERE t.invoice_year  = cleaned.invoice_year
      AND t.invoice_month = cleaned.invoice_month
)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m11:45:36.464022 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m11:45:37.314059 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.850 seconds
[0m11:45:37.323927 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:45:37.325944 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:45:37.485662 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.174 seconds
[0m11:45:37.503605 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:45:37.503605 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:45:37.654634 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.139 seconds
[0m11:45:37.680860 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:45:37.682088 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:45:37.817126 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.137 seconds
[0m11:45:37.842039 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:45:37.842039 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:45:37.990154 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.146 seconds
[0m11:45:38.002229 [debug] [Thread-4 (]: 
    In "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m11:45:38.002229 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.processed_sales"
[0m11:45:38.018078 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:45:38.018078 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m11:45:38.177903 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.154 seconds
[0m11:45:38.177903 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:45:38.177903 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: merge into INCREMENTALETL._processed.processed_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._processed.processed_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","GROSS_AMOUNT" = DBT_INTERNAL_SOURCE."GROSS_AMOUNT","NET_AMOUNT" = DBT_INTERNAL_SOURCE."NET_AMOUNT","TOTAL_AMOUNT" = DBT_INTERNAL_SOURCE."TOTAL_AMOUNT","IS_RETURNED" = DBT_INTERNAL_SOURCE."IS_RETURNED","IS_GUEST_CUSTOMER" = DBT_INTERNAL_SOURCE."IS_GUEST_CUSTOMER","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","PROCESSED_AT" = DBT_INTERNAL_SOURCE."PROCESSED_AT"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m11:45:39.170700 [debug] [Thread-4 (]: SQL status: SUCCESS 729 in 0.987 seconds
[0m11:45:39.172725 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:45:39.176773 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:45:39.461877 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.294 seconds
[0m11:45:39.487109 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._processed.processed_sales__dbt_tmp
[0m11:45:39.494174 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:45:39.494174 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: drop view if exists INCREMENTALETL._processed.processed_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:45:39.767554 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.276 seconds
[0m11:45:39.783761 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4190b719-4abf-4b15-b13d-7676893931e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002344745BF40>]}
[0m11:45:39.783761 [info ] [Thread-4 (]: 2 of 6 OK created sql incremental model _processed.processed_sales ............. [[32mSUCCESS 729[0m in 3.34s]
[0m11:45:39.797962 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.processed_sales
[0m11:45:39.801806 [debug] [Thread-3 (]: Began running node model.sales_pipelines.dim_customers
[0m11:45:39.801806 [debug] [Thread-2 (]: Began running node model.sales_pipelines.dim_date
[0m11:45:39.801806 [debug] [Thread-5 (]: Began running node model.sales_pipelines.dim_products
[0m11:45:39.801806 [info ] [Thread-3 (]: 3 of 6 START sql table model _marts.dim_customers .............................. [RUN]
[0m11:45:39.801806 [debug] [Thread-4 (]: Began running node model.sales_pipelines.fact_sales
[0m11:45:39.817952 [info ] [Thread-2 (]: 4 of 6 START sql table model _marts.dim_date ................................... [RUN]
[0m11:45:39.823709 [info ] [Thread-5 (]: 5 of 6 START sql table model _marts.dim_products ............................... [RUN]
[0m11:45:39.827143 [debug] [Thread-3 (]: Acquiring new snowflake connection 'model.sales_pipelines.dim_customers'
[0m11:45:39.827143 [info ] [Thread-4 (]: 6 of 6 START sql incremental model _marts.fact_sales ........................... [RUN]
[0m11:45:39.832173 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.sales_pipelines.stg_sales, now model.sales_pipelines.dim_date)
[0m11:45:39.834696 [debug] [Thread-5 (]: Acquiring new snowflake connection 'model.sales_pipelines.dim_products'
[0m11:45:39.837218 [debug] [Thread-3 (]: Began compiling node model.sales_pipelines.dim_customers
[0m11:45:39.837218 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.sales_pipelines.processed_sales, now model.sales_pipelines.fact_sales)
[0m11:45:39.839731 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.dim_date
[0m11:45:39.839731 [debug] [Thread-5 (]: Began compiling node model.sales_pipelines.dim_products
[0m11:45:39.847950 [debug] [Thread-3 (]: Writing injected SQL for node "model.sales_pipelines.dim_customers"
[0m11:45:39.847950 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.fact_sales
[0m11:45:39.859749 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.dim_date"
[0m11:45:39.868444 [debug] [Thread-5 (]: Writing injected SQL for node "model.sales_pipelines.dim_products"
[0m11:45:39.873514 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.fact_sales"
[0m11:45:39.873514 [debug] [Thread-3 (]: Began executing node model.sales_pipelines.dim_customers
[0m11:45:39.881515 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.dim_date
[0m11:45:39.898268 [debug] [Thread-5 (]: Began executing node model.sales_pipelines.dim_products
[0m11:45:39.914913 [debug] [Thread-3 (]: Writing runtime sql for node "model.sales_pipelines.dim_customers"
[0m11:45:39.928506 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.fact_sales
[0m11:45:39.931742 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.dim_date"
[0m11:45:39.936444 [debug] [Thread-5 (]: Writing runtime sql for node "model.sales_pipelines.dim_products"
[0m11:45:39.944534 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:45:39.947376 [debug] [Thread-3 (]: Using snowflake connection "model.sales_pipelines.dim_customers"
[0m11:45:39.950068 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.dim_date"
[0m11:45:39.950068 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: create or replace  temporary view INCREMENTALETL._marts.fact_sales__dbt_tmp
  
  
  
  
  as (
    

SELECT
    invoice_no,
    stock_code,
    customer_id,
    invoice_date::DATE AS date_key,

    quantity,
    unit_price,
    discount,
    shipping_cost,

    gross_amount,
    net_amount,
    total_amount,

    is_returned,
    processed_at

FROM INCREMENTALETL._processed.processed_sales


WHERE processed_at > (SELECT MAX(processed_at) FROM INCREMENTALETL._marts.fact_sales)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */;
[0m11:45:39.951636 [debug] [Thread-3 (]: On model.sales_pipelines.dim_customers: create or replace transient table INCREMENTALETL._marts.dim_customers
    
    
    
    as (

SELECT DISTINCT
    customer_id,
    country,
    payment_method,
    sales_channel,
    is_guest_customer,
    MIN(invoice_date) AS first_purchase_date,
    MAX(invoice_date) AS last_purchase_date,
    COUNT(DISTINCT invoice_no) AS total_orders
FROM INCREMENTALETL._processed.processed_sales
WHERE customer_id IS NOT NULL
GROUP BY
    customer_id,
    country,
    payment_method,
    sales_channel,
    is_guest_customer
    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.dim_customers"} */;
[0m11:45:39.951636 [debug] [Thread-5 (]: Using snowflake connection "model.sales_pipelines.dim_products"
[0m11:45:39.951636 [debug] [Thread-2 (]: On model.sales_pipelines.dim_date: create or replace transient table INCREMENTALETL._marts.dim_date
    
    
    
    as (

SELECT DISTINCT
    invoice_date::DATE AS date_key,
    YEAR(invoice_date)  AS year,
    MONTH(invoice_date) AS month,
    DAY(invoice_date)   AS day,
    DAYNAME(invoice_date) AS day_name,
    WEEK(invoice_date)  AS week_number,
    QUARTER(invoice_date) AS quarter
FROM INCREMENTALETL._processed.processed_sales
    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.dim_date"} */;
[0m11:45:39.951636 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m11:45:39.951636 [debug] [Thread-5 (]: On model.sales_pipelines.dim_products: create or replace transient table INCREMENTALETL._marts.dim_products
    
    
    
    as (

SELECT DISTINCT
    stock_code,
    description,
    category
FROM INCREMENTALETL._processed.processed_sales
    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.dim_products"} */;
[0m11:45:39.964082 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m11:45:40.212096 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.261 seconds
[0m11:45:40.224295 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:45:40.224295 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: describe table INCREMENTALETL._marts.fact_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:45:40.391605 [debug] [Thread-4 (]: SQL status: SUCCESS 13 in 0.168 seconds
[0m11:45:40.417773 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:45:40.434584 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: describe table INCREMENTALETL._marts.fact_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:45:40.610108 [debug] [Thread-4 (]: SQL status: SUCCESS 13 in 0.165 seconds
[0m11:45:40.619203 [debug] [Thread-2 (]: SQL status: SUCCESS 1339 in 0.658 seconds
[0m11:45:40.655939 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:45:40.687005 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4190b719-4abf-4b15-b13d-7676893931e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002345C777AF0>]}
[0m11:45:40.687005 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: describe table "INCREMENTALETL"."_MARTS"."FACT_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:45:40.695537 [info ] [Thread-2 (]: 4 of 6 OK created sql table model _marts.dim_date .............................. [[32mSUCCESS 1339[0m in 0.86s]
[0m11:45:40.706791 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.dim_date
[0m11:45:40.965675 [debug] [Thread-4 (]: SQL status: SUCCESS 13 in 0.263 seconds
[0m11:45:40.976950 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.fact_sales"
[0m11:45:40.983577 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:45:40.985592 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */;
[0m11:45:41.122213 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.146 seconds
[0m11:45:41.133267 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:45:41.133267 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: merge into INCREMENTALETL._marts.fact_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._marts.fact_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","DATE_KEY" = DBT_INTERNAL_SOURCE."DATE_KEY","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","GROSS_AMOUNT" = DBT_INTERNAL_SOURCE."GROSS_AMOUNT","NET_AMOUNT" = DBT_INTERNAL_SOURCE."NET_AMOUNT","TOTAL_AMOUNT" = DBT_INTERNAL_SOURCE."TOTAL_AMOUNT","IS_RETURNED" = DBT_INTERNAL_SOURCE."IS_RETURNED","PROCESSED_AT" = DBT_INTERNAL_SOURCE."PROCESSED_AT"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "CUSTOMER_ID", "DATE_KEY", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "PROCESSED_AT")
    values
        ("INVOICE_NO", "STOCK_CODE", "CUSTOMER_ID", "DATE_KEY", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "PROCESSED_AT")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */;
[0m11:45:41.422050 [debug] [Thread-5 (]: SQL status: SUCCESS 23911 in 1.460 seconds
[0m11:45:41.432896 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4190b719-4abf-4b15-b13d-7676893931e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002345C775570>]}
[0m11:45:41.443036 [info ] [Thread-5 (]: 5 of 6 OK created sql table model _marts.dim_products .......................... [[32mSUCCESS 23911[0m in 1.60s]
[0m11:45:41.443036 [debug] [Thread-5 (]: Finished running node model.sales_pipelines.dim_products
[0m11:45:41.495939 [debug] [Thread-3 (]: SQL status: SUCCESS 28903 in 1.539 seconds
[0m11:45:41.503546 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4190b719-4abf-4b15-b13d-7676893931e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002345C808C10>]}
[0m11:45:41.506072 [info ] [Thread-3 (]: 3 of 6 OK created sql table model _marts.dim_customers ......................... [[32mSUCCESS 28903[0m in 1.68s]
[0m11:45:41.506072 [debug] [Thread-3 (]: Finished running node model.sales_pipelines.dim_customers
[0m11:45:41.901554 [debug] [Thread-4 (]: SQL status: SUCCESS 729 in 0.765 seconds
[0m11:45:41.901554 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:45:41.910998 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:45:42.151245 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.242 seconds
[0m11:45:42.168615 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._marts.fact_sales__dbt_tmp
[0m11:45:42.168615 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:45:42.168615 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: drop view if exists INCREMENTALETL._marts.fact_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:45:42.332713 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.165 seconds
[0m11:45:42.354106 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4190b719-4abf-4b15-b13d-7676893931e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002345C943C10>]}
[0m11:45:42.361921 [info ] [Thread-4 (]: 6 of 6 OK created sql incremental model _marts.fact_sales ...................... [[32mSUCCESS 729[0m in 2.52s]
[0m11:45:42.364998 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.fact_sales
[0m11:45:42.368286 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:45:42.370136 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m11:45:42.370136 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m11:45:42.604773 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m11:45:42.604773 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m11:45:42.833289 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m11:45:42.833289 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m11:45:43.694440 [debug] [MainThread]: Connection 'list_INCREMENTALETL__processed' was left open.
[0m11:45:43.694440 [debug] [MainThread]: On list_INCREMENTALETL__processed: Close
[0m11:45:43.959611 [debug] [MainThread]: Connection 'list_INCREMENTALETL__marts' was left open.
[0m11:45:43.959611 [debug] [MainThread]: On list_INCREMENTALETL__marts: Close
[0m11:45:44.465369 [debug] [MainThread]: Connection 'list_INCREMENTALETL__staging' was left open.
[0m11:45:44.465369 [debug] [MainThread]: On list_INCREMENTALETL__staging: Close
[0m11:45:44.728714 [debug] [MainThread]: Connection 'model.sales_pipelines.dim_date' was left open.
[0m11:45:44.728714 [debug] [MainThread]: On model.sales_pipelines.dim_date: Close
[0m11:45:44.999416 [debug] [MainThread]: Connection 'model.sales_pipelines.fact_sales' was left open.
[0m11:45:44.999416 [debug] [MainThread]: On model.sales_pipelines.fact_sales: Close
[0m11:45:45.398114 [debug] [MainThread]: Connection 'model.sales_pipelines.dim_customers' was left open.
[0m11:45:45.402159 [debug] [MainThread]: On model.sales_pipelines.dim_customers: Close
[0m11:45:45.636328 [debug] [MainThread]: Connection 'model.sales_pipelines.dim_products' was left open.
[0m11:45:45.636328 [debug] [MainThread]: On model.sales_pipelines.dim_products: Close
[0m11:45:46.613122 [info ] [MainThread]: 
[0m11:45:46.625284 [info ] [MainThread]: Finished running 3 incremental models, 3 table models in 0 hours 0 minutes and 16.65 seconds (16.65s).
[0m11:45:46.635107 [debug] [MainThread]: Command end result
[0m11:45:46.696190 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m11:45:46.698022 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m11:45:46.714223 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\snowflake-incremental-pipeline\dbt_project\target\run_results.json
[0m11:45:46.715702 [info ] [MainThread]: 
[0m11:45:46.716841 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:45:46.718485 [info ] [MainThread]: 
[0m11:45:46.718485 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=6
[0m11:45:46.720938 [debug] [MainThread]: Command `dbt run` succeeded at 11:45:46.720938 after 19.26 seconds
[0m11:45:46.720938 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023447607730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002344895EF80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023446FFAA40>]}
[0m11:45:46.722947 [debug] [MainThread]: Flushing usage events
[0m11:45:48.665876 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:45:54.489181 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017BAFDF76D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017BB10A2620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017BB10A30D0>]}


============================== 11:45:54.495979 | 6a236df9-f784-4a68-ab66-3e206b3ab750 ==============================
[0m11:45:54.495979 [info ] [MainThread]: Running with dbt=1.11.6
[0m11:45:54.495979 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'D:\\snowflake-incremental-pipeline\\dbt_project', 'invocation_command': 'dbt run', 'use_colors': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'D:\\snowflake-incremental-pipeline\\dbt_project\\logs'}
[0m11:45:55.550785 [debug] [MainThread]: Snowflake adapter: Setting snowflake.connector to ERROR (file logging only)
[0m11:45:55.558352 [debug] [MainThread]: Snowflake adapter: Setting botocore to ERROR (file logging only)
[0m11:45:55.558352 [debug] [MainThread]: Snowflake adapter: Setting boto3 to ERROR (file logging only)
[0m11:45:55.908977 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6a236df9-f784-4a68-ab66-3e206b3ab750', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017BADB9EB00>]}
[0m11:45:55.988131 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6a236df9-f784-4a68-ab66-3e206b3ab750', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017BB08671C0>]}
[0m11:45:55.988131 [info ] [MainThread]: Registered adapter: snowflake=1.11.2
[0m11:45:56.514210 [debug] [MainThread]: checksum: ec27062d18352e9cfca161c166fcc73d2201661a23a3cf3c89196a27f32cfcb8, vars: {}, profile: , target: , version: 1.11.6
[0m11:45:56.798364 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:45:56.798364 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m11:45:56.798364 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:45:56.900095 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6a236df9-f784-4a68-ab66-3e206b3ab750', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017BC37A8A60>]}
[0m11:45:57.064975 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m11:45:57.064975 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m11:45:57.098963 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6a236df9-f784-4a68-ab66-3e206b3ab750', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017BC3721600>]}
[0m11:45:57.098963 [info ] [MainThread]: Found 6 models, 6 data tests, 1 source, 522 macros
[0m11:45:57.101358 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6a236df9-f784-4a68-ab66-3e206b3ab750', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017BC37A89A0>]}
[0m11:45:57.103368 [info ] [MainThread]: 
[0m11:45:57.103368 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m11:45:57.103368 [info ] [MainThread]: 
[0m11:45:57.107874 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m11:45:57.117269 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m11:45:57.138822 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m11:45:57.147707 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m11:45:57.235387 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m11:45:57.235387 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m11:45:57.235387 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m11:45:57.243406 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m11:45:57.243406 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m11:45:57.244687 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m11:45:57.244687 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:45:57.244687 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:45:57.247856 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:45:58.560651 [debug] [ThreadPool]: SQL status: SUCCESS 5 in 1.318 seconds
[0m11:45:58.560651 [debug] [ThreadPool]: SQL status: SUCCESS 5 in 1.321 seconds
[0m11:45:58.576540 [debug] [ThreadPool]: SQL status: SUCCESS 5 in 1.330 seconds
[0m11:45:58.592652 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__marts'
[0m11:45:58.600809 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__staging'
[0m11:45:58.640586 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__marts"
[0m11:45:58.656033 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__processed'
[0m11:45:58.657298 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__staging"
[0m11:45:58.657298 [debug] [ThreadPool]: On list_INCREMENTALETL__marts: show objects in INCREMENTALETL._marts
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__marts"} */;
[0m11:45:58.671987 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__processed"
[0m11:45:58.671987 [debug] [ThreadPool]: On list_INCREMENTALETL__staging: show objects in INCREMENTALETL._staging
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__staging"} */;
[0m11:45:58.671987 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:45:58.671987 [debug] [ThreadPool]: On list_INCREMENTALETL__processed: show objects in INCREMENTALETL._processed
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__processed"} */;
[0m11:45:58.671987 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:45:58.671987 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:45:59.606943 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.926 seconds
[0m11:45:59.662680 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.986 seconds
[0m11:45:59.777680 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 1.108 seconds
[0m11:45:59.793684 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6a236df9-f784-4a68-ab66-3e206b3ab750', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017BC2267C10>]}
[0m11:45:59.807271 [debug] [Thread-2 (]: Began running node model.sales_pipelines.stg_sales
[0m11:45:59.807271 [info ] [Thread-2 (]: 1 of 6 START sql incremental model _staging.stg_sales .......................... [RUN]
[0m11:45:59.824159 [debug] [Thread-2 (]: Acquiring new snowflake connection 'model.sales_pipelines.stg_sales'
[0m11:45:59.824823 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.stg_sales
[0m11:45:59.865237 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.stg_sales"
[0m11:45:59.865237 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.stg_sales
[0m11:45:59.953077 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:45:59.953077 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: create or replace  temporary view INCREMENTALETL._staging.stg_sales__dbt_tmp
  
  
  
  
  as (
    

WITH raw_data AS (
    SELECT * 
    FROM INCREMENTALETL.LANDINGZONE.RAW_SALES
),

staged AS (
    SELECT
        -- IDs
        CAST(INVOICENO AS VARCHAR)    AS invoice_no,
        CAST(STOCKCODE AS VARCHAR)    AS stock_code,
        CAST(DESCRIPTION AS VARCHAR)  AS description,

        -- Quantities & Prices
        TRY_CAST(QUANTITY AS INT)      AS quantity,
        TRY_CAST(UNITPRICE AS FLOAT)   AS unit_price,
        TRY_CAST(DISCOUNT AS FLOAT)    AS discount,
        TRY_CAST(SHIPPINGCOST AS FLOAT) AS shipping_cost,
        TRY_CAST(CUSTOMERID AS FLOAT)  AS customer_id,

        -- Convert once
        TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS') AS invoice_date,

        TRIM(UPPER(COUNTRY))            AS country,
        TRIM(UPPER(PAYMENTMETHOD))      AS payment_method,
        TRIM(UPPER(CATEGORY))           AS category,
        TRIM(UPPER(SALESCHANNEL))       AS sales_channel,
        TRIM(UPPER(RETURNSTATUS))       AS return_status,
        TRIM(UPPER(SHIPMENTPROVIDER))   AS shipment_provider,
        TRIM(UPPER(WAREHOUSELOCATION))  AS warehouse_location,
        TRIM(UPPER(ORDERPRIORITY))      AS order_priority,

        CURRENT_TIMESTAMP()             AS stg_loaded_at

    FROM raw_data
),

validated AS (
    SELECT
        *,
        YEAR(invoice_date)  AS invoice_year,
        MONTH(invoice_date) AS invoice_month,

        CASE
            WHEN invoice_date IS NULL           THEN 'INVALID_DATE'
            WHEN quantity IS NULL               THEN 'INVALID_QUANTITY'
            WHEN unit_price IS NULL             THEN 'INVALID_PRICE'
            WHEN shipping_cost < 0              THEN 'NEGATIVE_SHIPPING'
            WHEN discount < 0 OR discount > 1   THEN 'INVALID_DISCOUNT'
            WHEN invoice_no IS NULL             THEN 'NULL_INVOICE'
            ELSE 'VALID'
        END AS data_quality_flag
    FROM staged
)

SELECT *
FROM validated


WHERE NOT EXISTS (
    SELECT 1
    FROM INCREMENTALETL._staging.stg_sales t
    WHERE t.invoice_year  = validated.invoice_year
      AND t.invoice_month = validated.invoice_month
)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m11:45:59.953077 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m11:46:01.880768 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 1.922 seconds
[0m11:46:01.926823 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:46:01.930618 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:46:02.099517 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.169 seconds
[0m11:46:02.115276 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:46:02.115276 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:46:02.308548 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.179 seconds
[0m11:46:02.356191 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:46:02.356191 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:46:02.541879 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.192 seconds
[0m11:46:02.573930 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:46:02.573930 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table "INCREMENTALETL"."_STAGING"."STG_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:46:02.787217 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.215 seconds
[0m11:46:02.819162 [debug] [Thread-2 (]: 
    In "INCREMENTALETL"."_STAGING"."STG_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m11:46:02.872546 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.stg_sales"
[0m11:46:02.876559 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:46:02.876559 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m11:46:03.102090 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.228 seconds
[0m11:46:03.102090 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:46:03.102090 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: merge into INCREMENTALETL._staging.stg_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._staging.stg_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","DATA_QUALITY_FLAG" = DBT_INTERNAL_SOURCE."DATA_QUALITY_FLAG"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "INVOICE_YEAR", "INVOICE_MONTH", "DATA_QUALITY_FLAG")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "INVOICE_YEAR", "INVOICE_MONTH", "DATA_QUALITY_FLAG")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m11:46:04.227222 [debug] [Thread-2 (]: SQL status: SUCCESS 720 in 1.118 seconds
[0m11:46:04.227222 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:46:04.227222 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:46:04.568945 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.331 seconds
[0m11:46:04.580077 [debug] [Thread-2 (]: Applying DROP to: INCREMENTALETL._staging.stg_sales__dbt_tmp
[0m11:46:04.580077 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:46:04.597148 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: drop view if exists INCREMENTALETL._staging.stg_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:46:04.793019 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.199 seconds
[0m11:46:04.856447 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6a236df9-f784-4a68-ab66-3e206b3ab750', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017BAE7C1780>]}
[0m11:46:04.856447 [info ] [Thread-2 (]: 1 of 6 OK created sql incremental model _staging.stg_sales ..................... [[32mSUCCESS 720[0m in 5.05s]
[0m11:46:04.856447 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.stg_sales
[0m11:46:04.868165 [debug] [Thread-4 (]: Began running node model.sales_pipelines.processed_sales
[0m11:46:04.869372 [info ] [Thread-4 (]: 2 of 6 START sql incremental model _processed.processed_sales .................. [RUN]
[0m11:46:04.869372 [debug] [Thread-4 (]: Acquiring new snowflake connection 'model.sales_pipelines.processed_sales'
[0m11:46:04.872029 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.processed_sales
[0m11:46:04.879378 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.processed_sales"
[0m11:46:04.882937 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.processed_sales
[0m11:46:04.891484 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:46:04.891484 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: create or replace  temporary view INCREMENTALETL._processed.processed_sales__dbt_tmp
  
  
  
  
  as (
    

WITH staging AS (
    SELECT * 
    FROM INCREMENTALETL._staging.stg_sales
),

deduped AS (
    SELECT *,
        ROW_NUMBER() OVER (
            PARTITION BY invoice_no, stock_code
            ORDER BY stg_loaded_at DESC
        ) AS row_num
    FROM staging
),

cleaned AS (
    SELECT
        invoice_no,
        stock_code,
        description,
        quantity,
        unit_price,
        discount,
        shipping_cost,
        customer_id,
        invoice_date,
        invoice_year,
        invoice_month,
        country,
        payment_method,
        category,
        sales_channel,
        return_status,
        shipment_provider,
        warehouse_location,
        order_priority,

        -- Derived metrics
        ROUND(quantity * unit_price, 2) AS gross_amount,
        ROUND(quantity * unit_price * (1 - discount), 2) AS net_amount,
        ROUND(
            quantity * unit_price * (1 - discount)
            + COALESCE(shipping_cost, 0),
        2) AS total_amount,

        CASE WHEN return_status = 'RETURNED' THEN TRUE ELSE FALSE END AS is_returned,
        CASE WHEN customer_id IS NULL THEN TRUE ELSE FALSE END AS is_guest_customer,

        stg_loaded_at,
        CURRENT_TIMESTAMP() AS processed_at

    FROM deduped
    WHERE
        row_num = 1
        AND data_quality_flag = 'VALID'
        AND invoice_date IS NOT NULL
)

SELECT *
FROM cleaned


WHERE NOT EXISTS (
    SELECT 1
    FROM INCREMENTALETL._processed.processed_sales t
    WHERE t.invoice_year  = cleaned.invoice_year
      AND t.invoice_month = cleaned.invoice_month
)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m11:46:04.896998 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m11:46:05.917761 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 1.028 seconds
[0m11:46:05.944618 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:46:05.949346 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:46:06.100978 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.153 seconds
[0m11:46:06.116931 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:46:06.128297 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:46:06.323129 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.203 seconds
[0m11:46:06.354847 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:46:06.354847 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:46:06.550090 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.196 seconds
[0m11:46:06.582124 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:46:06.582124 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:46:06.739459 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.152 seconds
[0m11:46:06.761348 [debug] [Thread-4 (]: 
    In "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m11:46:06.775456 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.processed_sales"
[0m11:46:06.781185 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:46:06.783555 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m11:46:06.936559 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.156 seconds
[0m11:46:06.936559 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:46:06.936559 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: merge into INCREMENTALETL._processed.processed_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._processed.processed_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","GROSS_AMOUNT" = DBT_INTERNAL_SOURCE."GROSS_AMOUNT","NET_AMOUNT" = DBT_INTERNAL_SOURCE."NET_AMOUNT","TOTAL_AMOUNT" = DBT_INTERNAL_SOURCE."TOTAL_AMOUNT","IS_RETURNED" = DBT_INTERNAL_SOURCE."IS_RETURNED","IS_GUEST_CUSTOMER" = DBT_INTERNAL_SOURCE."IS_GUEST_CUSTOMER","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","PROCESSED_AT" = DBT_INTERNAL_SOURCE."PROCESSED_AT"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m11:46:07.899934 [debug] [Thread-4 (]: SQL status: SUCCESS 712 in 0.953 seconds
[0m11:46:07.899934 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:46:07.899934 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:46:08.224379 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.317 seconds
[0m11:46:08.224379 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._processed.processed_sales__dbt_tmp
[0m11:46:08.238389 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:46:08.238389 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: drop view if exists INCREMENTALETL._processed.processed_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:46:08.638756 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.398 seconds
[0m11:46:08.641048 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6a236df9-f784-4a68-ab66-3e206b3ab750', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017BC3BB1FF0>]}
[0m11:46:08.654719 [info ] [Thread-4 (]: 2 of 6 OK created sql incremental model _processed.processed_sales ............. [[32mSUCCESS 712[0m in 3.77s]
[0m11:46:08.654719 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.processed_sales
[0m11:46:08.654719 [debug] [Thread-3 (]: Began running node model.sales_pipelines.dim_customers
[0m11:46:08.654719 [debug] [Thread-2 (]: Began running node model.sales_pipelines.dim_date
[0m11:46:08.666018 [debug] [Thread-5 (]: Began running node model.sales_pipelines.dim_products
[0m11:46:08.666018 [debug] [Thread-4 (]: Began running node model.sales_pipelines.fact_sales
[0m11:46:08.666018 [info ] [Thread-3 (]: 3 of 6 START sql table model _marts.dim_customers .............................. [RUN]
[0m11:46:08.666018 [info ] [Thread-2 (]: 4 of 6 START sql table model _marts.dim_date ................................... [RUN]
[0m11:46:08.670690 [info ] [Thread-5 (]: 5 of 6 START sql table model _marts.dim_products ............................... [RUN]
[0m11:46:08.673158 [info ] [Thread-4 (]: 6 of 6 START sql incremental model _marts.fact_sales ........................... [RUN]
[0m11:46:08.675644 [debug] [Thread-3 (]: Acquiring new snowflake connection 'model.sales_pipelines.dim_customers'
[0m11:46:08.675644 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.sales_pipelines.stg_sales, now model.sales_pipelines.dim_date)
[0m11:46:08.679013 [debug] [Thread-5 (]: Acquiring new snowflake connection 'model.sales_pipelines.dim_products'
[0m11:46:08.681502 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.sales_pipelines.processed_sales, now model.sales_pipelines.fact_sales)
[0m11:46:08.681502 [debug] [Thread-3 (]: Began compiling node model.sales_pipelines.dim_customers
[0m11:46:08.684184 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.dim_date
[0m11:46:08.684184 [debug] [Thread-5 (]: Began compiling node model.sales_pipelines.dim_products
[0m11:46:08.686875 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.fact_sales
[0m11:46:08.690280 [debug] [Thread-3 (]: Writing injected SQL for node "model.sales_pipelines.dim_customers"
[0m11:46:08.702312 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.dim_date"
[0m11:46:08.708224 [debug] [Thread-5 (]: Writing injected SQL for node "model.sales_pipelines.dim_products"
[0m11:46:08.717016 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.fact_sales"
[0m11:46:08.721564 [debug] [Thread-3 (]: Began executing node model.sales_pipelines.dim_customers
[0m11:46:08.721564 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.dim_date
[0m11:46:08.721564 [debug] [Thread-5 (]: Began executing node model.sales_pipelines.dim_products
[0m11:46:08.721564 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.fact_sales
[0m11:46:08.766068 [debug] [Thread-3 (]: Writing runtime sql for node "model.sales_pipelines.dim_customers"
[0m11:46:08.769948 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.dim_date"
[0m11:46:08.781391 [debug] [Thread-5 (]: Writing runtime sql for node "model.sales_pipelines.dim_products"
[0m11:46:08.786298 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:46:08.786298 [debug] [Thread-3 (]: Using snowflake connection "model.sales_pipelines.dim_customers"
[0m11:46:08.786298 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.dim_date"
[0m11:46:08.796361 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: create or replace  temporary view INCREMENTALETL._marts.fact_sales__dbt_tmp
  
  
  
  
  as (
    

SELECT
    invoice_no,
    stock_code,
    customer_id,
    invoice_date::DATE AS date_key,

    quantity,
    unit_price,
    discount,
    shipping_cost,

    gross_amount,
    net_amount,
    total_amount,

    is_returned,
    processed_at

FROM INCREMENTALETL._processed.processed_sales


WHERE processed_at > (SELECT MAX(processed_at) FROM INCREMENTALETL._marts.fact_sales)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */;
[0m11:46:08.798052 [debug] [Thread-5 (]: Using snowflake connection "model.sales_pipelines.dim_products"
[0m11:46:08.798052 [debug] [Thread-3 (]: On model.sales_pipelines.dim_customers: create or replace transient table INCREMENTALETL._marts.dim_customers
    
    
    
    as (

SELECT DISTINCT
    customer_id,
    country,
    payment_method,
    sales_channel,
    is_guest_customer,
    MIN(invoice_date) AS first_purchase_date,
    MAX(invoice_date) AS last_purchase_date,
    COUNT(DISTINCT invoice_no) AS total_orders
FROM INCREMENTALETL._processed.processed_sales
WHERE customer_id IS NOT NULL
GROUP BY
    customer_id,
    country,
    payment_method,
    sales_channel,
    is_guest_customer
    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.dim_customers"} */;
[0m11:46:08.798052 [debug] [Thread-2 (]: On model.sales_pipelines.dim_date: create or replace transient table INCREMENTALETL._marts.dim_date
    
    
    
    as (

SELECT DISTINCT
    invoice_date::DATE AS date_key,
    YEAR(invoice_date)  AS year,
    MONTH(invoice_date) AS month,
    DAY(invoice_date)   AS day,
    DAYNAME(invoice_date) AS day_name,
    WEEK(invoice_date)  AS week_number,
    QUARTER(invoice_date) AS quarter
FROM INCREMENTALETL._processed.processed_sales
    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.dim_date"} */;
[0m11:46:08.805023 [debug] [Thread-5 (]: On model.sales_pipelines.dim_products: create or replace transient table INCREMENTALETL._marts.dim_products
    
    
    
    as (

SELECT DISTINCT
    stock_code,
    description,
    category
FROM INCREMENTALETL._processed.processed_sales
    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.dim_products"} */;
[0m11:46:08.805731 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m11:46:08.805731 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m11:46:09.047927 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.248 seconds
[0m11:46:09.055888 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:46:09.055888 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: describe table INCREMENTALETL._marts.fact_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:46:09.241685 [debug] [Thread-4 (]: SQL status: SUCCESS 13 in 0.183 seconds
[0m11:46:09.249626 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:46:09.264797 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: describe table INCREMENTALETL._marts.fact_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:46:09.401674 [debug] [Thread-4 (]: SQL status: SUCCESS 13 in 0.137 seconds
[0m11:46:09.418053 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:46:09.426211 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: describe table "INCREMENTALETL"."_MARTS"."FACT_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:46:09.490067 [debug] [Thread-2 (]: SQL status: SUCCESS 1369 in 0.685 seconds
[0m11:46:09.516263 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6a236df9-f784-4a68-ab66-3e206b3ab750', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017BC3FA5600>]}
[0m11:46:09.520785 [info ] [Thread-2 (]: 4 of 6 OK created sql table model _marts.dim_date .............................. [[32mSUCCESS 1369[0m in 0.84s]
[0m11:46:09.520785 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.dim_date
[0m11:46:09.565210 [debug] [Thread-4 (]: SQL status: SUCCESS 13 in 0.134 seconds
[0m11:46:09.571152 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.fact_sales"
[0m11:46:09.583096 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:46:09.583096 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */;
[0m11:46:09.760601 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.178 seconds
[0m11:46:09.760601 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:46:09.760601 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: merge into INCREMENTALETL._marts.fact_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._marts.fact_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","DATE_KEY" = DBT_INTERNAL_SOURCE."DATE_KEY","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","GROSS_AMOUNT" = DBT_INTERNAL_SOURCE."GROSS_AMOUNT","NET_AMOUNT" = DBT_INTERNAL_SOURCE."NET_AMOUNT","TOTAL_AMOUNT" = DBT_INTERNAL_SOURCE."TOTAL_AMOUNT","IS_RETURNED" = DBT_INTERNAL_SOURCE."IS_RETURNED","PROCESSED_AT" = DBT_INTERNAL_SOURCE."PROCESSED_AT"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "CUSTOMER_ID", "DATE_KEY", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "PROCESSED_AT")
    values
        ("INVOICE_NO", "STOCK_CODE", "CUSTOMER_ID", "DATE_KEY", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "PROCESSED_AT")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */;
[0m11:46:10.066745 [debug] [Thread-5 (]: SQL status: SUCCESS 24304 in 1.260 seconds
[0m11:46:10.083329 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6a236df9-f784-4a68-ab66-3e206b3ab750', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017BC3800850>]}
[0m11:46:10.086648 [info ] [Thread-5 (]: 5 of 6 OK created sql table model _marts.dim_products .......................... [[32mSUCCESS 24304[0m in 1.41s]
[0m11:46:10.091330 [debug] [Thread-5 (]: Finished running node model.sales_pipelines.dim_products
[0m11:46:10.386643 [debug] [Thread-3 (]: SQL status: SUCCESS 29533 in 1.581 seconds
[0m11:46:10.386643 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6a236df9-f784-4a68-ab66-3e206b3ab750', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017BC38010F0>]}
[0m11:46:10.402582 [info ] [Thread-3 (]: 3 of 6 OK created sql table model _marts.dim_customers ......................... [[32mSUCCESS 29533[0m in 1.71s]
[0m11:46:10.402582 [debug] [Thread-3 (]: Finished running node model.sales_pipelines.dim_customers
[0m11:46:10.583553 [debug] [Thread-4 (]: SQL status: SUCCESS 712 in 0.810 seconds
[0m11:46:10.583553 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:46:10.583553 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:46:10.879722 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.294 seconds
[0m11:46:10.879722 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._marts.fact_sales__dbt_tmp
[0m11:46:10.894170 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:46:10.895299 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: drop view if exists INCREMENTALETL._marts.fact_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:46:11.073512 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.177 seconds
[0m11:46:11.087435 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6a236df9-f784-4a68-ab66-3e206b3ab750', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017BB08F7CD0>]}
[0m11:46:11.087435 [info ] [Thread-4 (]: 6 of 6 OK created sql incremental model _marts.fact_sales ...................... [[32mSUCCESS 712[0m in 2.41s]
[0m11:46:11.096181 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.fact_sales
[0m11:46:11.105338 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:46:11.105338 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m11:46:11.109441 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m11:46:11.352223 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m11:46:11.352223 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m11:46:11.728884 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m11:46:11.731905 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m11:46:11.993697 [debug] [MainThread]: Connection 'list_INCREMENTALETL__marts' was left open.
[0m11:46:11.998517 [debug] [MainThread]: On list_INCREMENTALETL__marts: Close
[0m11:46:12.348497 [debug] [MainThread]: Connection 'list_INCREMENTALETL__staging' was left open.
[0m11:46:12.352007 [debug] [MainThread]: On list_INCREMENTALETL__staging: Close
[0m11:46:12.828110 [debug] [MainThread]: Connection 'list_INCREMENTALETL__processed' was left open.
[0m11:46:12.828110 [debug] [MainThread]: On list_INCREMENTALETL__processed: Close
[0m11:46:13.150896 [debug] [MainThread]: Connection 'model.sales_pipelines.dim_date' was left open.
[0m11:46:13.150896 [debug] [MainThread]: On model.sales_pipelines.dim_date: Close
[0m11:46:13.537818 [debug] [MainThread]: Connection 'model.sales_pipelines.fact_sales' was left open.
[0m11:46:13.537818 [debug] [MainThread]: On model.sales_pipelines.fact_sales: Close
[0m11:46:13.790220 [debug] [MainThread]: Connection 'model.sales_pipelines.dim_customers' was left open.
[0m11:46:13.790220 [debug] [MainThread]: On model.sales_pipelines.dim_customers: Close
[0m11:46:14.059512 [debug] [MainThread]: Connection 'model.sales_pipelines.dim_products' was left open.
[0m11:46:14.065664 [debug] [MainThread]: On model.sales_pipelines.dim_products: Close
[0m11:46:14.485411 [info ] [MainThread]: 
[0m11:46:14.485411 [info ] [MainThread]: Finished running 3 incremental models, 3 table models in 0 hours 0 minutes and 17.38 seconds (17.38s).
[0m11:46:14.501374 [debug] [MainThread]: Command end result
[0m11:46:14.565388 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m11:46:14.572495 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m11:46:14.581970 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\snowflake-incremental-pipeline\dbt_project\target\run_results.json
[0m11:46:14.581970 [info ] [MainThread]: 
[0m11:46:14.586356 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:46:14.586356 [info ] [MainThread]: 
[0m11:46:14.588689 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=6
[0m11:46:14.588689 [debug] [MainThread]: Command `dbt run` succeeded at 11:46:14.588689 after 20.23 seconds
[0m11:46:14.591498 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017BAFDF76D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017BC20B3880>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017BC37921D0>]}
[0m11:46:14.591498 [debug] [MainThread]: Flushing usage events
[0m11:46:16.512951 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:46:26.106592 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000177CA1D76A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000177CC451000>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000177CC4523B0>]}


============================== 11:46:26.106592 | aa5659a6-366f-4831-a167-2f36a50bc056 ==============================
[0m11:46:26.106592 [info ] [MainThread]: Running with dbt=1.11.6
[0m11:46:26.106592 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'profiles_dir': 'D:\\snowflake-incremental-pipeline\\dbt_project', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'indirect_selection': 'eager', 'use_colors': 'True', 'invocation_command': 'dbt run', 'use_experimental_parser': 'False', 'log_path': 'D:\\snowflake-incremental-pipeline\\dbt_project\\logs'}
[0m11:46:27.174366 [debug] [MainThread]: Snowflake adapter: Setting snowflake.connector to ERROR (file logging only)
[0m11:46:27.187934 [debug] [MainThread]: Snowflake adapter: Setting botocore to ERROR (file logging only)
[0m11:46:27.187934 [debug] [MainThread]: Snowflake adapter: Setting boto3 to ERROR (file logging only)
[0m11:46:27.487997 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'aa5659a6-366f-4831-a167-2f36a50bc056', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000177CC4952A0>]}
[0m11:46:27.562814 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'aa5659a6-366f-4831-a167-2f36a50bc056', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000177CC4AD4B0>]}
[0m11:46:27.562814 [info ] [MainThread]: Registered adapter: snowflake=1.11.2
[0m11:46:28.063210 [debug] [MainThread]: checksum: ec27062d18352e9cfca161c166fcc73d2201661a23a3cf3c89196a27f32cfcb8, vars: {}, profile: , target: , version: 1.11.6
[0m11:46:28.284713 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:46:28.284713 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m11:46:28.284713 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:46:28.363356 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'aa5659a6-366f-4831-a167-2f36a50bc056', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000177DEB58A30>]}
[0m11:46:28.505949 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m11:46:28.518351 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m11:46:28.537980 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'aa5659a6-366f-4831-a167-2f36a50bc056', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000177DEB58E20>]}
[0m11:46:28.539721 [info ] [MainThread]: Found 6 models, 6 data tests, 1 source, 522 macros
[0m11:46:28.539721 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'aa5659a6-366f-4831-a167-2f36a50bc056', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000177DEB59480>]}
[0m11:46:28.542428 [info ] [MainThread]: 
[0m11:46:28.542428 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m11:46:28.542428 [info ] [MainThread]: 
[0m11:46:28.547563 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m11:46:28.557016 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m11:46:28.563671 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m11:46:28.581693 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m11:46:28.655328 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m11:46:28.655328 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m11:46:28.655328 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m11:46:28.655328 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m11:46:28.655328 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m11:46:28.671057 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m11:46:28.671057 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:46:28.671057 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:46:28.672987 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:46:30.057647 [debug] [ThreadPool]: SQL status: SUCCESS 5 in 1.385 seconds
[0m11:46:30.061370 [debug] [ThreadPool]: SQL status: SUCCESS 5 in 1.389 seconds
[0m11:46:30.069153 [debug] [ThreadPool]: SQL status: SUCCESS 5 in 1.399 seconds
[0m11:46:30.077715 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__processed'
[0m11:46:30.096948 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__staging'
[0m11:46:30.103100 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__processed"
[0m11:46:30.103100 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__staging"
[0m11:46:30.103100 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__marts'
[0m11:46:30.112742 [debug] [ThreadPool]: On list_INCREMENTALETL__processed: show objects in INCREMENTALETL._processed
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__processed"} */;
[0m11:46:30.115502 [debug] [ThreadPool]: On list_INCREMENTALETL__staging: show objects in INCREMENTALETL._staging
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__staging"} */;
[0m11:46:30.117297 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__marts"
[0m11:46:30.117297 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:46:30.117297 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:46:30.126106 [debug] [ThreadPool]: On list_INCREMENTALETL__marts: show objects in INCREMENTALETL._marts
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__marts"} */;
[0m11:46:30.128524 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:46:31.264361 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 1.139 seconds
[0m11:46:31.271585 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 1.149 seconds
[0m11:46:31.750363 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 1.628 seconds
[0m11:46:31.760638 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'aa5659a6-366f-4831-a167-2f36a50bc056', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000177DEC70340>]}
[0m11:46:31.782815 [debug] [Thread-2 (]: Began running node model.sales_pipelines.stg_sales
[0m11:46:31.782815 [info ] [Thread-2 (]: 1 of 6 START sql incremental model _staging.stg_sales .......................... [RUN]
[0m11:46:31.792649 [debug] [Thread-2 (]: Acquiring new snowflake connection 'model.sales_pipelines.stg_sales'
[0m11:46:31.799012 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.stg_sales
[0m11:46:31.871302 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.stg_sales"
[0m11:46:31.871302 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.stg_sales
[0m11:46:31.970764 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:46:31.970764 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: create or replace  temporary view INCREMENTALETL._staging.stg_sales__dbt_tmp
  
  
  
  
  as (
    

WITH raw_data AS (
    SELECT * 
    FROM INCREMENTALETL.LANDINGZONE.RAW_SALES
),

staged AS (
    SELECT
        -- IDs
        CAST(INVOICENO AS VARCHAR)    AS invoice_no,
        CAST(STOCKCODE AS VARCHAR)    AS stock_code,
        CAST(DESCRIPTION AS VARCHAR)  AS description,

        -- Quantities & Prices
        TRY_CAST(QUANTITY AS INT)      AS quantity,
        TRY_CAST(UNITPRICE AS FLOAT)   AS unit_price,
        TRY_CAST(DISCOUNT AS FLOAT)    AS discount,
        TRY_CAST(SHIPPINGCOST AS FLOAT) AS shipping_cost,
        TRY_CAST(CUSTOMERID AS FLOAT)  AS customer_id,

        -- Convert once
        TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS') AS invoice_date,

        TRIM(UPPER(COUNTRY))            AS country,
        TRIM(UPPER(PAYMENTMETHOD))      AS payment_method,
        TRIM(UPPER(CATEGORY))           AS category,
        TRIM(UPPER(SALESCHANNEL))       AS sales_channel,
        TRIM(UPPER(RETURNSTATUS))       AS return_status,
        TRIM(UPPER(SHIPMENTPROVIDER))   AS shipment_provider,
        TRIM(UPPER(WAREHOUSELOCATION))  AS warehouse_location,
        TRIM(UPPER(ORDERPRIORITY))      AS order_priority,

        CURRENT_TIMESTAMP()             AS stg_loaded_at

    FROM raw_data
),

validated AS (
    SELECT
        *,
        YEAR(invoice_date)  AS invoice_year,
        MONTH(invoice_date) AS invoice_month,

        CASE
            WHEN invoice_date IS NULL           THEN 'INVALID_DATE'
            WHEN quantity IS NULL               THEN 'INVALID_QUANTITY'
            WHEN unit_price IS NULL             THEN 'INVALID_PRICE'
            WHEN shipping_cost < 0              THEN 'NEGATIVE_SHIPPING'
            WHEN discount < 0 OR discount > 1   THEN 'INVALID_DISCOUNT'
            WHEN invoice_no IS NULL             THEN 'NULL_INVOICE'
            ELSE 'VALID'
        END AS data_quality_flag
    FROM staged
)

SELECT *
FROM validated


WHERE NOT EXISTS (
    SELECT 1
    FROM INCREMENTALETL._staging.stg_sales t
    WHERE t.invoice_year  = validated.invoice_year
      AND t.invoice_month = validated.invoice_month
)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m11:46:31.970764 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m11:46:32.986321 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 1.019 seconds
[0m11:46:33.028818 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:46:33.028818 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:46:33.213987 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.185 seconds
[0m11:46:33.226201 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:46:33.243478 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:46:33.421081 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.175 seconds
[0m11:46:33.466459 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:46:33.468318 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:46:33.612067 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.155 seconds
[0m11:46:33.643776 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:46:33.643776 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table "INCREMENTALETL"."_STAGING"."STG_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:46:33.830449 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.180 seconds
[0m11:46:33.872911 [debug] [Thread-2 (]: 
    In "INCREMENTALETL"."_STAGING"."STG_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m11:46:33.904312 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.stg_sales"
[0m11:46:33.904312 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:46:33.904312 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m11:46:34.130909 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.217 seconds
[0m11:46:34.130909 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:46:34.130909 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: merge into INCREMENTALETL._staging.stg_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._staging.stg_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","DATA_QUALITY_FLAG" = DBT_INTERNAL_SOURCE."DATA_QUALITY_FLAG"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "INVOICE_YEAR", "INVOICE_MONTH", "DATA_QUALITY_FLAG")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "INVOICE_YEAR", "INVOICE_MONTH", "DATA_QUALITY_FLAG")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m11:46:35.277124 [debug] [Thread-2 (]: SQL status: SUCCESS 744 in 1.148 seconds
[0m11:46:35.293106 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:46:35.293106 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:46:35.662272 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.373 seconds
[0m11:46:35.691043 [debug] [Thread-2 (]: Applying DROP to: INCREMENTALETL._staging.stg_sales__dbt_tmp
[0m11:46:35.707018 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:46:35.707018 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: drop view if exists INCREMENTALETL._staging.stg_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:46:35.901550 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.193 seconds
[0m11:46:35.969649 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'aa5659a6-366f-4831-a167-2f36a50bc056', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000177CA028790>]}
[0m11:46:35.969649 [info ] [Thread-2 (]: 1 of 6 OK created sql incremental model _staging.stg_sales ..................... [[32mSUCCESS 744[0m in 4.17s]
[0m11:46:35.974553 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.stg_sales
[0m11:46:35.974553 [debug] [Thread-4 (]: Began running node model.sales_pipelines.processed_sales
[0m11:46:35.977370 [info ] [Thread-4 (]: 2 of 6 START sql incremental model _processed.processed_sales .................. [RUN]
[0m11:46:35.980565 [debug] [Thread-4 (]: Acquiring new snowflake connection 'model.sales_pipelines.processed_sales'
[0m11:46:35.982058 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.processed_sales
[0m11:46:35.992190 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.processed_sales"
[0m11:46:35.995417 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.processed_sales
[0m11:46:36.007544 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:46:36.007544 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: create or replace  temporary view INCREMENTALETL._processed.processed_sales__dbt_tmp
  
  
  
  
  as (
    

WITH staging AS (
    SELECT * 
    FROM INCREMENTALETL._staging.stg_sales
),

deduped AS (
    SELECT *,
        ROW_NUMBER() OVER (
            PARTITION BY invoice_no, stock_code
            ORDER BY stg_loaded_at DESC
        ) AS row_num
    FROM staging
),

cleaned AS (
    SELECT
        invoice_no,
        stock_code,
        description,
        quantity,
        unit_price,
        discount,
        shipping_cost,
        customer_id,
        invoice_date,
        invoice_year,
        invoice_month,
        country,
        payment_method,
        category,
        sales_channel,
        return_status,
        shipment_provider,
        warehouse_location,
        order_priority,

        -- Derived metrics
        ROUND(quantity * unit_price, 2) AS gross_amount,
        ROUND(quantity * unit_price * (1 - discount), 2) AS net_amount,
        ROUND(
            quantity * unit_price * (1 - discount)
            + COALESCE(shipping_cost, 0),
        2) AS total_amount,

        CASE WHEN return_status = 'RETURNED' THEN TRUE ELSE FALSE END AS is_returned,
        CASE WHEN customer_id IS NULL THEN TRUE ELSE FALSE END AS is_guest_customer,

        stg_loaded_at,
        CURRENT_TIMESTAMP() AS processed_at

    FROM deduped
    WHERE
        row_num = 1
        AND data_quality_flag = 'VALID'
        AND invoice_date IS NOT NULL
)

SELECT *
FROM cleaned


WHERE NOT EXISTS (
    SELECT 1
    FROM INCREMENTALETL._processed.processed_sales t
    WHERE t.invoice_year  = cleaned.invoice_year
      AND t.invoice_month = cleaned.invoice_month
)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m11:46:36.010210 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m11:46:36.851498 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.845 seconds
[0m11:46:36.860122 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:46:36.860122 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:46:36.999711 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.145 seconds
[0m11:46:37.018645 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:46:37.018645 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:46:37.149735 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.127 seconds
[0m11:46:37.166736 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:46:37.166736 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:46:37.409972 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.238 seconds
[0m11:46:37.424052 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:46:37.426059 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:46:37.561412 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.135 seconds
[0m11:46:37.597303 [debug] [Thread-4 (]: 
    In "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m11:46:37.601327 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.processed_sales"
[0m11:46:37.607097 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:46:37.609106 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m11:46:37.768521 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.159 seconds
[0m11:46:37.770537 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:46:37.772554 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: merge into INCREMENTALETL._processed.processed_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._processed.processed_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","GROSS_AMOUNT" = DBT_INTERNAL_SOURCE."GROSS_AMOUNT","NET_AMOUNT" = DBT_INTERNAL_SOURCE."NET_AMOUNT","TOTAL_AMOUNT" = DBT_INTERNAL_SOURCE."TOTAL_AMOUNT","IS_RETURNED" = DBT_INTERNAL_SOURCE."IS_RETURNED","IS_GUEST_CUSTOMER" = DBT_INTERNAL_SOURCE."IS_GUEST_CUSTOMER","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","PROCESSED_AT" = DBT_INTERNAL_SOURCE."PROCESSED_AT"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m11:46:38.839534 [debug] [Thread-4 (]: SQL status: SUCCESS 728 in 1.065 seconds
[0m11:46:38.839534 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:46:38.842559 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:46:39.169662 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.325 seconds
[0m11:46:39.174014 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._processed.processed_sales__dbt_tmp
[0m11:46:39.177886 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:46:39.177886 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: drop view if exists INCREMENTALETL._processed.processed_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:46:39.373583 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.197 seconds
[0m11:46:39.379961 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'aa5659a6-366f-4831-a167-2f36a50bc056', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000177DD657310>]}
[0m11:46:39.382307 [info ] [Thread-4 (]: 2 of 6 OK created sql incremental model _processed.processed_sales ............. [[32mSUCCESS 728[0m in 3.40s]
[0m11:46:39.383387 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.processed_sales
[0m11:46:39.383387 [debug] [Thread-3 (]: Began running node model.sales_pipelines.dim_customers
[0m11:46:39.383387 [debug] [Thread-2 (]: Began running node model.sales_pipelines.dim_date
[0m11:46:39.383387 [debug] [Thread-5 (]: Began running node model.sales_pipelines.dim_products
[0m11:46:39.383387 [debug] [Thread-4 (]: Began running node model.sales_pipelines.fact_sales
[0m11:46:39.383387 [info ] [Thread-3 (]: 3 of 6 START sql table model _marts.dim_customers .............................. [RUN]
[0m11:46:39.390209 [info ] [Thread-2 (]: 4 of 6 START sql table model _marts.dim_date ................................... [RUN]
[0m11:46:39.391496 [info ] [Thread-5 (]: 5 of 6 START sql table model _marts.dim_products ............................... [RUN]
[0m11:46:39.393081 [info ] [Thread-4 (]: 6 of 6 START sql incremental model _marts.fact_sales ........................... [RUN]
[0m11:46:39.394556 [debug] [Thread-3 (]: Acquiring new snowflake connection 'model.sales_pipelines.dim_customers'
[0m11:46:39.396576 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.sales_pipelines.stg_sales, now model.sales_pipelines.dim_date)
[0m11:46:39.396576 [debug] [Thread-5 (]: Acquiring new snowflake connection 'model.sales_pipelines.dim_products'
[0m11:46:39.398785 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.sales_pipelines.processed_sales, now model.sales_pipelines.fact_sales)
[0m11:46:39.400230 [debug] [Thread-3 (]: Began compiling node model.sales_pipelines.dim_customers
[0m11:46:39.401321 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.dim_date
[0m11:46:39.403363 [debug] [Thread-5 (]: Began compiling node model.sales_pipelines.dim_products
[0m11:46:39.403363 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.fact_sales
[0m11:46:39.416420 [debug] [Thread-3 (]: Writing injected SQL for node "model.sales_pipelines.dim_customers"
[0m11:46:39.424710 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.dim_date"
[0m11:46:39.431874 [debug] [Thread-5 (]: Writing injected SQL for node "model.sales_pipelines.dim_products"
[0m11:46:39.435910 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.fact_sales"
[0m11:46:39.435910 [debug] [Thread-3 (]: Began executing node model.sales_pipelines.dim_customers
[0m11:46:39.444352 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.dim_date
[0m11:46:39.445965 [debug] [Thread-5 (]: Began executing node model.sales_pipelines.dim_products
[0m11:46:39.484503 [debug] [Thread-3 (]: Writing runtime sql for node "model.sales_pipelines.dim_customers"
[0m11:46:39.484503 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.fact_sales
[0m11:46:39.495162 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.dim_date"
[0m11:46:39.498749 [debug] [Thread-5 (]: Writing runtime sql for node "model.sales_pipelines.dim_products"
[0m11:46:39.508940 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:46:39.512199 [debug] [Thread-3 (]: Using snowflake connection "model.sales_pipelines.dim_customers"
[0m11:46:39.514896 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.dim_date"
[0m11:46:39.515390 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: create or replace  temporary view INCREMENTALETL._marts.fact_sales__dbt_tmp
  
  
  
  
  as (
    

SELECT
    invoice_no,
    stock_code,
    customer_id,
    invoice_date::DATE AS date_key,

    quantity,
    unit_price,
    discount,
    shipping_cost,

    gross_amount,
    net_amount,
    total_amount,

    is_returned,
    processed_at

FROM INCREMENTALETL._processed.processed_sales


WHERE processed_at > (SELECT MAX(processed_at) FROM INCREMENTALETL._marts.fact_sales)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */;
[0m11:46:39.516418 [debug] [Thread-5 (]: Using snowflake connection "model.sales_pipelines.dim_products"
[0m11:46:39.516418 [debug] [Thread-3 (]: On model.sales_pipelines.dim_customers: create or replace transient table INCREMENTALETL._marts.dim_customers
    
    
    
    as (

SELECT DISTINCT
    customer_id,
    country,
    payment_method,
    sales_channel,
    is_guest_customer,
    MIN(invoice_date) AS first_purchase_date,
    MAX(invoice_date) AS last_purchase_date,
    COUNT(DISTINCT invoice_no) AS total_orders
FROM INCREMENTALETL._processed.processed_sales
WHERE customer_id IS NOT NULL
GROUP BY
    customer_id,
    country,
    payment_method,
    sales_channel,
    is_guest_customer
    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.dim_customers"} */;
[0m11:46:39.519107 [debug] [Thread-2 (]: On model.sales_pipelines.dim_date: create or replace transient table INCREMENTALETL._marts.dim_date
    
    
    
    as (

SELECT DISTINCT
    invoice_date::DATE AS date_key,
    YEAR(invoice_date)  AS year,
    MONTH(invoice_date) AS month,
    DAY(invoice_date)   AS day,
    DAYNAME(invoice_date) AS day_name,
    WEEK(invoice_date)  AS week_number,
    QUARTER(invoice_date) AS quarter
FROM INCREMENTALETL._processed.processed_sales
    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.dim_date"} */;
[0m11:46:39.519107 [debug] [Thread-5 (]: On model.sales_pipelines.dim_products: create or replace transient table INCREMENTALETL._marts.dim_products
    
    
    
    as (

SELECT DISTINCT
    stock_code,
    description,
    category
FROM INCREMENTALETL._processed.processed_sales
    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.dim_products"} */;
[0m11:46:39.522453 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m11:46:39.527478 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m11:46:39.771574 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.258 seconds
[0m11:46:39.782907 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:46:39.782907 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: describe table INCREMENTALETL._marts.fact_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:46:39.916950 [debug] [Thread-4 (]: SQL status: SUCCESS 13 in 0.140 seconds
[0m11:46:39.933269 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:46:39.933269 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: describe table INCREMENTALETL._marts.fact_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:46:40.059720 [debug] [Thread-4 (]: SQL status: SUCCESS 13 in 0.126 seconds
[0m11:46:40.065885 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:46:40.065885 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: describe table "INCREMENTALETL"."_MARTS"."FACT_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:46:40.134810 [debug] [Thread-2 (]: SQL status: SUCCESS 1400 in 0.613 seconds
[0m11:46:40.147364 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'aa5659a6-366f-4831-a167-2f36a50bc056', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000177DD48D5D0>]}
[0m11:46:40.149354 [info ] [Thread-2 (]: 4 of 6 OK created sql table model _marts.dim_date .............................. [[32mSUCCESS 1400[0m in 0.74s]
[0m11:46:40.149354 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.dim_date
[0m11:46:40.198786 [debug] [Thread-4 (]: SQL status: SUCCESS 13 in 0.128 seconds
[0m11:46:40.202522 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.fact_sales"
[0m11:46:40.202522 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:46:40.202522 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */;
[0m11:46:40.370301 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.164 seconds
[0m11:46:40.370301 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:46:40.370301 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: merge into INCREMENTALETL._marts.fact_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._marts.fact_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","DATE_KEY" = DBT_INTERNAL_SOURCE."DATE_KEY","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","GROSS_AMOUNT" = DBT_INTERNAL_SOURCE."GROSS_AMOUNT","NET_AMOUNT" = DBT_INTERNAL_SOURCE."NET_AMOUNT","TOTAL_AMOUNT" = DBT_INTERNAL_SOURCE."TOTAL_AMOUNT","IS_RETURNED" = DBT_INTERNAL_SOURCE."IS_RETURNED","PROCESSED_AT" = DBT_INTERNAL_SOURCE."PROCESSED_AT"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "CUSTOMER_ID", "DATE_KEY", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "PROCESSED_AT")
    values
        ("INVOICE_NO", "STOCK_CODE", "CUSTOMER_ID", "DATE_KEY", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "PROCESSED_AT")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */;
[0m11:46:40.998038 [debug] [Thread-3 (]: SQL status: SUCCESS 30196 in 1.475 seconds
[0m11:46:41.008136 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'aa5659a6-366f-4831-a167-2f36a50bc056', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000177DF48FB20>]}
[0m11:46:41.010160 [info ] [Thread-3 (]: 3 of 6 OK created sql table model _marts.dim_customers ......................... [[32mSUCCESS 30196[0m in 1.61s]
[0m11:46:41.014847 [debug] [Thread-3 (]: Finished running node model.sales_pipelines.dim_customers
[0m11:46:41.102108 [debug] [Thread-4 (]: SQL status: SUCCESS 728 in 0.723 seconds
[0m11:46:41.107587 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:46:41.107587 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:46:41.276259 [debug] [Thread-5 (]: SQL status: SUCCESS 24685 in 1.749 seconds
[0m11:46:41.276259 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'aa5659a6-366f-4831-a167-2f36a50bc056', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000177DF4F41C0>]}
[0m11:46:41.294108 [info ] [Thread-5 (]: 5 of 6 OK created sql table model _marts.dim_products .......................... [[32mSUCCESS 24685[0m in 1.88s]
[0m11:46:41.300326 [debug] [Thread-5 (]: Finished running node model.sales_pipelines.dim_products
[0m11:46:41.373048 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.261 seconds
[0m11:46:41.377067 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._marts.fact_sales__dbt_tmp
[0m11:46:41.379076 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:46:41.381084 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: drop view if exists INCREMENTALETL._marts.fact_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:46:41.568996 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.186 seconds
[0m11:46:41.568996 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'aa5659a6-366f-4831-a167-2f36a50bc056', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000177DF4F7AC0>]}
[0m11:46:41.585446 [info ] [Thread-4 (]: 6 of 6 OK created sql incremental model _marts.fact_sales ...................... [[32mSUCCESS 728[0m in 2.17s]
[0m11:46:41.585446 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.fact_sales
[0m11:46:41.600116 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:46:41.603839 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m11:46:41.605719 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m11:46:41.838643 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m11:46:41.842692 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m11:46:42.066426 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m11:46:42.066426 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m11:46:42.347251 [debug] [MainThread]: Connection 'list_INCREMENTALETL__processed' was left open.
[0m11:46:42.354298 [debug] [MainThread]: On list_INCREMENTALETL__processed: Close
[0m11:46:42.653378 [debug] [MainThread]: Connection 'list_INCREMENTALETL__staging' was left open.
[0m11:46:42.653378 [debug] [MainThread]: On list_INCREMENTALETL__staging: Close
[0m11:46:42.953273 [debug] [MainThread]: Connection 'list_INCREMENTALETL__marts' was left open.
[0m11:46:42.953273 [debug] [MainThread]: On list_INCREMENTALETL__marts: Close
[0m11:46:43.240830 [debug] [MainThread]: Connection 'model.sales_pipelines.dim_date' was left open.
[0m11:46:43.240830 [debug] [MainThread]: On model.sales_pipelines.dim_date: Close
[0m11:46:43.500822 [debug] [MainThread]: Connection 'model.sales_pipelines.fact_sales' was left open.
[0m11:46:43.500822 [debug] [MainThread]: On model.sales_pipelines.fact_sales: Close
[0m11:46:43.769558 [debug] [MainThread]: Connection 'model.sales_pipelines.dim_customers' was left open.
[0m11:46:43.769558 [debug] [MainThread]: On model.sales_pipelines.dim_customers: Close
[0m11:46:44.058796 [debug] [MainThread]: Connection 'model.sales_pipelines.dim_products' was left open.
[0m11:46:44.058796 [debug] [MainThread]: On model.sales_pipelines.dim_products: Close
[0m11:46:44.332699 [info ] [MainThread]: 
[0m11:46:44.332699 [info ] [MainThread]: Finished running 3 incremental models, 3 table models in 0 hours 0 minutes and 15.79 seconds (15.79s).
[0m11:46:44.348491 [debug] [MainThread]: Command end result
[0m11:46:44.416126 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m11:46:44.416126 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m11:46:44.434190 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\snowflake-incremental-pipeline\dbt_project\target\run_results.json
[0m11:46:44.434190 [info ] [MainThread]: 
[0m11:46:44.434190 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:46:44.434190 [info ] [MainThread]: 
[0m11:46:44.434190 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=6
[0m11:46:44.434190 [debug] [MainThread]: Command `dbt run` succeeded at 11:46:44.434190 after 18.46 seconds
[0m11:46:44.434190 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000177CA1D76A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000177DEB420B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000177CC4AD4B0>]}
[0m11:46:44.434190 [debug] [MainThread]: Flushing usage events
[0m11:46:47.029216 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:46:53.044131 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D8982D36D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D899586620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D8995870D0>]}


============================== 11:46:53.044131 | d6a55638-a1e9-42db-9c9f-a9d8cb8371aa ==============================
[0m11:46:53.044131 [info ] [MainThread]: Running with dbt=1.11.6
[0m11:46:53.044131 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'profiles_dir': 'D:\\snowflake-incremental-pipeline\\dbt_project', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'invocation_command': 'dbt run', 'use_colors': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'D:\\snowflake-incremental-pipeline\\dbt_project\\logs'}
[0m11:46:54.083208 [debug] [MainThread]: Snowflake adapter: Setting snowflake.connector to ERROR (file logging only)
[0m11:46:54.086102 [debug] [MainThread]: Snowflake adapter: Setting botocore to ERROR (file logging only)
[0m11:46:54.086102 [debug] [MainThread]: Snowflake adapter: Setting boto3 to ERROR (file logging only)
[0m11:46:54.446806 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd6a55638-a1e9-42db-9c9f-a9d8cb8371aa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D89607EB00>]}
[0m11:46:54.527129 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd6a55638-a1e9-42db-9c9f-a9d8cb8371aa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D898D471C0>]}
[0m11:46:54.527129 [info ] [MainThread]: Registered adapter: snowflake=1.11.2
[0m11:46:55.079624 [debug] [MainThread]: checksum: ec27062d18352e9cfca161c166fcc73d2201661a23a3cf3c89196a27f32cfcb8, vars: {}, profile: , target: , version: 1.11.6
[0m11:46:55.335644 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:46:55.335644 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m11:46:55.335644 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:46:55.416689 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd6a55638-a1e9-42db-9c9f-a9d8cb8371aa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D8ABC8CA60>]}
[0m11:46:55.583163 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m11:46:55.597923 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m11:46:55.616740 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd6a55638-a1e9-42db-9c9f-a9d8cb8371aa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D8ABC01600>]}
[0m11:46:55.616740 [info ] [MainThread]: Found 6 models, 6 data tests, 1 source, 522 macros
[0m11:46:55.616740 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd6a55638-a1e9-42db-9c9f-a9d8cb8371aa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D8ABC8C9A0>]}
[0m11:46:55.629727 [info ] [MainThread]: 
[0m11:46:55.629727 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m11:46:55.632412 [info ] [MainThread]: 
[0m11:46:55.634687 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m11:46:55.645838 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m11:46:55.647865 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m11:46:55.658554 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m11:46:55.765845 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m11:46:55.765845 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m11:46:55.779930 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m11:46:55.779930 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m11:46:55.779930 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m11:46:55.779930 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m11:46:55.779930 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:46:55.779930 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:46:55.779930 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:46:57.006467 [debug] [ThreadPool]: SQL status: SUCCESS 5 in 1.229 seconds
[0m11:46:57.017781 [debug] [ThreadPool]: SQL status: SUCCESS 5 in 1.240 seconds
[0m11:46:57.030602 [debug] [ThreadPool]: SQL status: SUCCESS 5 in 1.252 seconds
[0m11:46:57.046384 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__marts'
[0m11:46:57.062294 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__processed'
[0m11:46:57.094637 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__marts"
[0m11:46:57.094637 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__staging'
[0m11:46:57.094637 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__processed"
[0m11:46:57.094637 [debug] [ThreadPool]: On list_INCREMENTALETL__marts: show objects in INCREMENTALETL._marts
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__marts"} */;
[0m11:46:57.116161 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__staging"
[0m11:46:57.117274 [debug] [ThreadPool]: On list_INCREMENTALETL__processed: show objects in INCREMENTALETL._processed
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__processed"} */;
[0m11:46:57.119104 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:46:57.121229 [debug] [ThreadPool]: On list_INCREMENTALETL__staging: show objects in INCREMENTALETL._staging
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__staging"} */;
[0m11:46:57.123185 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:46:57.124761 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:46:57.978525 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 0.862 seconds
[0m11:46:58.026549 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.901 seconds
[0m11:46:58.042803 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.926 seconds
[0m11:46:58.058982 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd6a55638-a1e9-42db-9c9f-a9d8cb8371aa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D8AA747C10>]}
[0m11:46:58.074713 [debug] [Thread-2 (]: Began running node model.sales_pipelines.stg_sales
[0m11:46:58.074713 [info ] [Thread-2 (]: 1 of 6 START sql incremental model _staging.stg_sales .......................... [RUN]
[0m11:46:58.085866 [debug] [Thread-2 (]: Acquiring new snowflake connection 'model.sales_pipelines.stg_sales'
[0m11:46:58.085866 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.stg_sales
[0m11:46:58.136064 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.stg_sales"
[0m11:46:58.136064 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.stg_sales
[0m11:46:58.220099 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:46:58.232473 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: create or replace  temporary view INCREMENTALETL._staging.stg_sales__dbt_tmp
  
  
  
  
  as (
    

WITH raw_data AS (
    SELECT * 
    FROM INCREMENTALETL.LANDINGZONE.RAW_SALES
),

staged AS (
    SELECT
        -- IDs
        CAST(INVOICENO AS VARCHAR)    AS invoice_no,
        CAST(STOCKCODE AS VARCHAR)    AS stock_code,
        CAST(DESCRIPTION AS VARCHAR)  AS description,

        -- Quantities & Prices
        TRY_CAST(QUANTITY AS INT)      AS quantity,
        TRY_CAST(UNITPRICE AS FLOAT)   AS unit_price,
        TRY_CAST(DISCOUNT AS FLOAT)    AS discount,
        TRY_CAST(SHIPPINGCOST AS FLOAT) AS shipping_cost,
        TRY_CAST(CUSTOMERID AS FLOAT)  AS customer_id,

        -- Convert once
        TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS') AS invoice_date,

        TRIM(UPPER(COUNTRY))            AS country,
        TRIM(UPPER(PAYMENTMETHOD))      AS payment_method,
        TRIM(UPPER(CATEGORY))           AS category,
        TRIM(UPPER(SALESCHANNEL))       AS sales_channel,
        TRIM(UPPER(RETURNSTATUS))       AS return_status,
        TRIM(UPPER(SHIPMENTPROVIDER))   AS shipment_provider,
        TRIM(UPPER(WAREHOUSELOCATION))  AS warehouse_location,
        TRIM(UPPER(ORDERPRIORITY))      AS order_priority,

        CURRENT_TIMESTAMP()             AS stg_loaded_at

    FROM raw_data
),

validated AS (
    SELECT
        *,
        YEAR(invoice_date)  AS invoice_year,
        MONTH(invoice_date) AS invoice_month,

        CASE
            WHEN invoice_date IS NULL           THEN 'INVALID_DATE'
            WHEN quantity IS NULL               THEN 'INVALID_QUANTITY'
            WHEN unit_price IS NULL             THEN 'INVALID_PRICE'
            WHEN shipping_cost < 0              THEN 'NEGATIVE_SHIPPING'
            WHEN discount < 0 OR discount > 1   THEN 'INVALID_DISCOUNT'
            WHEN invoice_no IS NULL             THEN 'NULL_INVOICE'
            ELSE 'VALID'
        END AS data_quality_flag
    FROM staged
)

SELECT *
FROM validated


WHERE NOT EXISTS (
    SELECT 1
    FROM INCREMENTALETL._staging.stg_sales t
    WHERE t.invoice_year  = validated.invoice_year
      AND t.invoice_month = validated.invoice_month
)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m11:46:58.232473 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m11:46:59.318865 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 1.092 seconds
[0m11:46:59.446228 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:46:59.449326 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:46:59.610397 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.166 seconds
[0m11:46:59.639989 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:46:59.639989 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:46:59.836470 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.192 seconds
[0m11:46:59.885371 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:46:59.885371 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:47:00.138734 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.254 seconds
[0m11:47:00.148830 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:47:00.164659 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table "INCREMENTALETL"."_STAGING"."STG_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:47:00.441688 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.276 seconds
[0m11:47:00.486209 [debug] [Thread-2 (]: 
    In "INCREMENTALETL"."_STAGING"."STG_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m11:47:00.517895 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.stg_sales"
[0m11:47:00.517895 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:47:00.517895 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m11:47:00.758248 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.231 seconds
[0m11:47:00.758248 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:47:00.758248 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: merge into INCREMENTALETL._staging.stg_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._staging.stg_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","DATA_QUALITY_FLAG" = DBT_INTERNAL_SOURCE."DATA_QUALITY_FLAG"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "INVOICE_YEAR", "INVOICE_MONTH", "DATA_QUALITY_FLAG")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "INVOICE_YEAR", "INVOICE_MONTH", "DATA_QUALITY_FLAG")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m11:47:01.918139 [debug] [Thread-2 (]: SQL status: SUCCESS 720 in 1.151 seconds
[0m11:47:01.918139 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:47:01.918139 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:47:02.292280 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.365 seconds
[0m11:47:02.294853 [debug] [Thread-2 (]: Applying DROP to: INCREMENTALETL._staging.stg_sales__dbt_tmp
[0m11:47:02.312015 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:47:02.312015 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: drop view if exists INCREMENTALETL._staging.stg_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:47:02.594455 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.280 seconds
[0m11:47:02.643441 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd6a55638-a1e9-42db-9c9f-a9d8cb8371aa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D899618EE0>]}
[0m11:47:02.643441 [info ] [Thread-2 (]: 1 of 6 OK created sql incremental model _staging.stg_sales ..................... [[32mSUCCESS 720[0m in 4.57s]
[0m11:47:02.643441 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.stg_sales
[0m11:47:02.654317 [debug] [Thread-4 (]: Began running node model.sales_pipelines.processed_sales
[0m11:47:02.655668 [info ] [Thread-4 (]: 2 of 6 START sql incremental model _processed.processed_sales .................. [RUN]
[0m11:47:02.655668 [debug] [Thread-4 (]: Acquiring new snowflake connection 'model.sales_pipelines.processed_sales'
[0m11:47:02.658233 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.processed_sales
[0m11:47:02.666535 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.processed_sales"
[0m11:47:02.668988 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.processed_sales
[0m11:47:02.668988 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:47:02.668988 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: create or replace  temporary view INCREMENTALETL._processed.processed_sales__dbt_tmp
  
  
  
  
  as (
    

WITH staging AS (
    SELECT * 
    FROM INCREMENTALETL._staging.stg_sales
),

deduped AS (
    SELECT *,
        ROW_NUMBER() OVER (
            PARTITION BY invoice_no, stock_code
            ORDER BY stg_loaded_at DESC
        ) AS row_num
    FROM staging
),

cleaned AS (
    SELECT
        invoice_no,
        stock_code,
        description,
        quantity,
        unit_price,
        discount,
        shipping_cost,
        customer_id,
        invoice_date,
        invoice_year,
        invoice_month,
        country,
        payment_method,
        category,
        sales_channel,
        return_status,
        shipment_provider,
        warehouse_location,
        order_priority,

        -- Derived metrics
        ROUND(quantity * unit_price, 2) AS gross_amount,
        ROUND(quantity * unit_price * (1 - discount), 2) AS net_amount,
        ROUND(
            quantity * unit_price * (1 - discount)
            + COALESCE(shipping_cost, 0),
        2) AS total_amount,

        CASE WHEN return_status = 'RETURNED' THEN TRUE ELSE FALSE END AS is_returned,
        CASE WHEN customer_id IS NULL THEN TRUE ELSE FALSE END AS is_guest_customer,

        stg_loaded_at,
        CURRENT_TIMESTAMP() AS processed_at

    FROM deduped
    WHERE
        row_num = 1
        AND data_quality_flag = 'VALID'
        AND invoice_date IS NOT NULL
)

SELECT *
FROM cleaned


WHERE NOT EXISTS (
    SELECT 1
    FROM INCREMENTALETL._processed.processed_sales t
    WHERE t.invoice_year  = cleaned.invoice_year
      AND t.invoice_month = cleaned.invoice_month
)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m11:47:02.668988 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m11:47:05.385591 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 2.705 seconds
[0m11:47:05.412473 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:47:05.416882 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:47:05.593226 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.172 seconds
[0m11:47:05.625107 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:47:05.625107 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:47:05.843115 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.216 seconds
[0m11:47:05.873434 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:47:05.880991 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:47:06.087138 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.202 seconds
[0m11:47:06.119566 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:47:06.121116 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:47:06.327846 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.204 seconds
[0m11:47:06.357917 [debug] [Thread-4 (]: 
    In "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m11:47:06.367324 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.processed_sales"
[0m11:47:06.380257 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:47:06.383045 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m11:47:06.601971 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.216 seconds
[0m11:47:06.602870 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:47:06.610655 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: merge into INCREMENTALETL._processed.processed_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._processed.processed_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","GROSS_AMOUNT" = DBT_INTERNAL_SOURCE."GROSS_AMOUNT","NET_AMOUNT" = DBT_INTERNAL_SOURCE."NET_AMOUNT","TOTAL_AMOUNT" = DBT_INTERNAL_SOURCE."TOTAL_AMOUNT","IS_RETURNED" = DBT_INTERNAL_SOURCE."IS_RETURNED","IS_GUEST_CUSTOMER" = DBT_INTERNAL_SOURCE."IS_GUEST_CUSTOMER","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","PROCESSED_AT" = DBT_INTERNAL_SOURCE."PROCESSED_AT"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m11:47:07.526971 [debug] [Thread-4 (]: SQL status: SUCCESS 708 in 0.914 seconds
[0m11:47:07.526971 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:47:07.526971 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:47:07.863126 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.328 seconds
[0m11:47:07.873697 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._processed.processed_sales__dbt_tmp
[0m11:47:07.873697 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:47:07.873697 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: drop view if exists INCREMENTALETL._processed.processed_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:47:08.312086 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.434 seconds
[0m11:47:08.323596 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd6a55638-a1e9-42db-9c9f-a9d8cb8371aa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D898D98E80>]}
[0m11:47:08.330883 [info ] [Thread-4 (]: 2 of 6 OK created sql incremental model _processed.processed_sales ............. [[32mSUCCESS 708[0m in 5.67s]
[0m11:47:08.337202 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.processed_sales
[0m11:47:08.337202 [debug] [Thread-3 (]: Began running node model.sales_pipelines.dim_customers
[0m11:47:08.337202 [debug] [Thread-2 (]: Began running node model.sales_pipelines.dim_date
[0m11:47:08.349157 [debug] [Thread-5 (]: Began running node model.sales_pipelines.dim_products
[0m11:47:08.353215 [debug] [Thread-4 (]: Began running node model.sales_pipelines.fact_sales
[0m11:47:08.359282 [info ] [Thread-3 (]: 3 of 6 START sql table model _marts.dim_customers .............................. [RUN]
[0m11:47:08.366986 [info ] [Thread-2 (]: 4 of 6 START sql table model _marts.dim_date ................................... [RUN]
[0m11:47:08.371629 [info ] [Thread-5 (]: 5 of 6 START sql table model _marts.dim_products ............................... [RUN]
[0m11:47:08.371629 [info ] [Thread-4 (]: 6 of 6 START sql incremental model _marts.fact_sales ........................... [RUN]
[0m11:47:08.379874 [debug] [Thread-3 (]: Acquiring new snowflake connection 'model.sales_pipelines.dim_customers'
[0m11:47:08.381881 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.sales_pipelines.stg_sales, now model.sales_pipelines.dim_date)
[0m11:47:08.384920 [debug] [Thread-5 (]: Acquiring new snowflake connection 'model.sales_pipelines.dim_products'
[0m11:47:08.387949 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.sales_pipelines.processed_sales, now model.sales_pipelines.fact_sales)
[0m11:47:08.389049 [debug] [Thread-3 (]: Began compiling node model.sales_pipelines.dim_customers
[0m11:47:08.389049 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.dim_date
[0m11:47:08.396340 [debug] [Thread-5 (]: Began compiling node model.sales_pipelines.dim_products
[0m11:47:08.399469 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.fact_sales
[0m11:47:08.416246 [debug] [Thread-3 (]: Writing injected SQL for node "model.sales_pipelines.dim_customers"
[0m11:47:08.421432 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.dim_date"
[0m11:47:08.434034 [debug] [Thread-5 (]: Writing injected SQL for node "model.sales_pipelines.dim_products"
[0m11:47:08.450671 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.fact_sales"
[0m11:47:08.460940 [debug] [Thread-3 (]: Began executing node model.sales_pipelines.dim_customers
[0m11:47:08.460940 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.dim_date
[0m11:47:08.466182 [debug] [Thread-5 (]: Began executing node model.sales_pipelines.dim_products
[0m11:47:08.479856 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.fact_sales
[0m11:47:08.580808 [debug] [Thread-3 (]: Writing runtime sql for node "model.sales_pipelines.dim_customers"
[0m11:47:08.596889 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.dim_date"
[0m11:47:08.605292 [debug] [Thread-5 (]: Writing runtime sql for node "model.sales_pipelines.dim_products"
[0m11:47:08.627209 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:47:08.632697 [debug] [Thread-3 (]: Using snowflake connection "model.sales_pipelines.dim_customers"
[0m11:47:08.637260 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.dim_date"
[0m11:47:08.637260 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: create or replace  temporary view INCREMENTALETL._marts.fact_sales__dbt_tmp
  
  
  
  
  as (
    

SELECT
    invoice_no,
    stock_code,
    customer_id,
    invoice_date::DATE AS date_key,

    quantity,
    unit_price,
    discount,
    shipping_cost,

    gross_amount,
    net_amount,
    total_amount,

    is_returned,
    processed_at

FROM INCREMENTALETL._processed.processed_sales


WHERE processed_at > (SELECT MAX(processed_at) FROM INCREMENTALETL._marts.fact_sales)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */;
[0m11:47:08.640784 [debug] [Thread-5 (]: Using snowflake connection "model.sales_pipelines.dim_products"
[0m11:47:08.640784 [debug] [Thread-3 (]: On model.sales_pipelines.dim_customers: create or replace transient table INCREMENTALETL._marts.dim_customers
    
    
    
    as (

SELECT DISTINCT
    customer_id,
    country,
    payment_method,
    sales_channel,
    is_guest_customer,
    MIN(invoice_date) AS first_purchase_date,
    MAX(invoice_date) AS last_purchase_date,
    COUNT(DISTINCT invoice_no) AS total_orders
FROM INCREMENTALETL._processed.processed_sales
WHERE customer_id IS NOT NULL
GROUP BY
    customer_id,
    country,
    payment_method,
    sales_channel,
    is_guest_customer
    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.dim_customers"} */;
[0m11:47:08.640784 [debug] [Thread-2 (]: On model.sales_pipelines.dim_date: create or replace transient table INCREMENTALETL._marts.dim_date
    
    
    
    as (

SELECT DISTINCT
    invoice_date::DATE AS date_key,
    YEAR(invoice_date)  AS year,
    MONTH(invoice_date) AS month,
    DAY(invoice_date)   AS day,
    DAYNAME(invoice_date) AS day_name,
    WEEK(invoice_date)  AS week_number,
    QUARTER(invoice_date) AS quarter
FROM INCREMENTALETL._processed.processed_sales
    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.dim_date"} */;
[0m11:47:08.646032 [debug] [Thread-5 (]: On model.sales_pipelines.dim_products: create or replace transient table INCREMENTALETL._marts.dim_products
    
    
    
    as (

SELECT DISTINCT
    stock_code,
    description,
    category
FROM INCREMENTALETL._processed.processed_sales
    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.dim_products"} */;
[0m11:47:08.648930 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m11:47:08.652992 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m11:47:08.906296 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.262 seconds
[0m11:47:08.916890 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:47:08.916890 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: describe table INCREMENTALETL._marts.fact_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:47:09.099623 [debug] [Thread-4 (]: SQL status: SUCCESS 13 in 0.183 seconds
[0m11:47:09.123040 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:47:09.123040 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: describe table INCREMENTALETL._marts.fact_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:47:09.286142 [debug] [Thread-2 (]: SQL status: SUCCESS 1430 in 0.635 seconds
[0m11:47:09.293366 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd6a55638-a1e9-42db-9c9f-a9d8cb8371aa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D8ABD7B730>]}
[0m11:47:09.302688 [info ] [Thread-2 (]: 4 of 6 OK created sql table model _marts.dim_date .............................. [[32mSUCCESS 1430[0m in 0.91s]
[0m11:47:09.306290 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.dim_date
[0m11:47:09.401518 [debug] [Thread-4 (]: SQL status: SUCCESS 13 in 0.256 seconds
[0m11:47:09.417487 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:47:09.424150 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: describe table "INCREMENTALETL"."_MARTS"."FACT_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:47:09.586628 [debug] [Thread-4 (]: SQL status: SUCCESS 13 in 0.159 seconds
[0m11:47:09.592153 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.fact_sales"
[0m11:47:09.599886 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:47:09.606173 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */;
[0m11:47:09.774627 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.168 seconds
[0m11:47:09.776987 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:47:09.776987 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: merge into INCREMENTALETL._marts.fact_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._marts.fact_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","DATE_KEY" = DBT_INTERNAL_SOURCE."DATE_KEY","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","GROSS_AMOUNT" = DBT_INTERNAL_SOURCE."GROSS_AMOUNT","NET_AMOUNT" = DBT_INTERNAL_SOURCE."NET_AMOUNT","TOTAL_AMOUNT" = DBT_INTERNAL_SOURCE."TOTAL_AMOUNT","IS_RETURNED" = DBT_INTERNAL_SOURCE."IS_RETURNED","PROCESSED_AT" = DBT_INTERNAL_SOURCE."PROCESSED_AT"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "CUSTOMER_ID", "DATE_KEY", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "PROCESSED_AT")
    values
        ("INVOICE_NO", "STOCK_CODE", "CUSTOMER_ID", "DATE_KEY", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "PROCESSED_AT")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */;
[0m11:47:10.280451 [debug] [Thread-3 (]: SQL status: SUCCESS 30842 in 1.631 seconds
[0m11:47:10.290903 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd6a55638-a1e9-42db-9c9f-a9d8cb8371aa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D899618EE0>]}
[0m11:47:10.296193 [info ] [Thread-3 (]: 3 of 6 OK created sql table model _marts.dim_customers ......................... [[32mSUCCESS 30842[0m in 1.92s]
[0m11:47:10.300761 [debug] [Thread-3 (]: Finished running node model.sales_pipelines.dim_customers
[0m11:47:10.578255 [debug] [Thread-5 (]: SQL status: SUCCESS 25076 in 1.934 seconds
[0m11:47:10.578255 [debug] [Thread-4 (]: SQL status: SUCCESS 708 in 0.812 seconds
[0m11:47:10.605156 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd6a55638-a1e9-42db-9c9f-a9d8cb8371aa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D8AA745960>]}
[0m11:47:10.610099 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:47:10.616541 [info ] [Thread-5 (]: 5 of 6 OK created sql table model _marts.dim_products .......................... [[32mSUCCESS 25076[0m in 2.22s]
[0m11:47:10.618570 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:47:10.627889 [debug] [Thread-5 (]: Finished running node model.sales_pipelines.dim_products
[0m11:47:10.944579 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.317 seconds
[0m11:47:10.952154 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._marts.fact_sales__dbt_tmp
[0m11:47:10.952154 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:47:10.952154 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: drop view if exists INCREMENTALETL._marts.fact_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:47:11.201899 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.244 seconds
[0m11:47:11.222522 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd6a55638-a1e9-42db-9c9f-a9d8cb8371aa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D8ABD7A7D0>]}
[0m11:47:11.222522 [info ] [Thread-4 (]: 6 of 6 OK created sql incremental model _marts.fact_sales ...................... [[32mSUCCESS 708[0m in 2.83s]
[0m11:47:11.230717 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.fact_sales
[0m11:47:11.237082 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:47:11.237082 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m11:47:11.237082 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m11:47:11.483844 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m11:47:11.487620 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m11:47:11.800785 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m11:47:11.816293 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m11:47:12.133079 [debug] [MainThread]: Connection 'list_INCREMENTALETL__marts' was left open.
[0m11:47:12.135734 [debug] [MainThread]: On list_INCREMENTALETL__marts: Close
[0m11:47:12.374091 [debug] [MainThread]: Connection 'list_INCREMENTALETL__processed' was left open.
[0m11:47:12.379982 [debug] [MainThread]: On list_INCREMENTALETL__processed: Close
[0m11:47:12.736275 [debug] [MainThread]: Connection 'list_INCREMENTALETL__staging' was left open.
[0m11:47:12.736275 [debug] [MainThread]: On list_INCREMENTALETL__staging: Close
[0m11:47:13.061284 [debug] [MainThread]: Connection 'model.sales_pipelines.dim_date' was left open.
[0m11:47:13.066629 [debug] [MainThread]: On model.sales_pipelines.dim_date: Close
[0m11:47:13.367992 [debug] [MainThread]: Connection 'model.sales_pipelines.fact_sales' was left open.
[0m11:47:13.377527 [debug] [MainThread]: On model.sales_pipelines.fact_sales: Close
[0m11:47:13.767722 [debug] [MainThread]: Connection 'model.sales_pipelines.dim_customers' was left open.
[0m11:47:13.769746 [debug] [MainThread]: On model.sales_pipelines.dim_customers: Close
[0m11:47:14.029863 [debug] [MainThread]: Connection 'model.sales_pipelines.dim_products' was left open.
[0m11:47:14.033567 [debug] [MainThread]: On model.sales_pipelines.dim_products: Close
[0m11:47:14.283732 [info ] [MainThread]: 
[0m11:47:14.288621 [info ] [MainThread]: Finished running 3 incremental models, 3 table models in 0 hours 0 minutes and 18.65 seconds (18.65s).
[0m11:47:14.292638 [debug] [MainThread]: Command end result
[0m11:47:14.359870 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m11:47:14.359870 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m11:47:14.375552 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\snowflake-incremental-pipeline\dbt_project\target\run_results.json
[0m11:47:14.375552 [info ] [MainThread]: 
[0m11:47:14.383139 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:47:14.385151 [info ] [MainThread]: 
[0m11:47:14.387163 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=6
[0m11:47:14.390463 [debug] [MainThread]: Command `dbt run` succeeded at 11:47:14.390463 after 21.47 seconds
[0m11:47:14.391949 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D8982D36D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D8961C4610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D8961C5DB0>]}
[0m11:47:14.391949 [debug] [MainThread]: Flushing usage events
[0m11:47:15.599694 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:47:32.001999 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002508BB176D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002508DD92620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002508DD930D0>]}


============================== 11:47:32.010188 | 3c768dfd-5f22-4733-81f0-ee808578d287 ==============================
[0m11:47:32.010188 [info ] [MainThread]: Running with dbt=1.11.6
[0m11:47:32.013736 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'D:\\snowflake-incremental-pipeline\\dbt_project', 'indirect_selection': 'eager', 'invocation_command': 'dbt run', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'D:\\snowflake-incremental-pipeline\\dbt_project\\logs'}
[0m11:47:33.001361 [debug] [MainThread]: Snowflake adapter: Setting snowflake.connector to ERROR (file logging only)
[0m11:47:33.001361 [debug] [MainThread]: Snowflake adapter: Setting botocore to ERROR (file logging only)
[0m11:47:33.001361 [debug] [MainThread]: Snowflake adapter: Setting boto3 to ERROR (file logging only)
[0m11:47:33.302171 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3c768dfd-5f22-4733-81f0-ee808578d287', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002508A092B00>]}
[0m11:47:33.381651 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '3c768dfd-5f22-4733-81f0-ee808578d287', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002508D55B1C0>]}
[0m11:47:33.381651 [info ] [MainThread]: Registered adapter: snowflake=1.11.2
[0m11:47:33.899534 [debug] [MainThread]: checksum: ec27062d18352e9cfca161c166fcc73d2201661a23a3cf3c89196a27f32cfcb8, vars: {}, profile: , target: , version: 1.11.6
[0m11:47:34.133787 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:47:34.133787 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m11:47:34.133787 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:47:34.212718 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3c768dfd-5f22-4733-81f0-ee808578d287', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000250A0498A60>]}
[0m11:47:34.353391 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m11:47:34.353391 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m11:47:34.367938 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3c768dfd-5f22-4733-81f0-ee808578d287', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000250A0411600>]}
[0m11:47:34.367938 [info ] [MainThread]: Found 6 models, 6 data tests, 1 source, 522 macros
[0m11:47:34.367938 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3c768dfd-5f22-4733-81f0-ee808578d287', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000250A04989A0>]}
[0m11:47:34.383675 [info ] [MainThread]: 
[0m11:47:34.383675 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m11:47:34.383675 [info ] [MainThread]: 
[0m11:47:34.388205 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m11:47:34.398325 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m11:47:34.400108 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m11:47:34.421907 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m11:47:34.505485 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m11:47:34.505485 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m11:47:34.505485 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m11:47:34.505485 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m11:47:34.505485 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m11:47:34.505485 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m11:47:34.514080 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:47:34.514080 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:47:34.514080 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:47:36.034800 [debug] [ThreadPool]: SQL status: SUCCESS 5 in 1.519 seconds
[0m11:47:36.042520 [debug] [ThreadPool]: SQL status: SUCCESS 5 in 1.529 seconds
[0m11:47:36.042520 [debug] [ThreadPool]: SQL status: SUCCESS 5 in 1.532 seconds
[0m11:47:36.070496 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__staging'
[0m11:47:36.074572 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__marts'
[0m11:47:36.106419 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__processed'
[0m11:47:36.122191 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__staging"
[0m11:47:36.138387 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__marts"
[0m11:47:36.142134 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__processed"
[0m11:47:36.142134 [debug] [ThreadPool]: On list_INCREMENTALETL__staging: show objects in INCREMENTALETL._staging
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__staging"} */;
[0m11:47:36.152764 [debug] [ThreadPool]: On list_INCREMENTALETL__marts: show objects in INCREMENTALETL._marts
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__marts"} */;
[0m11:47:36.154524 [debug] [ThreadPool]: On list_INCREMENTALETL__processed: show objects in INCREMENTALETL._processed
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__processed"} */;
[0m11:47:36.155959 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:47:36.158011 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:47:36.158011 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:47:36.980460 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 0.822 seconds
[0m11:47:37.012552 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.856 seconds
[0m11:47:37.060386 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.910 seconds
[0m11:47:37.076233 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3c768dfd-5f22-4733-81f0-ee808578d287', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000250A04F1570>]}
[0m11:47:37.091994 [debug] [Thread-2 (]: Began running node model.sales_pipelines.stg_sales
[0m11:47:37.091994 [info ] [Thread-2 (]: 1 of 6 START sql incremental model _staging.stg_sales .......................... [RUN]
[0m11:47:37.091994 [debug] [Thread-2 (]: Acquiring new snowflake connection 'model.sales_pipelines.stg_sales'
[0m11:47:37.107974 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.stg_sales
[0m11:47:37.152457 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.stg_sales"
[0m11:47:37.154466 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.stg_sales
[0m11:47:37.235841 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:47:37.235841 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: create or replace  temporary view INCREMENTALETL._staging.stg_sales__dbt_tmp
  
  
  
  
  as (
    

WITH raw_data AS (
    SELECT * 
    FROM INCREMENTALETL.LANDINGZONE.RAW_SALES
),

staged AS (
    SELECT
        -- IDs
        CAST(INVOICENO AS VARCHAR)    AS invoice_no,
        CAST(STOCKCODE AS VARCHAR)    AS stock_code,
        CAST(DESCRIPTION AS VARCHAR)  AS description,

        -- Quantities & Prices
        TRY_CAST(QUANTITY AS INT)      AS quantity,
        TRY_CAST(UNITPRICE AS FLOAT)   AS unit_price,
        TRY_CAST(DISCOUNT AS FLOAT)    AS discount,
        TRY_CAST(SHIPPINGCOST AS FLOAT) AS shipping_cost,
        TRY_CAST(CUSTOMERID AS FLOAT)  AS customer_id,

        -- Convert once
        TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS') AS invoice_date,

        TRIM(UPPER(COUNTRY))            AS country,
        TRIM(UPPER(PAYMENTMETHOD))      AS payment_method,
        TRIM(UPPER(CATEGORY))           AS category,
        TRIM(UPPER(SALESCHANNEL))       AS sales_channel,
        TRIM(UPPER(RETURNSTATUS))       AS return_status,
        TRIM(UPPER(SHIPMENTPROVIDER))   AS shipment_provider,
        TRIM(UPPER(WAREHOUSELOCATION))  AS warehouse_location,
        TRIM(UPPER(ORDERPRIORITY))      AS order_priority,

        CURRENT_TIMESTAMP()             AS stg_loaded_at

    FROM raw_data
),

validated AS (
    SELECT
        *,
        YEAR(invoice_date)  AS invoice_year,
        MONTH(invoice_date) AS invoice_month,

        CASE
            WHEN invoice_date IS NULL           THEN 'INVALID_DATE'
            WHEN quantity IS NULL               THEN 'INVALID_QUANTITY'
            WHEN unit_price IS NULL             THEN 'INVALID_PRICE'
            WHEN shipping_cost < 0              THEN 'NEGATIVE_SHIPPING'
            WHEN discount < 0 OR discount > 1   THEN 'INVALID_DISCOUNT'
            WHEN invoice_no IS NULL             THEN 'NULL_INVOICE'
            ELSE 'VALID'
        END AS data_quality_flag
    FROM staged
)

SELECT *
FROM validated


WHERE NOT EXISTS (
    SELECT 1
    FROM INCREMENTALETL._staging.stg_sales t
    WHERE t.invoice_year  = validated.invoice_year
      AND t.invoice_month = validated.invoice_month
)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m11:47:37.235841 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m11:47:37.986112 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.753 seconds
[0m11:47:38.096926 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:47:38.111492 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:47:38.254022 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.143 seconds
[0m11:47:38.272085 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:47:38.272085 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:47:38.421204 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.140 seconds
[0m11:47:38.470826 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:47:38.470826 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:47:38.596233 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.130 seconds
[0m11:47:38.613538 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:47:38.627993 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table "INCREMENTALETL"."_STAGING"."STG_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:47:38.843578 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.216 seconds
[0m11:47:38.889667 [debug] [Thread-2 (]: 
    In "INCREMENTALETL"."_STAGING"."STG_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m11:47:38.929074 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.stg_sales"
[0m11:47:38.929074 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:47:38.929074 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m11:47:39.119592 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.189 seconds
[0m11:47:39.119592 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:47:39.130170 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: merge into INCREMENTALETL._staging.stg_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._staging.stg_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","DATA_QUALITY_FLAG" = DBT_INTERNAL_SOURCE."DATA_QUALITY_FLAG"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "INVOICE_YEAR", "INVOICE_MONTH", "DATA_QUALITY_FLAG")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "INVOICE_YEAR", "INVOICE_MONTH", "DATA_QUALITY_FLAG")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m11:47:40.283516 [debug] [Thread-2 (]: SQL status: SUCCESS 744 in 1.151 seconds
[0m11:47:40.284706 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:47:40.284706 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:47:40.526629 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.238 seconds
[0m11:47:40.538069 [debug] [Thread-2 (]: Applying DROP to: INCREMENTALETL._staging.stg_sales__dbt_tmp
[0m11:47:40.553915 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:47:40.553915 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: drop view if exists INCREMENTALETL._staging.stg_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:47:40.775654 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.207 seconds
[0m11:47:40.821328 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3c768dfd-5f22-4733-81f0-ee808578d287', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002508B969720>]}
[0m11:47:40.821328 [info ] [Thread-2 (]: 1 of 6 OK created sql incremental model _staging.stg_sales ..................... [[32mSUCCESS 744[0m in 3.73s]
[0m11:47:40.823075 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.stg_sales
[0m11:47:40.825084 [debug] [Thread-4 (]: Began running node model.sales_pipelines.processed_sales
[0m11:47:40.827090 [info ] [Thread-4 (]: 2 of 6 START sql incremental model _processed.processed_sales .................. [RUN]
[0m11:47:40.829098 [debug] [Thread-4 (]: Acquiring new snowflake connection 'model.sales_pipelines.processed_sales'
[0m11:47:40.829601 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.processed_sales
[0m11:47:40.837900 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.processed_sales"
[0m11:47:40.841127 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.processed_sales
[0m11:47:40.850504 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:47:40.850504 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: create or replace  temporary view INCREMENTALETL._processed.processed_sales__dbt_tmp
  
  
  
  
  as (
    

WITH staging AS (
    SELECT * 
    FROM INCREMENTALETL._staging.stg_sales
),

deduped AS (
    SELECT *,
        ROW_NUMBER() OVER (
            PARTITION BY invoice_no, stock_code
            ORDER BY stg_loaded_at DESC
        ) AS row_num
    FROM staging
),

cleaned AS (
    SELECT
        invoice_no,
        stock_code,
        description,
        quantity,
        unit_price,
        discount,
        shipping_cost,
        customer_id,
        invoice_date,
        invoice_year,
        invoice_month,
        country,
        payment_method,
        category,
        sales_channel,
        return_status,
        shipment_provider,
        warehouse_location,
        order_priority,

        -- Derived metrics
        ROUND(quantity * unit_price, 2) AS gross_amount,
        ROUND(quantity * unit_price * (1 - discount), 2) AS net_amount,
        ROUND(
            quantity * unit_price * (1 - discount)
            + COALESCE(shipping_cost, 0),
        2) AS total_amount,

        CASE WHEN return_status = 'RETURNED' THEN TRUE ELSE FALSE END AS is_returned,
        CASE WHEN customer_id IS NULL THEN TRUE ELSE FALSE END AS is_guest_customer,

        stg_loaded_at,
        CURRENT_TIMESTAMP() AS processed_at

    FROM deduped
    WHERE
        row_num = 1
        AND data_quality_flag = 'VALID'
        AND invoice_date IS NOT NULL
)

SELECT *
FROM cleaned


WHERE NOT EXISTS (
    SELECT 1
    FROM INCREMENTALETL._processed.processed_sales t
    WHERE t.invoice_year  = cleaned.invoice_year
      AND t.invoice_month = cleaned.invoice_month
)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m11:47:40.853178 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m11:47:41.815466 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.967 seconds
[0m11:47:41.822315 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:47:41.822315 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:47:42.030202 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.203 seconds
[0m11:47:42.061941 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:47:42.061941 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:47:42.230557 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.165 seconds
[0m11:47:42.239255 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:47:42.239255 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:47:42.622402 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.381 seconds
[0m11:47:42.638341 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:47:42.638341 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:47:42.783662 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.142 seconds
[0m11:47:42.803250 [debug] [Thread-4 (]: 
    In "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m11:47:42.808785 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.processed_sales"
[0m11:47:42.814764 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:47:42.814764 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m11:47:43.045210 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.229 seconds
[0m11:47:43.047225 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:47:43.051431 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: merge into INCREMENTALETL._processed.processed_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._processed.processed_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","GROSS_AMOUNT" = DBT_INTERNAL_SOURCE."GROSS_AMOUNT","NET_AMOUNT" = DBT_INTERNAL_SOURCE."NET_AMOUNT","TOTAL_AMOUNT" = DBT_INTERNAL_SOURCE."TOTAL_AMOUNT","IS_RETURNED" = DBT_INTERNAL_SOURCE."IS_RETURNED","IS_GUEST_CUSTOMER" = DBT_INTERNAL_SOURCE."IS_GUEST_CUSTOMER","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","PROCESSED_AT" = DBT_INTERNAL_SOURCE."PROCESSED_AT"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m11:47:43.960555 [debug] [Thread-4 (]: SQL status: SUCCESS 732 in 0.918 seconds
[0m11:47:43.960555 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:47:43.976577 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:47:44.228690 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.257 seconds
[0m11:47:44.247945 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._processed.processed_sales__dbt_tmp
[0m11:47:44.257261 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:47:44.257261 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: drop view if exists INCREMENTALETL._processed.processed_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:47:44.448764 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.193 seconds
[0m11:47:44.462009 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3c768dfd-5f22-4733-81f0-ee808578d287', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000250A0582AD0>]}
[0m11:47:44.462009 [info ] [Thread-4 (]: 2 of 6 OK created sql incremental model _processed.processed_sales ............. [[32mSUCCESS 732[0m in 3.63s]
[0m11:47:44.474207 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.processed_sales
[0m11:47:44.477985 [debug] [Thread-3 (]: Began running node model.sales_pipelines.dim_customers
[0m11:47:44.480100 [debug] [Thread-2 (]: Began running node model.sales_pipelines.dim_date
[0m11:47:44.480100 [debug] [Thread-5 (]: Began running node model.sales_pipelines.dim_products
[0m11:47:44.484923 [debug] [Thread-4 (]: Began running node model.sales_pipelines.fact_sales
[0m11:47:44.486464 [info ] [Thread-3 (]: 3 of 6 START sql table model _marts.dim_customers .............................. [RUN]
[0m11:47:44.488253 [info ] [Thread-2 (]: 4 of 6 START sql table model _marts.dim_date ................................... [RUN]
[0m11:47:44.491079 [info ] [Thread-5 (]: 5 of 6 START sql table model _marts.dim_products ............................... [RUN]
[0m11:47:44.493163 [info ] [Thread-4 (]: 6 of 6 START sql incremental model _marts.fact_sales ........................... [RUN]
[0m11:47:44.494905 [debug] [Thread-3 (]: Acquiring new snowflake connection 'model.sales_pipelines.dim_customers'
[0m11:47:44.496612 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.sales_pipelines.stg_sales, now model.sales_pipelines.dim_date)
[0m11:47:44.498918 [debug] [Thread-5 (]: Acquiring new snowflake connection 'model.sales_pipelines.dim_products'
[0m11:47:44.500152 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.sales_pipelines.processed_sales, now model.sales_pipelines.fact_sales)
[0m11:47:44.500152 [debug] [Thread-3 (]: Began compiling node model.sales_pipelines.dim_customers
[0m11:47:44.504294 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.dim_date
[0m11:47:44.504294 [debug] [Thread-5 (]: Began compiling node model.sales_pipelines.dim_products
[0m11:47:44.508129 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.fact_sales
[0m11:47:44.513602 [debug] [Thread-3 (]: Writing injected SQL for node "model.sales_pipelines.dim_customers"
[0m11:47:44.519921 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.dim_date"
[0m11:47:44.525423 [debug] [Thread-5 (]: Writing injected SQL for node "model.sales_pipelines.dim_products"
[0m11:47:44.535122 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.fact_sales"
[0m11:47:44.535122 [debug] [Thread-3 (]: Began executing node model.sales_pipelines.dim_customers
[0m11:47:44.535122 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.dim_date
[0m11:47:44.535122 [debug] [Thread-5 (]: Began executing node model.sales_pipelines.dim_products
[0m11:47:44.591206 [debug] [Thread-3 (]: Writing runtime sql for node "model.sales_pipelines.dim_customers"
[0m11:47:44.607049 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.dim_date"
[0m11:47:44.617944 [debug] [Thread-5 (]: Writing runtime sql for node "model.sales_pipelines.dim_products"
[0m11:47:44.617944 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.fact_sales
[0m11:47:44.621958 [debug] [Thread-3 (]: Using snowflake connection "model.sales_pipelines.dim_customers"
[0m11:47:44.624066 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.dim_date"
[0m11:47:44.627682 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:47:44.627682 [debug] [Thread-5 (]: Using snowflake connection "model.sales_pipelines.dim_products"
[0m11:47:44.636989 [debug] [Thread-3 (]: On model.sales_pipelines.dim_customers: create or replace transient table INCREMENTALETL._marts.dim_customers
    
    
    
    as (

SELECT DISTINCT
    customer_id,
    country,
    payment_method,
    sales_channel,
    is_guest_customer,
    MIN(invoice_date) AS first_purchase_date,
    MAX(invoice_date) AS last_purchase_date,
    COUNT(DISTINCT invoice_no) AS total_orders
FROM INCREMENTALETL._processed.processed_sales
WHERE customer_id IS NOT NULL
GROUP BY
    customer_id,
    country,
    payment_method,
    sales_channel,
    is_guest_customer
    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.dim_customers"} */;
[0m11:47:44.638735 [debug] [Thread-2 (]: On model.sales_pipelines.dim_date: create or replace transient table INCREMENTALETL._marts.dim_date
    
    
    
    as (

SELECT DISTINCT
    invoice_date::DATE AS date_key,
    YEAR(invoice_date)  AS year,
    MONTH(invoice_date) AS month,
    DAY(invoice_date)   AS day,
    DAYNAME(invoice_date) AS day_name,
    WEEK(invoice_date)  AS week_number,
    QUARTER(invoice_date) AS quarter
FROM INCREMENTALETL._processed.processed_sales
    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.dim_date"} */;
[0m11:47:44.639719 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: create or replace  temporary view INCREMENTALETL._marts.fact_sales__dbt_tmp
  
  
  
  
  as (
    

SELECT
    invoice_no,
    stock_code,
    customer_id,
    invoice_date::DATE AS date_key,

    quantity,
    unit_price,
    discount,
    shipping_cost,

    gross_amount,
    net_amount,
    total_amount,

    is_returned,
    processed_at

FROM INCREMENTALETL._processed.processed_sales


WHERE processed_at > (SELECT MAX(processed_at) FROM INCREMENTALETL._marts.fact_sales)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */;
[0m11:47:44.641568 [debug] [Thread-5 (]: On model.sales_pipelines.dim_products: create or replace transient table INCREMENTALETL._marts.dim_products
    
    
    
    as (

SELECT DISTINCT
    stock_code,
    description,
    category
FROM INCREMENTALETL._processed.processed_sales
    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.dim_products"} */;
[0m11:47:44.641568 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m11:47:44.649681 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m11:47:44.896145 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.250 seconds
[0m11:47:44.896145 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:47:44.896145 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: describe table INCREMENTALETL._marts.fact_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:47:45.065870 [debug] [Thread-4 (]: SQL status: SUCCESS 13 in 0.164 seconds
[0m11:47:45.081852 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:47:45.091202 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: describe table INCREMENTALETL._marts.fact_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:47:45.229099 [debug] [Thread-4 (]: SQL status: SUCCESS 13 in 0.140 seconds
[0m11:47:45.239569 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:47:45.255380 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: describe table "INCREMENTALETL"."_MARTS"."FACT_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:47:45.255380 [debug] [Thread-2 (]: SQL status: SUCCESS 1461 in 0.623 seconds
[0m11:47:45.271155 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3c768dfd-5f22-4733-81f0-ee808578d287', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000250A0C92F50>]}
[0m11:47:45.271155 [info ] [Thread-2 (]: 4 of 6 OK created sql table model _marts.dim_date .............................. [[32mSUCCESS 1461[0m in 0.77s]
[0m11:47:45.271155 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.dim_date
[0m11:47:45.400733 [debug] [Thread-4 (]: SQL status: SUCCESS 13 in 0.151 seconds
[0m11:47:45.418768 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.fact_sales"
[0m11:47:45.432327 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:47:45.432327 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */;
[0m11:47:45.589769 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.161 seconds
[0m11:47:45.589769 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:47:45.605729 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: merge into INCREMENTALETL._marts.fact_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._marts.fact_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","DATE_KEY" = DBT_INTERNAL_SOURCE."DATE_KEY","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","GROSS_AMOUNT" = DBT_INTERNAL_SOURCE."GROSS_AMOUNT","NET_AMOUNT" = DBT_INTERNAL_SOURCE."NET_AMOUNT","TOTAL_AMOUNT" = DBT_INTERNAL_SOURCE."TOTAL_AMOUNT","IS_RETURNED" = DBT_INTERNAL_SOURCE."IS_RETURNED","PROCESSED_AT" = DBT_INTERNAL_SOURCE."PROCESSED_AT"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "CUSTOMER_ID", "DATE_KEY", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "PROCESSED_AT")
    values
        ("INVOICE_NO", "STOCK_CODE", "CUSTOMER_ID", "DATE_KEY", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "PROCESSED_AT")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */;
[0m11:47:45.806425 [debug] [Thread-5 (]: SQL status: SUCCESS 25440 in 1.162 seconds
[0m11:47:45.822169 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3c768dfd-5f22-4733-81f0-ee808578d287', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002508CE781C0>]}
[0m11:47:45.822169 [info ] [Thread-5 (]: 5 of 6 OK created sql table model _marts.dim_products .......................... [[32mSUCCESS 25440[0m in 1.32s]
[0m11:47:45.832385 [debug] [Thread-5 (]: Finished running node model.sales_pipelines.dim_products
[0m11:47:45.941035 [debug] [Thread-3 (]: SQL status: SUCCESS 31504 in 1.298 seconds
[0m11:47:45.945442 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3c768dfd-5f22-4733-81f0-ee808578d287', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000250A04F6470>]}
[0m11:47:45.957531 [info ] [Thread-3 (]: 3 of 6 OK created sql table model _marts.dim_customers ......................... [[32mSUCCESS 31504[0m in 1.45s]
[0m11:47:45.959038 [debug] [Thread-3 (]: Finished running node model.sales_pipelines.dim_customers
[0m11:47:46.314963 [debug] [Thread-4 (]: SQL status: SUCCESS 732 in 0.710 seconds
[0m11:47:46.322583 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:47:46.322583 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:47:46.595044 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.276 seconds
[0m11:47:46.625297 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._marts.fact_sales__dbt_tmp
[0m11:47:46.625297 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:47:46.625297 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: drop view if exists INCREMENTALETL._marts.fact_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:47:46.833413 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.201 seconds
[0m11:47:46.845553 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3c768dfd-5f22-4733-81f0-ee808578d287', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000250A0E6E7D0>]}
[0m11:47:46.849346 [info ] [Thread-4 (]: 6 of 6 OK created sql incremental model _marts.fact_sales ...................... [[32mSUCCESS 732[0m in 2.34s]
[0m11:47:46.849346 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.fact_sales
[0m11:47:46.849346 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:47:46.863150 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m11:47:46.866717 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m11:47:47.309177 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m11:47:47.309177 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m11:47:52.259666 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m11:47:52.259666 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m11:47:52.660949 [debug] [MainThread]: Connection 'list_INCREMENTALETL__staging' was left open.
[0m11:47:52.676808 [debug] [MainThread]: On list_INCREMENTALETL__staging: Close
[0m11:47:53.081876 [debug] [MainThread]: Connection 'list_INCREMENTALETL__marts' was left open.
[0m11:47:53.090345 [debug] [MainThread]: On list_INCREMENTALETL__marts: Close
[0m11:47:53.494506 [debug] [MainThread]: Connection 'list_INCREMENTALETL__processed' was left open.
[0m11:47:53.494506 [debug] [MainThread]: On list_INCREMENTALETL__processed: Close
[0m11:47:53.793661 [debug] [MainThread]: Connection 'model.sales_pipelines.dim_date' was left open.
[0m11:47:53.793661 [debug] [MainThread]: On model.sales_pipelines.dim_date: Close
[0m11:47:54.044267 [debug] [MainThread]: Connection 'model.sales_pipelines.fact_sales' was left open.
[0m11:47:54.044267 [debug] [MainThread]: On model.sales_pipelines.fact_sales: Close
[0m11:47:54.455497 [debug] [MainThread]: Connection 'model.sales_pipelines.dim_customers' was left open.
[0m11:47:54.458608 [debug] [MainThread]: On model.sales_pipelines.dim_customers: Close
[0m11:47:54.720150 [debug] [MainThread]: Connection 'model.sales_pipelines.dim_products' was left open.
[0m11:47:54.720150 [debug] [MainThread]: On model.sales_pipelines.dim_products: Close
[0m11:47:54.971030 [info ] [MainThread]: 
[0m11:47:54.971030 [info ] [MainThread]: Finished running 3 incremental models, 3 table models in 0 hours 0 minutes and 20.58 seconds (20.58s).
[0m11:47:54.984896 [debug] [MainThread]: Command end result
[0m11:47:55.088213 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m11:47:55.095024 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m11:47:55.102743 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\snowflake-incremental-pipeline\dbt_project\target\run_results.json
[0m11:47:55.102743 [info ] [MainThread]: 
[0m11:47:55.111960 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:47:55.111960 [info ] [MainThread]: 
[0m11:47:55.114129 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=6
[0m11:47:55.117294 [debug] [MainThread]: Command `dbt run` succeeded at 11:47:55.114129 after 23.24 seconds
[0m11:47:55.118932 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002508BB176D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000250A04821D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000250A0DE70A0>]}
[0m11:47:55.118932 [debug] [MainThread]: Flushing usage events
[0m11:47:57.178137 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:48:14.247465 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002022D9D7700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002022FC52230>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002022FC50AF0>]}


============================== 11:48:14.257643 | 5c00902f-5a77-4f0b-bc8b-82205793b670 ==============================
[0m11:48:14.257643 [info ] [MainThread]: Running with dbt=1.11.6
[0m11:48:14.259693 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'D:\\snowflake-incremental-pipeline\\dbt_project', 'indirect_selection': 'eager', 'use_colors': 'True', 'invocation_command': 'dbt run', 'use_experimental_parser': 'False', 'log_path': 'D:\\snowflake-incremental-pipeline\\dbt_project\\logs'}
[0m11:48:15.377206 [debug] [MainThread]: Snowflake adapter: Setting snowflake.connector to ERROR (file logging only)
[0m11:48:15.379214 [debug] [MainThread]: Snowflake adapter: Setting botocore to ERROR (file logging only)
[0m11:48:15.379214 [debug] [MainThread]: Snowflake adapter: Setting boto3 to ERROR (file logging only)
[0m11:48:15.681467 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5c00902f-5a77-4f0b-bc8b-82205793b670', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002022F444A90>]}
[0m11:48:15.761298 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5c00902f-5a77-4f0b-bc8b-82205793b670', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002022FCEA8F0>]}
[0m11:48:15.761298 [info ] [MainThread]: Registered adapter: snowflake=1.11.2
[0m11:48:16.287382 [debug] [MainThread]: checksum: ec27062d18352e9cfca161c166fcc73d2201661a23a3cf3c89196a27f32cfcb8, vars: {}, profile: , target: , version: 1.11.6
[0m11:48:16.508997 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:48:16.508997 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m11:48:16.508997 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:48:16.602026 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5c00902f-5a77-4f0b-bc8b-82205793b670', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020242358A90>]}
[0m11:48:16.731458 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m11:48:16.731458 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m11:48:16.763325 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5c00902f-5a77-4f0b-bc8b-82205793b670', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020240E10190>]}
[0m11:48:16.763325 [info ] [MainThread]: Found 6 models, 6 data tests, 1 source, 522 macros
[0m11:48:16.763325 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5c00902f-5a77-4f0b-bc8b-82205793b670', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020242358370>]}
[0m11:48:16.769760 [info ] [MainThread]: 
[0m11:48:16.769760 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m11:48:16.769760 [info ] [MainThread]: 
[0m11:48:16.774057 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m11:48:16.782721 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m11:48:16.785464 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m11:48:16.808260 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m11:48:16.890363 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m11:48:16.890363 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m11:48:16.890363 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m11:48:16.890363 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m11:48:16.890363 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m11:48:16.890363 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m11:48:16.890363 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:48:16.890363 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:48:16.890363 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:48:18.107446 [debug] [ThreadPool]: SQL status: SUCCESS 5 in 1.206 seconds
[0m11:48:18.107446 [debug] [ThreadPool]: SQL status: SUCCESS 5 in 1.210 seconds
[0m11:48:18.116052 [debug] [ThreadPool]: SQL status: SUCCESS 5 in 1.213 seconds
[0m11:48:18.133514 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__marts'
[0m11:48:18.147830 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__staging'
[0m11:48:18.179974 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__processed'
[0m11:48:18.182489 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__marts"
[0m11:48:18.197495 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__staging"
[0m11:48:18.197495 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__processed"
[0m11:48:18.212003 [debug] [ThreadPool]: On list_INCREMENTALETL__marts: show objects in INCREMENTALETL._marts
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__marts"} */;
[0m11:48:18.214233 [debug] [ThreadPool]: On list_INCREMENTALETL__staging: show objects in INCREMENTALETL._staging
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__staging"} */;
[0m11:48:18.215796 [debug] [ThreadPool]: On list_INCREMENTALETL__processed: show objects in INCREMENTALETL._processed
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__processed"} */;
[0m11:48:18.218292 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:48:18.218292 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:48:18.222035 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:48:19.125181 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.909 seconds
[0m11:48:19.135407 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 0.916 seconds
[0m11:48:19.174792 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.956 seconds
[0m11:48:19.188695 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5c00902f-5a77-4f0b-bc8b-82205793b670', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020240D15DE0>]}
[0m11:48:19.204296 [debug] [Thread-2 (]: Began running node model.sales_pipelines.stg_sales
[0m11:48:19.209846 [info ] [Thread-2 (]: 1 of 6 START sql incremental model _staging.stg_sales .......................... [RUN]
[0m11:48:19.209846 [debug] [Thread-2 (]: Acquiring new snowflake connection 'model.sales_pipelines.stg_sales'
[0m11:48:19.217666 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.stg_sales
[0m11:48:19.267675 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.stg_sales"
[0m11:48:19.267675 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.stg_sales
[0m11:48:19.346539 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:48:19.346539 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: create or replace  temporary view INCREMENTALETL._staging.stg_sales__dbt_tmp
  
  
  
  
  as (
    

WITH raw_data AS (
    SELECT * 
    FROM INCREMENTALETL.LANDINGZONE.RAW_SALES
),

staged AS (
    SELECT
        -- IDs
        CAST(INVOICENO AS VARCHAR)    AS invoice_no,
        CAST(STOCKCODE AS VARCHAR)    AS stock_code,
        CAST(DESCRIPTION AS VARCHAR)  AS description,

        -- Quantities & Prices
        TRY_CAST(QUANTITY AS INT)      AS quantity,
        TRY_CAST(UNITPRICE AS FLOAT)   AS unit_price,
        TRY_CAST(DISCOUNT AS FLOAT)    AS discount,
        TRY_CAST(SHIPPINGCOST AS FLOAT) AS shipping_cost,
        TRY_CAST(CUSTOMERID AS FLOAT)  AS customer_id,

        -- Convert once
        TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS') AS invoice_date,

        TRIM(UPPER(COUNTRY))            AS country,
        TRIM(UPPER(PAYMENTMETHOD))      AS payment_method,
        TRIM(UPPER(CATEGORY))           AS category,
        TRIM(UPPER(SALESCHANNEL))       AS sales_channel,
        TRIM(UPPER(RETURNSTATUS))       AS return_status,
        TRIM(UPPER(SHIPMENTPROVIDER))   AS shipment_provider,
        TRIM(UPPER(WAREHOUSELOCATION))  AS warehouse_location,
        TRIM(UPPER(ORDERPRIORITY))      AS order_priority,

        CURRENT_TIMESTAMP()             AS stg_loaded_at

    FROM raw_data
),

validated AS (
    SELECT
        *,
        YEAR(invoice_date)  AS invoice_year,
        MONTH(invoice_date) AS invoice_month,

        CASE
            WHEN invoice_date IS NULL           THEN 'INVALID_DATE'
            WHEN quantity IS NULL               THEN 'INVALID_QUANTITY'
            WHEN unit_price IS NULL             THEN 'INVALID_PRICE'
            WHEN shipping_cost < 0              THEN 'NEGATIVE_SHIPPING'
            WHEN discount < 0 OR discount > 1   THEN 'INVALID_DISCOUNT'
            WHEN invoice_no IS NULL             THEN 'NULL_INVOICE'
            ELSE 'VALID'
        END AS data_quality_flag
    FROM staged
)

SELECT *
FROM validated


WHERE NOT EXISTS (
    SELECT 1
    FROM INCREMENTALETL._staging.stg_sales t
    WHERE t.invoice_year  = validated.invoice_year
      AND t.invoice_month = validated.invoice_month
)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m11:48:19.346539 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m11:48:20.330541 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.988 seconds
[0m11:48:20.442429 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:48:20.458579 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:48:20.635506 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.176 seconds
[0m11:48:20.651572 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:48:20.651572 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:48:20.934011 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.272 seconds
[0m11:48:20.965809 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:48:20.965809 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:48:21.127454 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.160 seconds
[0m11:48:21.145068 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:48:21.145068 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table "INCREMENTALETL"."_STAGING"."STG_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:48:21.306679 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.156 seconds
[0m11:48:21.352147 [debug] [Thread-2 (]: 
    In "INCREMENTALETL"."_STAGING"."STG_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m11:48:21.383650 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.stg_sales"
[0m11:48:21.396486 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:48:21.396486 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m11:48:21.565521 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.173 seconds
[0m11:48:21.565521 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:48:21.565521 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: merge into INCREMENTALETL._staging.stg_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._staging.stg_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","DATA_QUALITY_FLAG" = DBT_INTERNAL_SOURCE."DATA_QUALITY_FLAG"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "INVOICE_YEAR", "INVOICE_MONTH", "DATA_QUALITY_FLAG")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "INVOICE_YEAR", "INVOICE_MONTH", "DATA_QUALITY_FLAG")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m11:48:22.727723 [debug] [Thread-2 (]: SQL status: SUCCESS 744 in 1.150 seconds
[0m11:48:22.727723 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:48:22.727723 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:48:23.103247 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.369 seconds
[0m11:48:23.115325 [debug] [Thread-2 (]: Applying DROP to: INCREMENTALETL._staging.stg_sales__dbt_tmp
[0m11:48:23.115325 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:48:23.115325 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: drop view if exists INCREMENTALETL._staging.stg_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:48:23.311617 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.185 seconds
[0m11:48:23.359525 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5c00902f-5a77-4f0b-bc8b-82205793b670', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020240DE6950>]}
[0m11:48:23.359525 [info ] [Thread-2 (]: 1 of 6 OK created sql incremental model _staging.stg_sales ..................... [[32mSUCCESS 744[0m in 4.15s]
[0m11:48:23.359525 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.stg_sales
[0m11:48:23.375529 [debug] [Thread-4 (]: Began running node model.sales_pipelines.processed_sales
[0m11:48:23.376956 [info ] [Thread-4 (]: 2 of 6 START sql incremental model _processed.processed_sales .................. [RUN]
[0m11:48:23.376956 [debug] [Thread-4 (]: Acquiring new snowflake connection 'model.sales_pipelines.processed_sales'
[0m11:48:23.379591 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.processed_sales
[0m11:48:23.379591 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.processed_sales"
[0m11:48:23.379591 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.processed_sales
[0m11:48:23.399927 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:48:23.401488 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: create or replace  temporary view INCREMENTALETL._processed.processed_sales__dbt_tmp
  
  
  
  
  as (
    

WITH staging AS (
    SELECT * 
    FROM INCREMENTALETL._staging.stg_sales
),

deduped AS (
    SELECT *,
        ROW_NUMBER() OVER (
            PARTITION BY invoice_no, stock_code
            ORDER BY stg_loaded_at DESC
        ) AS row_num
    FROM staging
),

cleaned AS (
    SELECT
        invoice_no,
        stock_code,
        description,
        quantity,
        unit_price,
        discount,
        shipping_cost,
        customer_id,
        invoice_date,
        invoice_year,
        invoice_month,
        country,
        payment_method,
        category,
        sales_channel,
        return_status,
        shipment_provider,
        warehouse_location,
        order_priority,

        -- Derived metrics
        ROUND(quantity * unit_price, 2) AS gross_amount,
        ROUND(quantity * unit_price * (1 - discount), 2) AS net_amount,
        ROUND(
            quantity * unit_price * (1 - discount)
            + COALESCE(shipping_cost, 0),
        2) AS total_amount,

        CASE WHEN return_status = 'RETURNED' THEN TRUE ELSE FALSE END AS is_returned,
        CASE WHEN customer_id IS NULL THEN TRUE ELSE FALSE END AS is_guest_customer,

        stg_loaded_at,
        CURRENT_TIMESTAMP() AS processed_at

    FROM deduped
    WHERE
        row_num = 1
        AND data_quality_flag = 'VALID'
        AND invoice_date IS NOT NULL
)

SELECT *
FROM cleaned


WHERE NOT EXISTS (
    SELECT 1
    FROM INCREMENTALETL._processed.processed_sales t
    WHERE t.invoice_year  = cleaned.invoice_year
      AND t.invoice_month = cleaned.invoice_month
)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m11:48:23.401488 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m11:48:25.223965 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 1.819 seconds
[0m11:48:25.241442 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:48:25.243461 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:48:25.434308 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.192 seconds
[0m11:48:25.467783 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:48:25.467783 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:48:25.662822 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.195 seconds
[0m11:48:25.684767 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:48:25.684767 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:48:25.878473 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.184 seconds
[0m11:48:25.894559 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:48:25.894559 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:48:26.158074 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.253 seconds
[0m11:48:26.187206 [debug] [Thread-4 (]: 
    In "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m11:48:26.187206 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.processed_sales"
[0m11:48:26.197533 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:48:26.197533 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m11:48:26.408821 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.208 seconds
[0m11:48:26.408821 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:48:26.408821 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: merge into INCREMENTALETL._processed.processed_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._processed.processed_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","GROSS_AMOUNT" = DBT_INTERNAL_SOURCE."GROSS_AMOUNT","NET_AMOUNT" = DBT_INTERNAL_SOURCE."NET_AMOUNT","TOTAL_AMOUNT" = DBT_INTERNAL_SOURCE."TOTAL_AMOUNT","IS_RETURNED" = DBT_INTERNAL_SOURCE."IS_RETURNED","IS_GUEST_CUSTOMER" = DBT_INTERNAL_SOURCE."IS_GUEST_CUSTOMER","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","PROCESSED_AT" = DBT_INTERNAL_SOURCE."PROCESSED_AT"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m11:48:27.392497 [debug] [Thread-4 (]: SQL status: SUCCESS 724 in 0.981 seconds
[0m11:48:27.392497 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:48:27.392497 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:48:27.693233 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.291 seconds
[0m11:48:27.697024 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._processed.processed_sales__dbt_tmp
[0m11:48:27.712879 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:48:27.712879 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: drop view if exists INCREMENTALETL._processed.processed_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:48:28.000174 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.283 seconds
[0m11:48:28.000174 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5c00902f-5a77-4f0b-bc8b-82205793b670', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000202423ABA30>]}
[0m11:48:28.019168 [info ] [Thread-4 (]: 2 of 6 OK created sql incremental model _processed.processed_sales ............. [[32mSUCCESS 724[0m in 4.62s]
[0m11:48:28.022836 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.processed_sales
[0m11:48:28.028153 [debug] [Thread-3 (]: Began running node model.sales_pipelines.dim_customers
[0m11:48:28.028153 [debug] [Thread-2 (]: Began running node model.sales_pipelines.dim_date
[0m11:48:28.032536 [debug] [Thread-5 (]: Began running node model.sales_pipelines.dim_products
[0m11:48:28.035609 [debug] [Thread-4 (]: Began running node model.sales_pipelines.fact_sales
[0m11:48:28.040735 [info ] [Thread-3 (]: 3 of 6 START sql table model _marts.dim_customers .............................. [RUN]
[0m11:48:28.040735 [info ] [Thread-2 (]: 4 of 6 START sql table model _marts.dim_date ................................... [RUN]
[0m11:48:28.051020 [info ] [Thread-5 (]: 5 of 6 START sql table model _marts.dim_products ............................... [RUN]
[0m11:48:28.055554 [info ] [Thread-4 (]: 6 of 6 START sql incremental model _marts.fact_sales ........................... [RUN]
[0m11:48:28.060753 [debug] [Thread-3 (]: Acquiring new snowflake connection 'model.sales_pipelines.dim_customers'
[0m11:48:28.063544 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.sales_pipelines.stg_sales, now model.sales_pipelines.dim_date)
[0m11:48:28.063544 [debug] [Thread-5 (]: Acquiring new snowflake connection 'model.sales_pipelines.dim_products'
[0m11:48:28.068535 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.sales_pipelines.processed_sales, now model.sales_pipelines.fact_sales)
[0m11:48:28.068535 [debug] [Thread-3 (]: Began compiling node model.sales_pipelines.dim_customers
[0m11:48:28.068535 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.dim_date
[0m11:48:28.074931 [debug] [Thread-5 (]: Began compiling node model.sales_pipelines.dim_products
[0m11:48:28.076223 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.fact_sales
[0m11:48:28.081976 [debug] [Thread-3 (]: Writing injected SQL for node "model.sales_pipelines.dim_customers"
[0m11:48:28.088492 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.dim_date"
[0m11:48:28.094983 [debug] [Thread-5 (]: Writing injected SQL for node "model.sales_pipelines.dim_products"
[0m11:48:28.101906 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.fact_sales"
[0m11:48:28.101906 [debug] [Thread-3 (]: Began executing node model.sales_pipelines.dim_customers
[0m11:48:28.101906 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.dim_date
[0m11:48:28.111062 [debug] [Thread-5 (]: Began executing node model.sales_pipelines.dim_products
[0m11:48:28.127165 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.fact_sales
[0m11:48:28.152577 [debug] [Thread-3 (]: Writing runtime sql for node "model.sales_pipelines.dim_customers"
[0m11:48:28.152577 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.dim_date"
[0m11:48:28.159732 [debug] [Thread-5 (]: Writing runtime sql for node "model.sales_pipelines.dim_products"
[0m11:48:28.176151 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:48:28.176151 [debug] [Thread-3 (]: Using snowflake connection "model.sales_pipelines.dim_customers"
[0m11:48:28.182649 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.dim_date"
[0m11:48:28.182649 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: create or replace  temporary view INCREMENTALETL._marts.fact_sales__dbt_tmp
  
  
  
  
  as (
    

SELECT
    invoice_no,
    stock_code,
    customer_id,
    invoice_date::DATE AS date_key,

    quantity,
    unit_price,
    discount,
    shipping_cost,

    gross_amount,
    net_amount,
    total_amount,

    is_returned,
    processed_at

FROM INCREMENTALETL._processed.processed_sales


WHERE processed_at > (SELECT MAX(processed_at) FROM INCREMENTALETL._marts.fact_sales)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */;
[0m11:48:28.182649 [debug] [Thread-5 (]: Using snowflake connection "model.sales_pipelines.dim_products"
[0m11:48:28.188969 [debug] [Thread-3 (]: On model.sales_pipelines.dim_customers: create or replace transient table INCREMENTALETL._marts.dim_customers
    
    
    
    as (

SELECT DISTINCT
    customer_id,
    country,
    payment_method,
    sales_channel,
    is_guest_customer,
    MIN(invoice_date) AS first_purchase_date,
    MAX(invoice_date) AS last_purchase_date,
    COUNT(DISTINCT invoice_no) AS total_orders
FROM INCREMENTALETL._processed.processed_sales
WHERE customer_id IS NOT NULL
GROUP BY
    customer_id,
    country,
    payment_method,
    sales_channel,
    is_guest_customer
    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.dim_customers"} */;
[0m11:48:28.190910 [debug] [Thread-2 (]: On model.sales_pipelines.dim_date: create or replace transient table INCREMENTALETL._marts.dim_date
    
    
    
    as (

SELECT DISTINCT
    invoice_date::DATE AS date_key,
    YEAR(invoice_date)  AS year,
    MONTH(invoice_date) AS month,
    DAY(invoice_date)   AS day,
    DAYNAME(invoice_date) AS day_name,
    WEEK(invoice_date)  AS week_number,
    QUARTER(invoice_date) AS quarter
FROM INCREMENTALETL._processed.processed_sales
    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.dim_date"} */;
[0m11:48:28.196962 [debug] [Thread-5 (]: On model.sales_pipelines.dim_products: create or replace transient table INCREMENTALETL._marts.dim_products
    
    
    
    as (

SELECT DISTINCT
    stock_code,
    description,
    category
FROM INCREMENTALETL._processed.processed_sales
    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.dim_products"} */;
[0m11:48:28.196962 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m11:48:28.196962 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m11:48:28.464128 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.284 seconds
[0m11:48:28.479951 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:48:28.479951 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: describe table INCREMENTALETL._marts.fact_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:48:28.683330 [debug] [Thread-4 (]: SQL status: SUCCESS 13 in 0.199 seconds
[0m11:48:28.694627 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:48:28.710361 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: describe table INCREMENTALETL._marts.fact_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:48:28.877206 [debug] [Thread-2 (]: SQL status: SUCCESS 1492 in 0.677 seconds
[0m11:48:28.884983 [debug] [Thread-4 (]: SQL status: SUCCESS 13 in 0.171 seconds
[0m11:48:28.908928 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5c00902f-5a77-4f0b-bc8b-82205793b670', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020242B51FF0>]}
[0m11:48:28.924818 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:48:28.924818 [info ] [Thread-2 (]: 4 of 6 OK created sql table model _marts.dim_date .............................. [[32mSUCCESS 1492[0m in 0.85s]
[0m11:48:28.939401 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: describe table "INCREMENTALETL"."_MARTS"."FACT_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:48:28.940919 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.dim_date
[0m11:48:29.119046 [debug] [Thread-4 (]: SQL status: SUCCESS 13 in 0.183 seconds
[0m11:48:29.134859 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.fact_sales"
[0m11:48:29.150924 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:48:29.150924 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */;
[0m11:48:29.328910 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.180 seconds
[0m11:48:29.328910 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:48:29.343406 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: merge into INCREMENTALETL._marts.fact_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._marts.fact_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","DATE_KEY" = DBT_INTERNAL_SOURCE."DATE_KEY","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","GROSS_AMOUNT" = DBT_INTERNAL_SOURCE."GROSS_AMOUNT","NET_AMOUNT" = DBT_INTERNAL_SOURCE."NET_AMOUNT","TOTAL_AMOUNT" = DBT_INTERNAL_SOURCE."TOTAL_AMOUNT","IS_RETURNED" = DBT_INTERNAL_SOURCE."IS_RETURNED","PROCESSED_AT" = DBT_INTERNAL_SOURCE."PROCESSED_AT"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "CUSTOMER_ID", "DATE_KEY", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "PROCESSED_AT")
    values
        ("INVOICE_NO", "STOCK_CODE", "CUSTOMER_ID", "DATE_KEY", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "PROCESSED_AT")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */;
[0m11:48:29.534092 [debug] [Thread-3 (]: SQL status: SUCCESS 32164 in 1.337 seconds
[0m11:48:29.546792 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5c00902f-5a77-4f0b-bc8b-82205793b670', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002022FC27F70>]}
[0m11:48:29.546792 [info ] [Thread-3 (]: 3 of 6 OK created sql table model _marts.dim_customers ......................... [[32mSUCCESS 32164[0m in 1.49s]
[0m11:48:29.556937 [debug] [Thread-3 (]: Finished running node model.sales_pipelines.dim_customers
[0m11:48:29.582538 [debug] [Thread-5 (]: SQL status: SUCCESS 25842 in 1.379 seconds
[0m11:48:29.595801 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5c00902f-5a77-4f0b-bc8b-82205793b670', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020242BD75E0>]}
[0m11:48:29.597814 [info ] [Thread-5 (]: 5 of 6 OK created sql table model _marts.dim_products .......................... [[32mSUCCESS 25842[0m in 1.53s]
[0m11:48:29.610694 [debug] [Thread-5 (]: Finished running node model.sales_pipelines.dim_products
[0m11:48:30.149350 [debug] [Thread-4 (]: SQL status: SUCCESS 724 in 0.806 seconds
[0m11:48:30.149350 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:48:30.149350 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:48:30.442043 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.288 seconds
[0m11:48:30.451459 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._marts.fact_sales__dbt_tmp
[0m11:48:30.452550 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:48:30.452550 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: drop view if exists INCREMENTALETL._marts.fact_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:48:30.663063 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.211 seconds
[0m11:48:30.677068 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5c00902f-5a77-4f0b-bc8b-82205793b670', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002022F4A3D00>]}
[0m11:48:30.677068 [info ] [Thread-4 (]: 6 of 6 OK created sql incremental model _marts.fact_sales ...................... [[32mSUCCESS 724[0m in 2.61s]
[0m11:48:30.685817 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.fact_sales
[0m11:48:30.694190 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:48:30.697772 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m11:48:30.700605 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m11:48:30.949189 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m11:48:30.949189 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m11:48:31.219265 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m11:48:31.219265 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m11:48:31.493755 [debug] [MainThread]: Connection 'list_INCREMENTALETL__marts' was left open.
[0m11:48:31.497391 [debug] [MainThread]: On list_INCREMENTALETL__marts: Close
[0m11:48:31.728231 [debug] [MainThread]: Connection 'list_INCREMENTALETL__staging' was left open.
[0m11:48:31.744384 [debug] [MainThread]: On list_INCREMENTALETL__staging: Close
[0m11:48:32.102101 [debug] [MainThread]: Connection 'list_INCREMENTALETL__processed' was left open.
[0m11:48:32.102101 [debug] [MainThread]: On list_INCREMENTALETL__processed: Close
[0m11:48:32.509002 [debug] [MainThread]: Connection 'model.sales_pipelines.dim_date' was left open.
[0m11:48:32.509002 [debug] [MainThread]: On model.sales_pipelines.dim_date: Close
[0m11:48:32.807817 [debug] [MainThread]: Connection 'model.sales_pipelines.fact_sales' was left open.
[0m11:48:32.807817 [debug] [MainThread]: On model.sales_pipelines.fact_sales: Close
[0m11:48:33.118032 [debug] [MainThread]: Connection 'model.sales_pipelines.dim_customers' was left open.
[0m11:48:33.118032 [debug] [MainThread]: On model.sales_pipelines.dim_customers: Close
[0m11:48:33.408543 [debug] [MainThread]: Connection 'model.sales_pipelines.dim_products' was left open.
[0m11:48:33.409868 [debug] [MainThread]: On model.sales_pipelines.dim_products: Close
[0m11:48:33.739304 [info ] [MainThread]: 
[0m11:48:33.742370 [info ] [MainThread]: Finished running 3 incremental models, 3 table models in 0 hours 0 minutes and 16.96 seconds (16.96s).
[0m11:48:33.753252 [debug] [MainThread]: Command end result
[0m11:48:33.819056 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m11:48:33.819056 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m11:48:33.834756 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\snowflake-incremental-pipeline\dbt_project\target\run_results.json
[0m11:48:33.835824 [info ] [MainThread]: 
[0m11:48:33.835824 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:48:33.837425 [info ] [MainThread]: 
[0m11:48:33.837425 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=6
[0m11:48:33.837425 [debug] [MainThread]: Command `dbt run` succeeded at 11:48:33.837425 after 19.72 seconds
[0m11:48:33.840764 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002022D9D7700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020242342140>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020242B3DAB0>]}
[0m11:48:33.841813 [debug] [MainThread]: Flushing usage events
[0m11:48:35.262216 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:48:57.042367 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A32D4C76A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A32F741000>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A32F7423B0>]}


============================== 11:48:57.042367 | 4d59562c-3c11-47e3-b112-414b3cb0f44d ==============================
[0m11:48:57.042367 [info ] [MainThread]: Running with dbt=1.11.6
[0m11:48:57.042367 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'profiles_dir': 'D:\\snowflake-incremental-pipeline\\dbt_project', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'indirect_selection': 'eager', 'invocation_command': 'dbt run', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'D:\\snowflake-incremental-pipeline\\dbt_project\\logs'}
[0m11:48:58.076331 [debug] [MainThread]: Snowflake adapter: Setting snowflake.connector to ERROR (file logging only)
[0m11:48:58.076331 [debug] [MainThread]: Snowflake adapter: Setting botocore to ERROR (file logging only)
[0m11:48:58.076331 [debug] [MainThread]: Snowflake adapter: Setting boto3 to ERROR (file logging only)
[0m11:48:58.394660 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4d59562c-3c11-47e3-b112-414b3cb0f44d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A32F7852A0>]}
[0m11:48:58.482715 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4d59562c-3c11-47e3-b112-414b3cb0f44d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A32F7A94B0>]}
[0m11:48:58.482715 [info ] [MainThread]: Registered adapter: snowflake=1.11.2
[0m11:48:59.032852 [debug] [MainThread]: checksum: ec27062d18352e9cfca161c166fcc73d2201661a23a3cf3c89196a27f32cfcb8, vars: {}, profile: , target: , version: 1.11.6
[0m11:48:59.269626 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:48:59.269626 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m11:48:59.269626 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:48:59.349198 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4d59562c-3c11-47e3-b112-414b3cb0f44d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A341E4CA30>]}
[0m11:48:59.493411 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m11:48:59.493411 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m11:48:59.517749 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4d59562c-3c11-47e3-b112-414b3cb0f44d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A341E4CE20>]}
[0m11:48:59.517749 [info ] [MainThread]: Found 6 models, 6 data tests, 1 source, 522 macros
[0m11:48:59.517749 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4d59562c-3c11-47e3-b112-414b3cb0f44d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A341E4D480>]}
[0m11:48:59.517749 [info ] [MainThread]: 
[0m11:48:59.517749 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m11:48:59.517749 [info ] [MainThread]: 
[0m11:48:59.530151 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m11:48:59.542249 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m11:48:59.546766 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m11:48:59.566414 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m11:48:59.654554 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m11:48:59.654554 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m11:48:59.654554 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m11:48:59.654554 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m11:48:59.654554 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m11:48:59.654554 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m11:48:59.654554 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:48:59.670406 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:48:59.671414 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:49:00.897299 [debug] [ThreadPool]: SQL status: SUCCESS 5 in 1.235 seconds
[0m11:49:00.916234 [debug] [ThreadPool]: SQL status: SUCCESS 5 in 1.247 seconds
[0m11:49:01.650261 [debug] [ThreadPool]: SQL status: SUCCESS 5 in 1.982 seconds
[0m11:49:01.666268 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__staging'
[0m11:49:01.682478 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__processed'
[0m11:49:01.714112 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__marts'
[0m11:49:01.717645 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__staging"
[0m11:49:01.732214 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__processed"
[0m11:49:01.732214 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__marts"
[0m11:49:01.745996 [debug] [ThreadPool]: On list_INCREMENTALETL__staging: show objects in INCREMENTALETL._staging
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__staging"} */;
[0m11:49:01.745996 [debug] [ThreadPool]: On list_INCREMENTALETL__processed: show objects in INCREMENTALETL._processed
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__processed"} */;
[0m11:49:01.750153 [debug] [ThreadPool]: On list_INCREMENTALETL__marts: show objects in INCREMENTALETL._marts
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__marts"} */;
[0m11:49:01.750153 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:49:01.753715 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:49:01.753715 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:49:02.572301 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 0.821 seconds
[0m11:49:02.648756 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.898 seconds
[0m11:49:02.839657 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 1.088 seconds
[0m11:49:02.845823 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4d59562c-3c11-47e3-b112-414b3cb0f44d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A341F336D0>]}
[0m11:49:02.861641 [debug] [Thread-2 (]: Began running node model.sales_pipelines.stg_sales
[0m11:49:02.873709 [info ] [Thread-2 (]: 1 of 6 START sql incremental model _staging.stg_sales .......................... [RUN]
[0m11:49:02.878135 [debug] [Thread-2 (]: Acquiring new snowflake connection 'model.sales_pipelines.stg_sales'
[0m11:49:02.878135 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.stg_sales
[0m11:49:02.923824 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.stg_sales"
[0m11:49:02.923824 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.stg_sales
[0m11:49:03.005756 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:49:03.005756 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: create or replace  temporary view INCREMENTALETL._staging.stg_sales__dbt_tmp
  
  
  
  
  as (
    

WITH raw_data AS (
    SELECT * 
    FROM INCREMENTALETL.LANDINGZONE.RAW_SALES
),

staged AS (
    SELECT
        -- IDs
        CAST(INVOICENO AS VARCHAR)    AS invoice_no,
        CAST(STOCKCODE AS VARCHAR)    AS stock_code,
        CAST(DESCRIPTION AS VARCHAR)  AS description,

        -- Quantities & Prices
        TRY_CAST(QUANTITY AS INT)      AS quantity,
        TRY_CAST(UNITPRICE AS FLOAT)   AS unit_price,
        TRY_CAST(DISCOUNT AS FLOAT)    AS discount,
        TRY_CAST(SHIPPINGCOST AS FLOAT) AS shipping_cost,
        TRY_CAST(CUSTOMERID AS FLOAT)  AS customer_id,

        -- Convert once
        TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS') AS invoice_date,

        TRIM(UPPER(COUNTRY))            AS country,
        TRIM(UPPER(PAYMENTMETHOD))      AS payment_method,
        TRIM(UPPER(CATEGORY))           AS category,
        TRIM(UPPER(SALESCHANNEL))       AS sales_channel,
        TRIM(UPPER(RETURNSTATUS))       AS return_status,
        TRIM(UPPER(SHIPMENTPROVIDER))   AS shipment_provider,
        TRIM(UPPER(WAREHOUSELOCATION))  AS warehouse_location,
        TRIM(UPPER(ORDERPRIORITY))      AS order_priority,

        CURRENT_TIMESTAMP()             AS stg_loaded_at

    FROM raw_data
),

validated AS (
    SELECT
        *,
        YEAR(invoice_date)  AS invoice_year,
        MONTH(invoice_date) AS invoice_month,

        CASE
            WHEN invoice_date IS NULL           THEN 'INVALID_DATE'
            WHEN quantity IS NULL               THEN 'INVALID_QUANTITY'
            WHEN unit_price IS NULL             THEN 'INVALID_PRICE'
            WHEN shipping_cost < 0              THEN 'NEGATIVE_SHIPPING'
            WHEN discount < 0 OR discount > 1   THEN 'INVALID_DISCOUNT'
            WHEN invoice_no IS NULL             THEN 'NULL_INVOICE'
            ELSE 'VALID'
        END AS data_quality_flag
    FROM staged
)

SELECT *
FROM validated


WHERE NOT EXISTS (
    SELECT 1
    FROM INCREMENTALETL._staging.stg_sales t
    WHERE t.invoice_year  = validated.invoice_year
      AND t.invoice_month = validated.invoice_month
)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m11:49:03.005756 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m11:49:04.142317 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 1.146 seconds
[0m11:49:04.268953 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:49:04.268953 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:49:04.446690 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.181 seconds
[0m11:49:04.478460 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:49:04.478460 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:49:04.626676 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.148 seconds
[0m11:49:04.670717 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:49:04.670717 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:49:04.850428 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.184 seconds
[0m11:49:04.882278 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:49:04.882278 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table "INCREMENTALETL"."_STAGING"."STG_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:49:05.082869 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.195 seconds
[0m11:49:05.125032 [debug] [Thread-2 (]: 
    In "INCREMENTALETL"."_STAGING"."STG_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m11:49:05.153800 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.stg_sales"
[0m11:49:05.165340 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:49:05.165340 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m11:49:05.367370 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.213 seconds
[0m11:49:05.383297 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:49:05.383297 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: merge into INCREMENTALETL._staging.stg_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._staging.stg_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","DATA_QUALITY_FLAG" = DBT_INTERNAL_SOURCE."DATA_QUALITY_FLAG"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "INVOICE_YEAR", "INVOICE_MONTH", "DATA_QUALITY_FLAG")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "INVOICE_YEAR", "INVOICE_MONTH", "DATA_QUALITY_FLAG")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m11:49:06.613550 [debug] [Thread-2 (]: SQL status: SUCCESS 696 in 1.223 seconds
[0m11:49:06.613550 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:49:06.616367 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:49:06.896799 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.279 seconds
[0m11:49:06.936094 [debug] [Thread-2 (]: Applying DROP to: INCREMENTALETL._staging.stg_sales__dbt_tmp
[0m11:49:06.968146 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:49:06.968146 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: drop view if exists INCREMENTALETL._staging.stg_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:49:07.639683 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.668 seconds
[0m11:49:07.695333 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4d59562c-3c11-47e3-b112-414b3cb0f44d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A3426CD270>]}
[0m11:49:07.697340 [info ] [Thread-2 (]: 1 of 6 OK created sql incremental model _staging.stg_sales ..................... [[32mSUCCESS 696[0m in 4.82s]
[0m11:49:07.697340 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.stg_sales
[0m11:49:07.700766 [debug] [Thread-4 (]: Began running node model.sales_pipelines.processed_sales
[0m11:49:07.700766 [info ] [Thread-4 (]: 2 of 6 START sql incremental model _processed.processed_sales .................. [RUN]
[0m11:49:07.703429 [debug] [Thread-4 (]: Acquiring new snowflake connection 'model.sales_pipelines.processed_sales'
[0m11:49:07.704918 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.processed_sales
[0m11:49:07.711446 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.processed_sales"
[0m11:49:07.711446 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.processed_sales
[0m11:49:07.723831 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:49:07.723831 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: create or replace  temporary view INCREMENTALETL._processed.processed_sales__dbt_tmp
  
  
  
  
  as (
    

WITH staging AS (
    SELECT * 
    FROM INCREMENTALETL._staging.stg_sales
),

deduped AS (
    SELECT *,
        ROW_NUMBER() OVER (
            PARTITION BY invoice_no, stock_code
            ORDER BY stg_loaded_at DESC
        ) AS row_num
    FROM staging
),

cleaned AS (
    SELECT
        invoice_no,
        stock_code,
        description,
        quantity,
        unit_price,
        discount,
        shipping_cost,
        customer_id,
        invoice_date,
        invoice_year,
        invoice_month,
        country,
        payment_method,
        category,
        sales_channel,
        return_status,
        shipment_provider,
        warehouse_location,
        order_priority,

        -- Derived metrics
        ROUND(quantity * unit_price, 2) AS gross_amount,
        ROUND(quantity * unit_price * (1 - discount), 2) AS net_amount,
        ROUND(
            quantity * unit_price * (1 - discount)
            + COALESCE(shipping_cost, 0),
        2) AS total_amount,

        CASE WHEN return_status = 'RETURNED' THEN TRUE ELSE FALSE END AS is_returned,
        CASE WHEN customer_id IS NULL THEN TRUE ELSE FALSE END AS is_guest_customer,

        stg_loaded_at,
        CURRENT_TIMESTAMP() AS processed_at

    FROM deduped
    WHERE
        row_num = 1
        AND data_quality_flag = 'VALID'
        AND invoice_date IS NOT NULL
)

SELECT *
FROM cleaned


WHERE NOT EXISTS (
    SELECT 1
    FROM INCREMENTALETL._processed.processed_sales t
    WHERE t.invoice_year  = cleaned.invoice_year
      AND t.invoice_month = cleaned.invoice_month
)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m11:49:07.723831 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m11:49:08.860290 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 1.137 seconds
[0m11:49:08.872397 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:49:08.872397 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:49:09.069701 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.192 seconds
[0m11:49:09.100633 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:49:09.100633 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:49:09.261483 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.167 seconds
[0m11:49:09.295120 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:49:09.295120 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:49:09.483122 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.184 seconds
[0m11:49:09.487215 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:49:09.487215 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:49:09.664638 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.168 seconds
[0m11:49:09.682364 [debug] [Thread-4 (]: 
    In "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m11:49:09.696416 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.processed_sales"
[0m11:49:09.696416 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:49:09.696416 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m11:49:09.903024 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.199 seconds
[0m11:49:09.903024 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:49:09.903024 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: merge into INCREMENTALETL._processed.processed_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._processed.processed_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","GROSS_AMOUNT" = DBT_INTERNAL_SOURCE."GROSS_AMOUNT","NET_AMOUNT" = DBT_INTERNAL_SOURCE."NET_AMOUNT","TOTAL_AMOUNT" = DBT_INTERNAL_SOURCE."TOTAL_AMOUNT","IS_RETURNED" = DBT_INTERNAL_SOURCE."IS_RETURNED","IS_GUEST_CUSTOMER" = DBT_INTERNAL_SOURCE."IS_GUEST_CUSTOMER","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","PROCESSED_AT" = DBT_INTERNAL_SOURCE."PROCESSED_AT"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m11:49:10.894864 [debug] [Thread-4 (]: SQL status: SUCCESS 676 in 0.990 seconds
[0m11:49:10.908251 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:49:10.908251 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:49:11.305219 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.402 seconds
[0m11:49:11.317384 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._processed.processed_sales__dbt_tmp
[0m11:49:11.319391 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:49:11.319391 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: drop view if exists INCREMENTALETL._processed.processed_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:49:11.527953 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.206 seconds
[0m11:49:11.541318 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4d59562c-3c11-47e3-b112-414b3cb0f44d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A34077E7D0>]}
[0m11:49:11.542862 [info ] [Thread-4 (]: 2 of 6 OK created sql incremental model _processed.processed_sales ............. [[32mSUCCESS 676[0m in 3.83s]
[0m11:49:11.549365 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.processed_sales
[0m11:49:11.555848 [debug] [Thread-3 (]: Began running node model.sales_pipelines.dim_customers
[0m11:49:11.558607 [debug] [Thread-2 (]: Began running node model.sales_pipelines.dim_date
[0m11:49:11.559631 [debug] [Thread-5 (]: Began running node model.sales_pipelines.dim_products
[0m11:49:11.559631 [debug] [Thread-4 (]: Began running node model.sales_pipelines.fact_sales
[0m11:49:11.567247 [info ] [Thread-3 (]: 3 of 6 START sql table model _marts.dim_customers .............................. [RUN]
[0m11:49:11.571953 [info ] [Thread-2 (]: 4 of 6 START sql table model _marts.dim_date ................................... [RUN]
[0m11:49:11.577964 [info ] [Thread-5 (]: 5 of 6 START sql table model _marts.dim_products ............................... [RUN]
[0m11:49:11.583505 [info ] [Thread-4 (]: 6 of 6 START sql incremental model _marts.fact_sales ........................... [RUN]
[0m11:49:11.588062 [debug] [Thread-3 (]: Acquiring new snowflake connection 'model.sales_pipelines.dim_customers'
[0m11:49:11.591925 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.sales_pipelines.stg_sales, now model.sales_pipelines.dim_date)
[0m11:49:11.599483 [debug] [Thread-5 (]: Acquiring new snowflake connection 'model.sales_pipelines.dim_products'
[0m11:49:11.604143 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.sales_pipelines.processed_sales, now model.sales_pipelines.fact_sales)
[0m11:49:11.604143 [debug] [Thread-3 (]: Began compiling node model.sales_pipelines.dim_customers
[0m11:49:11.604143 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.dim_date
[0m11:49:11.604143 [debug] [Thread-5 (]: Began compiling node model.sales_pipelines.dim_products
[0m11:49:11.604143 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.fact_sales
[0m11:49:11.637336 [debug] [Thread-3 (]: Writing injected SQL for node "model.sales_pipelines.dim_customers"
[0m11:49:11.661185 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.dim_date"
[0m11:49:11.675501 [debug] [Thread-5 (]: Writing injected SQL for node "model.sales_pipelines.dim_products"
[0m11:49:11.695348 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.fact_sales"
[0m11:49:11.703294 [debug] [Thread-3 (]: Began executing node model.sales_pipelines.dim_customers
[0m11:49:11.710611 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.dim_date
[0m11:49:11.710611 [debug] [Thread-5 (]: Began executing node model.sales_pipelines.dim_products
[0m11:49:11.732245 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.fact_sales
[0m11:49:11.927392 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.dim_date"
[0m11:49:11.939196 [debug] [Thread-3 (]: Writing runtime sql for node "model.sales_pipelines.dim_customers"
[0m11:49:11.939196 [debug] [Thread-5 (]: Writing runtime sql for node "model.sales_pipelines.dim_products"
[0m11:49:11.949713 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:49:11.952154 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.dim_date"
[0m11:49:11.953222 [debug] [Thread-3 (]: Using snowflake connection "model.sales_pipelines.dim_customers"
[0m11:49:11.955392 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: create or replace  temporary view INCREMENTALETL._marts.fact_sales__dbt_tmp
  
  
  
  
  as (
    

SELECT
    invoice_no,
    stock_code,
    customer_id,
    invoice_date::DATE AS date_key,

    quantity,
    unit_price,
    discount,
    shipping_cost,

    gross_amount,
    net_amount,
    total_amount,

    is_returned,
    processed_at

FROM INCREMENTALETL._processed.processed_sales


WHERE processed_at > (SELECT MAX(processed_at) FROM INCREMENTALETL._marts.fact_sales)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */;
[0m11:49:11.955392 [debug] [Thread-5 (]: Using snowflake connection "model.sales_pipelines.dim_products"
[0m11:49:11.958105 [debug] [Thread-2 (]: On model.sales_pipelines.dim_date: create or replace transient table INCREMENTALETL._marts.dim_date
    
    
    
    as (

SELECT DISTINCT
    invoice_date::DATE AS date_key,
    YEAR(invoice_date)  AS year,
    MONTH(invoice_date) AS month,
    DAY(invoice_date)   AS day,
    DAYNAME(invoice_date) AS day_name,
    WEEK(invoice_date)  AS week_number,
    QUARTER(invoice_date) AS quarter
FROM INCREMENTALETL._processed.processed_sales
    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.dim_date"} */;
[0m11:49:11.959021 [debug] [Thread-3 (]: On model.sales_pipelines.dim_customers: create or replace transient table INCREMENTALETL._marts.dim_customers
    
    
    
    as (

SELECT DISTINCT
    customer_id,
    country,
    payment_method,
    sales_channel,
    is_guest_customer,
    MIN(invoice_date) AS first_purchase_date,
    MAX(invoice_date) AS last_purchase_date,
    COUNT(DISTINCT invoice_no) AS total_orders
FROM INCREMENTALETL._processed.processed_sales
WHERE customer_id IS NOT NULL
GROUP BY
    customer_id,
    country,
    payment_method,
    sales_channel,
    is_guest_customer
    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.dim_customers"} */;
[0m11:49:11.960334 [debug] [Thread-5 (]: On model.sales_pipelines.dim_products: create or replace transient table INCREMENTALETL._marts.dim_products
    
    
    
    as (

SELECT DISTINCT
    stock_code,
    description,
    category
FROM INCREMENTALETL._processed.processed_sales
    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.dim_products"} */;
[0m11:49:11.964343 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m11:49:11.966295 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m11:49:12.203919 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.247 seconds
[0m11:49:12.211053 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:49:12.219072 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: describe table INCREMENTALETL._marts.fact_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:49:12.382162 [debug] [Thread-4 (]: SQL status: SUCCESS 13 in 0.165 seconds
[0m11:49:12.407259 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:49:12.409274 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: describe table INCREMENTALETL._marts.fact_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:49:12.564437 [debug] [Thread-4 (]: SQL status: SUCCESS 13 in 0.161 seconds
[0m11:49:12.580170 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:49:12.596360 [debug] [Thread-2 (]: SQL status: SUCCESS 1521 in 0.639 seconds
[0m11:49:12.596360 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: describe table "INCREMENTALETL"."_MARTS"."FACT_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:49:12.627931 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4d59562c-3c11-47e3-b112-414b3cb0f44d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A3426359F0>]}
[0m11:49:12.627931 [info ] [Thread-2 (]: 4 of 6 OK created sql table model _marts.dim_date .............................. [[32mSUCCESS 1521[0m in 1.04s]
[0m11:49:12.627931 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.dim_date
[0m11:49:12.801887 [debug] [Thread-4 (]: SQL status: SUCCESS 13 in 0.170 seconds
[0m11:49:12.823865 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.fact_sales"
[0m11:49:12.833701 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:49:12.833701 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */;
[0m11:49:13.057184 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.225 seconds
[0m11:49:13.057184 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:49:13.057184 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: merge into INCREMENTALETL._marts.fact_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._marts.fact_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","DATE_KEY" = DBT_INTERNAL_SOURCE."DATE_KEY","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","GROSS_AMOUNT" = DBT_INTERNAL_SOURCE."GROSS_AMOUNT","NET_AMOUNT" = DBT_INTERNAL_SOURCE."NET_AMOUNT","TOTAL_AMOUNT" = DBT_INTERNAL_SOURCE."TOTAL_AMOUNT","IS_RETURNED" = DBT_INTERNAL_SOURCE."IS_RETURNED","PROCESSED_AT" = DBT_INTERNAL_SOURCE."PROCESSED_AT"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "CUSTOMER_ID", "DATE_KEY", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "PROCESSED_AT")
    values
        ("INVOICE_NO", "STOCK_CODE", "CUSTOMER_ID", "DATE_KEY", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "PROCESSED_AT")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */;
[0m11:49:13.123935 [debug] [Thread-5 (]: SQL status: SUCCESS 26216 in 1.162 seconds
[0m11:49:13.139473 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4d59562c-3c11-47e3-b112-414b3cb0f44d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A342739FC0>]}
[0m11:49:13.139473 [info ] [Thread-5 (]: 5 of 6 OK created sql table model _marts.dim_products .......................... [[32mSUCCESS 26216[0m in 1.54s]
[0m11:49:13.149612 [debug] [Thread-5 (]: Finished running node model.sales_pipelines.dim_products
[0m11:49:13.213711 [debug] [Thread-3 (]: SQL status: SUCCESS 32779 in 1.248 seconds
[0m11:49:13.220650 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4d59562c-3c11-47e3-b112-414b3cb0f44d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A3427AF7F0>]}
[0m11:49:13.220650 [info ] [Thread-3 (]: 3 of 6 OK created sql table model _marts.dim_customers ......................... [[32mSUCCESS 32779[0m in 1.64s]
[0m11:49:13.234939 [debug] [Thread-3 (]: Finished running node model.sales_pipelines.dim_customers
[0m11:49:14.172199 [debug] [Thread-4 (]: SQL status: SUCCESS 676 in 1.110 seconds
[0m11:49:14.182928 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:49:14.182928 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:49:14.444357 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.269 seconds
[0m11:49:14.460312 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._marts.fact_sales__dbt_tmp
[0m11:49:14.476465 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:49:14.476465 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: drop view if exists INCREMENTALETL._marts.fact_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:49:14.668839 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.193 seconds
[0m11:49:14.685007 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4d59562c-3c11-47e3-b112-414b3cb0f44d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A32F7DB0D0>]}
[0m11:49:14.685007 [info ] [Thread-4 (]: 6 of 6 OK created sql incremental model _marts.fact_sales ...................... [[32mSUCCESS 676[0m in 3.08s]
[0m11:49:14.690711 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.fact_sales
[0m11:49:14.690711 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:49:14.690711 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m11:49:14.690711 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m11:49:15.014434 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m11:49:15.014434 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m11:49:15.305444 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m11:49:15.305444 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m11:49:15.611229 [debug] [MainThread]: Connection 'list_INCREMENTALETL__staging' was left open.
[0m11:49:15.611229 [debug] [MainThread]: On list_INCREMENTALETL__staging: Close
[0m11:49:15.925141 [debug] [MainThread]: Connection 'list_INCREMENTALETL__processed' was left open.
[0m11:49:15.925141 [debug] [MainThread]: On list_INCREMENTALETL__processed: Close
[0m11:49:16.239785 [debug] [MainThread]: Connection 'list_INCREMENTALETL__marts' was left open.
[0m11:49:16.239785 [debug] [MainThread]: On list_INCREMENTALETL__marts: Close
[0m11:49:16.482053 [debug] [MainThread]: Connection 'model.sales_pipelines.dim_date' was left open.
[0m11:49:16.482053 [debug] [MainThread]: On model.sales_pipelines.dim_date: Close
[0m11:49:16.835384 [debug] [MainThread]: Connection 'model.sales_pipelines.fact_sales' was left open.
[0m11:49:16.851448 [debug] [MainThread]: On model.sales_pipelines.fact_sales: Close
[0m11:49:17.145778 [debug] [MainThread]: Connection 'model.sales_pipelines.dim_customers' was left open.
[0m11:49:17.145778 [debug] [MainThread]: On model.sales_pipelines.dim_customers: Close
[0m11:49:17.389429 [debug] [MainThread]: Connection 'model.sales_pipelines.dim_products' was left open.
[0m11:49:17.389429 [debug] [MainThread]: On model.sales_pipelines.dim_products: Close
[0m11:49:17.643165 [info ] [MainThread]: 
[0m11:49:17.643165 [info ] [MainThread]: Finished running 3 incremental models, 3 table models in 0 hours 0 minutes and 18.11 seconds (18.11s).
[0m11:49:17.661669 [debug] [MainThread]: Command end result
[0m11:49:17.708062 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m11:49:17.711211 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m11:49:17.719482 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\snowflake-incremental-pipeline\dbt_project\target\run_results.json
[0m11:49:17.719482 [info ] [MainThread]: 
[0m11:49:17.727124 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:49:17.727124 [info ] [MainThread]: 
[0m11:49:17.729205 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=6
[0m11:49:17.729205 [debug] [MainThread]: Command `dbt run` succeeded at 11:49:17.729205 after 20.82 seconds
[0m11:49:17.729205 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A32D4C76A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A341E320B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A32CEBF6D0>]}
[0m11:49:17.729205 [debug] [MainThread]: Flushing usage events
[0m11:49:19.201550 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:49:36.028276 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000250E7F876A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000250E9231000>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000250E92323B0>]}


============================== 11:49:36.045575 | 0287cde7-12cc-415c-95b3-19f035814598 ==============================
[0m11:49:36.045575 [info ] [MainThread]: Running with dbt=1.11.6
[0m11:49:36.045575 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'profiles_dir': 'D:\\snowflake-incremental-pipeline\\dbt_project', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'indirect_selection': 'eager', 'invocation_command': 'dbt run', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'D:\\snowflake-incremental-pipeline\\dbt_project\\logs'}
[0m11:49:37.122564 [debug] [MainThread]: Snowflake adapter: Setting snowflake.connector to ERROR (file logging only)
[0m11:49:37.122564 [debug] [MainThread]: Snowflake adapter: Setting botocore to ERROR (file logging only)
[0m11:49:37.122564 [debug] [MainThread]: Snowflake adapter: Setting boto3 to ERROR (file logging only)
[0m11:49:37.491008 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0287cde7-12cc-415c-95b3-19f035814598', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000250E92752A0>]}
[0m11:49:37.562999 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '0287cde7-12cc-415c-95b3-19f035814598', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000250E92954B0>]}
[0m11:49:37.562999 [info ] [MainThread]: Registered adapter: snowflake=1.11.2
[0m11:49:38.140771 [debug] [MainThread]: checksum: ec27062d18352e9cfca161c166fcc73d2201661a23a3cf3c89196a27f32cfcb8, vars: {}, profile: , target: , version: 1.11.6
[0m11:49:38.399299 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:49:38.399299 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m11:49:38.403302 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:49:38.486582 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0287cde7-12cc-415c-95b3-19f035814598', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000250FB938A30>]}
[0m11:49:38.669909 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m11:49:38.669909 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m11:49:38.686690 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0287cde7-12cc-415c-95b3-19f035814598', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000250FB938E20>]}
[0m11:49:38.686690 [info ] [MainThread]: Found 6 models, 6 data tests, 1 source, 522 macros
[0m11:49:38.686690 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0287cde7-12cc-415c-95b3-19f035814598', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000250FB939480>]}
[0m11:49:38.702626 [info ] [MainThread]: 
[0m11:49:38.704295 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m11:49:38.705170 [info ] [MainThread]: 
[0m11:49:38.706822 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m11:49:38.710215 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m11:49:38.719446 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m11:49:38.736157 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m11:49:38.841886 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m11:49:38.841886 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m11:49:38.841886 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m11:49:38.850031 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m11:49:38.850031 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m11:49:38.850031 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m11:49:38.852894 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:49:38.852894 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:49:38.854998 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:49:40.091042 [debug] [ThreadPool]: SQL status: SUCCESS 5 in 1.238 seconds
[0m11:49:40.091042 [debug] [ThreadPool]: SQL status: SUCCESS 5 in 1.245 seconds
[0m11:49:40.104779 [debug] [ThreadPool]: SQL status: SUCCESS 5 in 1.252 seconds
[0m11:49:40.115361 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__marts'
[0m11:49:40.133462 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__staging'
[0m11:49:40.147979 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__processed'
[0m11:49:40.211596 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__marts"
[0m11:49:40.211596 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__staging"
[0m11:49:40.227434 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__processed"
[0m11:49:40.227434 [debug] [ThreadPool]: On list_INCREMENTALETL__marts: show objects in INCREMENTALETL._marts
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__marts"} */;
[0m11:49:40.243158 [debug] [ThreadPool]: On list_INCREMENTALETL__staging: show objects in INCREMENTALETL._staging
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__staging"} */;
[0m11:49:40.243158 [debug] [ThreadPool]: On list_INCREMENTALETL__processed: show objects in INCREMENTALETL._processed
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__processed"} */;
[0m11:49:40.243158 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:49:40.243158 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:49:40.243158 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:49:41.131564 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.880 seconds
[0m11:49:41.205180 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.951 seconds
[0m11:49:41.208735 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 0.960 seconds
[0m11:49:41.222947 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0287cde7-12cc-415c-95b3-19f035814598', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000250FB97C970>]}
[0m11:49:41.237421 [debug] [Thread-2 (]: Began running node model.sales_pipelines.stg_sales
[0m11:49:41.239378 [info ] [Thread-2 (]: 1 of 6 START sql incremental model _staging.stg_sales .......................... [RUN]
[0m11:49:41.243149 [debug] [Thread-2 (]: Acquiring new snowflake connection 'model.sales_pipelines.stg_sales'
[0m11:49:41.245164 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.stg_sales
[0m11:49:41.273714 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.stg_sales"
[0m11:49:41.273714 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.stg_sales
[0m11:49:41.376787 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:49:41.376787 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: create or replace  temporary view INCREMENTALETL._staging.stg_sales__dbt_tmp
  
  
  
  
  as (
    

WITH raw_data AS (
    SELECT * 
    FROM INCREMENTALETL.LANDINGZONE.RAW_SALES
),

staged AS (
    SELECT
        -- IDs
        CAST(INVOICENO AS VARCHAR)    AS invoice_no,
        CAST(STOCKCODE AS VARCHAR)    AS stock_code,
        CAST(DESCRIPTION AS VARCHAR)  AS description,

        -- Quantities & Prices
        TRY_CAST(QUANTITY AS INT)      AS quantity,
        TRY_CAST(UNITPRICE AS FLOAT)   AS unit_price,
        TRY_CAST(DISCOUNT AS FLOAT)    AS discount,
        TRY_CAST(SHIPPINGCOST AS FLOAT) AS shipping_cost,
        TRY_CAST(CUSTOMERID AS FLOAT)  AS customer_id,

        -- Convert once
        TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS') AS invoice_date,

        TRIM(UPPER(COUNTRY))            AS country,
        TRIM(UPPER(PAYMENTMETHOD))      AS payment_method,
        TRIM(UPPER(CATEGORY))           AS category,
        TRIM(UPPER(SALESCHANNEL))       AS sales_channel,
        TRIM(UPPER(RETURNSTATUS))       AS return_status,
        TRIM(UPPER(SHIPMENTPROVIDER))   AS shipment_provider,
        TRIM(UPPER(WAREHOUSELOCATION))  AS warehouse_location,
        TRIM(UPPER(ORDERPRIORITY))      AS order_priority,

        CURRENT_TIMESTAMP()             AS stg_loaded_at

    FROM raw_data
),

validated AS (
    SELECT
        *,
        YEAR(invoice_date)  AS invoice_year,
        MONTH(invoice_date) AS invoice_month,

        CASE
            WHEN invoice_date IS NULL           THEN 'INVALID_DATE'
            WHEN quantity IS NULL               THEN 'INVALID_QUANTITY'
            WHEN unit_price IS NULL             THEN 'INVALID_PRICE'
            WHEN shipping_cost < 0              THEN 'NEGATIVE_SHIPPING'
            WHEN discount < 0 OR discount > 1   THEN 'INVALID_DISCOUNT'
            WHEN invoice_no IS NULL             THEN 'NULL_INVOICE'
            ELSE 'VALID'
        END AS data_quality_flag
    FROM staged
)

SELECT *
FROM validated


WHERE NOT EXISTS (
    SELECT 1
    FROM INCREMENTALETL._staging.stg_sales t
    WHERE t.invoice_year  = validated.invoice_year
      AND t.invoice_month = validated.invoice_month
)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m11:49:41.376787 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m11:49:42.462458 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 1.078 seconds
[0m11:49:42.533994 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:49:42.539140 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:49:42.749129 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.206 seconds
[0m11:49:42.797178 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:49:42.812317 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:49:43.012710 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.197 seconds
[0m11:49:43.126709 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:49:43.126709 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:49:43.371959 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.243 seconds
[0m11:49:43.395950 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:49:43.395950 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table "INCREMENTALETL"."_STAGING"."STG_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:49:43.541223 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.131 seconds
[0m11:49:43.573003 [debug] [Thread-2 (]: 
    In "INCREMENTALETL"."_STAGING"."STG_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m11:49:43.620347 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.stg_sales"
[0m11:49:43.620347 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:49:43.620347 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m11:49:43.780171 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.151 seconds
[0m11:49:43.780171 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:49:43.780171 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: merge into INCREMENTALETL._staging.stg_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._staging.stg_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","DATA_QUALITY_FLAG" = DBT_INTERNAL_SOURCE."DATA_QUALITY_FLAG"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "INVOICE_YEAR", "INVOICE_MONTH", "DATA_QUALITY_FLAG")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "INVOICE_YEAR", "INVOICE_MONTH", "DATA_QUALITY_FLAG")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m11:49:45.003766 [debug] [Thread-2 (]: SQL status: SUCCESS 744 in 1.219 seconds
[0m11:49:45.003766 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:49:45.003766 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:49:45.307995 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.297 seconds
[0m11:49:45.339876 [debug] [Thread-2 (]: Applying DROP to: INCREMENTALETL._staging.stg_sales__dbt_tmp
[0m11:49:45.339876 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:49:45.339876 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: drop view if exists INCREMENTALETL._staging.stg_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:49:45.522871 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.167 seconds
[0m11:49:45.569850 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0287cde7-12cc-415c-95b3-19f035814598', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000250E92C8EE0>]}
[0m11:49:45.571667 [info ] [Thread-2 (]: 1 of 6 OK created sql incremental model _staging.stg_sales ..................... [[32mSUCCESS 744[0m in 4.33s]
[0m11:49:45.571667 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.stg_sales
[0m11:49:45.576463 [debug] [Thread-4 (]: Began running node model.sales_pipelines.processed_sales
[0m11:49:45.576463 [info ] [Thread-4 (]: 2 of 6 START sql incremental model _processed.processed_sales .................. [RUN]
[0m11:49:45.576463 [debug] [Thread-4 (]: Acquiring new snowflake connection 'model.sales_pipelines.processed_sales'
[0m11:49:45.580954 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.processed_sales
[0m11:49:45.589339 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.processed_sales"
[0m11:49:45.589339 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.processed_sales
[0m11:49:45.609196 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:49:45.609196 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: create or replace  temporary view INCREMENTALETL._processed.processed_sales__dbt_tmp
  
  
  
  
  as (
    

WITH staging AS (
    SELECT * 
    FROM INCREMENTALETL._staging.stg_sales
),

deduped AS (
    SELECT *,
        ROW_NUMBER() OVER (
            PARTITION BY invoice_no, stock_code
            ORDER BY stg_loaded_at DESC
        ) AS row_num
    FROM staging
),

cleaned AS (
    SELECT
        invoice_no,
        stock_code,
        description,
        quantity,
        unit_price,
        discount,
        shipping_cost,
        customer_id,
        invoice_date,
        invoice_year,
        invoice_month,
        country,
        payment_method,
        category,
        sales_channel,
        return_status,
        shipment_provider,
        warehouse_location,
        order_priority,

        -- Derived metrics
        ROUND(quantity * unit_price, 2) AS gross_amount,
        ROUND(quantity * unit_price * (1 - discount), 2) AS net_amount,
        ROUND(
            quantity * unit_price * (1 - discount)
            + COALESCE(shipping_cost, 0),
        2) AS total_amount,

        CASE WHEN return_status = 'RETURNED' THEN TRUE ELSE FALSE END AS is_returned,
        CASE WHEN customer_id IS NULL THEN TRUE ELSE FALSE END AS is_guest_customer,

        stg_loaded_at,
        CURRENT_TIMESTAMP() AS processed_at

    FROM deduped
    WHERE
        row_num = 1
        AND data_quality_flag = 'VALID'
        AND invoice_date IS NOT NULL
)

SELECT *
FROM cleaned


WHERE NOT EXISTS (
    SELECT 1
    FROM INCREMENTALETL._processed.processed_sales t
    WHERE t.invoice_year  = cleaned.invoice_year
      AND t.invoice_month = cleaned.invoice_month
)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m11:49:45.611310 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m11:49:46.437678 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.829 seconds
[0m11:49:46.437678 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:49:46.453886 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:49:46.604531 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.147 seconds
[0m11:49:46.624563 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:49:46.624563 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:49:46.754238 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.127 seconds
[0m11:49:46.771255 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:49:46.773525 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:49:46.908761 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.132 seconds
[0m11:49:46.935032 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:49:46.937042 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:49:47.068173 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.131 seconds
[0m11:49:47.101979 [debug] [Thread-4 (]: 
    In "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m11:49:47.101979 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.processed_sales"
[0m11:49:47.114167 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:49:47.115906 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m11:49:47.278564 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.166 seconds
[0m11:49:47.278564 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:49:47.286565 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: merge into INCREMENTALETL._processed.processed_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._processed.processed_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","GROSS_AMOUNT" = DBT_INTERNAL_SOURCE."GROSS_AMOUNT","NET_AMOUNT" = DBT_INTERNAL_SOURCE."NET_AMOUNT","TOTAL_AMOUNT" = DBT_INTERNAL_SOURCE."TOTAL_AMOUNT","IS_RETURNED" = DBT_INTERNAL_SOURCE."IS_RETURNED","IS_GUEST_CUSTOMER" = DBT_INTERNAL_SOURCE."IS_GUEST_CUSTOMER","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","PROCESSED_AT" = DBT_INTERNAL_SOURCE."PROCESSED_AT"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m11:49:48.277424 [debug] [Thread-4 (]: SQL status: SUCCESS 723 in 1.002 seconds
[0m11:49:48.293368 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:49:48.293368 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:49:48.563810 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.270 seconds
[0m11:49:48.572425 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._processed.processed_sales__dbt_tmp
[0m11:49:48.585828 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:49:48.588504 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: drop view if exists INCREMENTALETL._processed.processed_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:49:48.772094 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.179 seconds
[0m11:49:48.786309 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0287cde7-12cc-415c-95b3-19f035814598', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000250FB97E2F0>]}
[0m11:49:48.786309 [info ] [Thread-4 (]: 2 of 6 OK created sql incremental model _processed.processed_sales ............. [[32mSUCCESS 723[0m in 3.21s]
[0m11:49:48.786309 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.processed_sales
[0m11:49:48.786309 [debug] [Thread-3 (]: Began running node model.sales_pipelines.dim_customers
[0m11:49:48.796710 [debug] [Thread-2 (]: Began running node model.sales_pipelines.dim_date
[0m11:49:48.797939 [debug] [Thread-5 (]: Began running node model.sales_pipelines.dim_products
[0m11:49:48.799850 [debug] [Thread-4 (]: Began running node model.sales_pipelines.fact_sales
[0m11:49:48.797939 [info ] [Thread-3 (]: 3 of 6 START sql table model _marts.dim_customers .............................. [RUN]
[0m11:49:48.801846 [info ] [Thread-2 (]: 4 of 6 START sql table model _marts.dim_date ................................... [RUN]
[0m11:49:48.804242 [info ] [Thread-5 (]: 5 of 6 START sql table model _marts.dim_products ............................... [RUN]
[0m11:49:48.804242 [info ] [Thread-4 (]: 6 of 6 START sql incremental model _marts.fact_sales ........................... [RUN]
[0m11:49:48.805950 [debug] [Thread-3 (]: Acquiring new snowflake connection 'model.sales_pipelines.dim_customers'
[0m11:49:48.808456 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.sales_pipelines.stg_sales, now model.sales_pipelines.dim_date)
[0m11:49:48.810389 [debug] [Thread-5 (]: Acquiring new snowflake connection 'model.sales_pipelines.dim_products'
[0m11:49:48.812405 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.sales_pipelines.processed_sales, now model.sales_pipelines.fact_sales)
[0m11:49:48.812405 [debug] [Thread-3 (]: Began compiling node model.sales_pipelines.dim_customers
[0m11:49:48.812405 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.dim_date
[0m11:49:48.812405 [debug] [Thread-5 (]: Began compiling node model.sales_pipelines.dim_products
[0m11:49:48.812405 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.fact_sales
[0m11:49:48.824911 [debug] [Thread-3 (]: Writing injected SQL for node "model.sales_pipelines.dim_customers"
[0m11:49:48.828757 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.dim_date"
[0m11:49:48.836917 [debug] [Thread-5 (]: Writing injected SQL for node "model.sales_pipelines.dim_products"
[0m11:49:48.841925 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.fact_sales"
[0m11:49:48.848954 [debug] [Thread-3 (]: Began executing node model.sales_pipelines.dim_customers
[0m11:49:48.848954 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.dim_date
[0m11:49:48.853010 [debug] [Thread-5 (]: Began executing node model.sales_pipelines.dim_products
[0m11:49:48.880739 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.fact_sales
[0m11:49:48.896730 [debug] [Thread-3 (]: Writing runtime sql for node "model.sales_pipelines.dim_customers"
[0m11:49:48.903098 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.dim_date"
[0m11:49:48.906552 [debug] [Thread-5 (]: Writing runtime sql for node "model.sales_pipelines.dim_products"
[0m11:49:48.912661 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:49:48.919648 [debug] [Thread-3 (]: Using snowflake connection "model.sales_pipelines.dim_customers"
[0m11:49:48.920687 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: create or replace  temporary view INCREMENTALETL._marts.fact_sales__dbt_tmp
  
  
  
  
  as (
    

SELECT
    invoice_no,
    stock_code,
    customer_id,
    invoice_date::DATE AS date_key,

    quantity,
    unit_price,
    discount,
    shipping_cost,

    gross_amount,
    net_amount,
    total_amount,

    is_returned,
    processed_at

FROM INCREMENTALETL._processed.processed_sales


WHERE processed_at > (SELECT MAX(processed_at) FROM INCREMENTALETL._marts.fact_sales)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */;
[0m11:49:48.923279 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.dim_date"
[0m11:49:48.923279 [debug] [Thread-5 (]: Using snowflake connection "model.sales_pipelines.dim_products"
[0m11:49:48.923279 [debug] [Thread-3 (]: On model.sales_pipelines.dim_customers: create or replace transient table INCREMENTALETL._marts.dim_customers
    
    
    
    as (

SELECT DISTINCT
    customer_id,
    country,
    payment_method,
    sales_channel,
    is_guest_customer,
    MIN(invoice_date) AS first_purchase_date,
    MAX(invoice_date) AS last_purchase_date,
    COUNT(DISTINCT invoice_no) AS total_orders
FROM INCREMENTALETL._processed.processed_sales
WHERE customer_id IS NOT NULL
GROUP BY
    customer_id,
    country,
    payment_method,
    sales_channel,
    is_guest_customer
    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.dim_customers"} */;
[0m11:49:48.928377 [debug] [Thread-2 (]: On model.sales_pipelines.dim_date: create or replace transient table INCREMENTALETL._marts.dim_date
    
    
    
    as (

SELECT DISTINCT
    invoice_date::DATE AS date_key,
    YEAR(invoice_date)  AS year,
    MONTH(invoice_date) AS month,
    DAY(invoice_date)   AS day,
    DAYNAME(invoice_date) AS day_name,
    WEEK(invoice_date)  AS week_number,
    QUARTER(invoice_date) AS quarter
FROM INCREMENTALETL._processed.processed_sales
    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.dim_date"} */;
[0m11:49:48.928377 [debug] [Thread-5 (]: On model.sales_pipelines.dim_products: create or replace transient table INCREMENTALETL._marts.dim_products
    
    
    
    as (

SELECT DISTINCT
    stock_code,
    description,
    category
FROM INCREMENTALETL._processed.processed_sales
    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.dim_products"} */;
[0m11:49:48.930944 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m11:49:48.930944 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m11:49:49.160064 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.234 seconds
[0m11:49:49.163820 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:49:49.165829 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: describe table INCREMENTALETL._marts.fact_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:49:49.386806 [debug] [Thread-4 (]: SQL status: SUCCESS 13 in 0.219 seconds
[0m11:49:49.409604 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:49:49.409604 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: describe table INCREMENTALETL._marts.fact_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:49:49.528434 [debug] [Thread-2 (]: SQL status: SUCCESS 1552 in 0.598 seconds
[0m11:49:49.553177 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0287cde7-12cc-415c-95b3-19f035814598', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000250FC00C700>]}
[0m11:49:49.556203 [debug] [Thread-4 (]: SQL status: SUCCESS 13 in 0.143 seconds
[0m11:49:49.558727 [info ] [Thread-2 (]: 4 of 6 OK created sql table model _marts.dim_date .............................. [[32mSUCCESS 1552[0m in 0.74s]
[0m11:49:49.570901 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:49:49.575834 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.dim_date
[0m11:49:49.577458 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: describe table "INCREMENTALETL"."_MARTS"."FACT_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:49:49.711190 [debug] [Thread-4 (]: SQL status: SUCCESS 13 in 0.129 seconds
[0m11:49:49.727201 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.fact_sales"
[0m11:49:49.727201 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:49:49.742998 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */;
[0m11:49:49.899735 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.155 seconds
[0m11:49:49.902187 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:49:49.904791 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: merge into INCREMENTALETL._marts.fact_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._marts.fact_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","DATE_KEY" = DBT_INTERNAL_SOURCE."DATE_KEY","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","GROSS_AMOUNT" = DBT_INTERNAL_SOURCE."GROSS_AMOUNT","NET_AMOUNT" = DBT_INTERNAL_SOURCE."NET_AMOUNT","TOTAL_AMOUNT" = DBT_INTERNAL_SOURCE."TOTAL_AMOUNT","IS_RETURNED" = DBT_INTERNAL_SOURCE."IS_RETURNED","PROCESSED_AT" = DBT_INTERNAL_SOURCE."PROCESSED_AT"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "CUSTOMER_ID", "DATE_KEY", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "PROCESSED_AT")
    values
        ("INVOICE_NO", "STOCK_CODE", "CUSTOMER_ID", "DATE_KEY", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "PROCESSED_AT")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */;
[0m11:49:50.633488 [debug] [Thread-4 (]: SQL status: SUCCESS 723 in 0.726 seconds
[0m11:49:50.638267 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:49:50.641367 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:49:50.940998 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.300 seconds
[0m11:49:50.948160 [debug] [Thread-5 (]: SQL status: SUCCESS 26571 in 2.017 seconds
[0m11:49:50.965741 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._marts.fact_sales__dbt_tmp
[0m11:49:50.981331 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0287cde7-12cc-415c-95b3-19f035814598', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000250FC28AAA0>]}
[0m11:49:50.988171 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:49:50.991935 [info ] [Thread-5 (]: 5 of 6 OK created sql table model _marts.dim_products .......................... [[32mSUCCESS 26571[0m in 2.17s]
[0m11:49:50.995674 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: drop view if exists INCREMENTALETL._marts.fact_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:49:50.999222 [debug] [Thread-5 (]: Finished running node model.sales_pipelines.dim_products
[0m11:49:51.059031 [debug] [Thread-3 (]: SQL status: SUCCESS 33451 in 2.128 seconds
[0m11:49:51.059031 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0287cde7-12cc-415c-95b3-19f035814598', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000250FC000B20>]}
[0m11:49:51.059031 [info ] [Thread-3 (]: 3 of 6 OK created sql table model _marts.dim_customers ......................... [[32mSUCCESS 33451[0m in 2.25s]
[0m11:49:51.070924 [debug] [Thread-3 (]: Finished running node model.sales_pipelines.dim_customers
[0m11:49:51.163281 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.166 seconds
[0m11:49:51.171034 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0287cde7-12cc-415c-95b3-19f035814598', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000250FC00D5D0>]}
[0m11:49:51.178126 [info ] [Thread-4 (]: 6 of 6 OK created sql incremental model _marts.fact_sales ...................... [[32mSUCCESS 723[0m in 2.36s]
[0m11:49:51.178126 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.fact_sales
[0m11:49:51.189340 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:49:51.189340 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m11:49:51.189340 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m11:49:51.468676 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m11:49:51.472731 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m11:49:51.871373 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m11:49:51.871373 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m11:49:52.192920 [debug] [MainThread]: Connection 'list_INCREMENTALETL__marts' was left open.
[0m11:49:52.192920 [debug] [MainThread]: On list_INCREMENTALETL__marts: Close
[0m11:49:52.484580 [debug] [MainThread]: Connection 'list_INCREMENTALETL__staging' was left open.
[0m11:49:52.487934 [debug] [MainThread]: On list_INCREMENTALETL__staging: Close
[0m11:49:52.794271 [debug] [MainThread]: Connection 'list_INCREMENTALETL__processed' was left open.
[0m11:49:52.796298 [debug] [MainThread]: On list_INCREMENTALETL__processed: Close
[0m11:49:53.102693 [debug] [MainThread]: Connection 'model.sales_pipelines.dim_date' was left open.
[0m11:49:53.105933 [debug] [MainThread]: On model.sales_pipelines.dim_date: Close
[0m11:49:53.350802 [debug] [MainThread]: Connection 'model.sales_pipelines.fact_sales' was left open.
[0m11:49:53.352829 [debug] [MainThread]: On model.sales_pipelines.fact_sales: Close
[0m11:49:53.605366 [debug] [MainThread]: Connection 'model.sales_pipelines.dim_customers' was left open.
[0m11:49:53.607390 [debug] [MainThread]: On model.sales_pipelines.dim_customers: Close
[0m11:49:53.895676 [debug] [MainThread]: Connection 'model.sales_pipelines.dim_products' was left open.
[0m11:49:53.895676 [debug] [MainThread]: On model.sales_pipelines.dim_products: Close
[0m11:49:54.380099 [info ] [MainThread]: 
[0m11:49:54.383641 [info ] [MainThread]: Finished running 3 incremental models, 3 table models in 0 hours 0 minutes and 15.67 seconds (15.67s).
[0m11:49:54.387993 [debug] [MainThread]: Command end result
[0m11:49:54.453626 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m11:49:54.453626 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m11:49:54.471683 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\snowflake-incremental-pipeline\dbt_project\target\run_results.json
[0m11:49:54.471683 [info ] [MainThread]: 
[0m11:49:54.471683 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:49:54.478667 [info ] [MainThread]: 
[0m11:49:54.479772 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=6
[0m11:49:54.480978 [debug] [MainThread]: Command `dbt run` succeeded at 11:49:54.480978 after 18.57 seconds
[0m11:49:54.482209 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000250E7F876A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000250E69DF6D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000250E92954B0>]}
[0m11:49:54.483258 [debug] [MainThread]: Flushing usage events
[0m11:49:55.874254 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:50:18.300331 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020A6D1A7670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020A6E457340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020A6E4569B0>]}


============================== 11:50:18.300331 | 144044ed-a2b1-4724-920c-d18de3ffb45b ==============================
[0m11:50:18.300331 [info ] [MainThread]: Running with dbt=1.11.6
[0m11:50:18.312347 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'profiles_dir': 'D:\\snowflake-incremental-pipeline\\dbt_project', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'invocation_command': 'dbt run', 'indirect_selection': 'eager', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'D:\\snowflake-incremental-pipeline\\dbt_project\\logs'}
[0m11:50:19.267831 [debug] [MainThread]: Snowflake adapter: Setting snowflake.connector to ERROR (file logging only)
[0m11:50:19.267831 [debug] [MainThread]: Snowflake adapter: Setting botocore to ERROR (file logging only)
[0m11:50:19.267831 [debug] [MainThread]: Snowflake adapter: Setting boto3 to ERROR (file logging only)
[0m11:50:19.564786 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '144044ed-a2b1-4724-920c-d18de3ffb45b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020A7F5E11B0>]}
[0m11:50:19.644537 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '144044ed-a2b1-4724-920c-d18de3ffb45b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020A6E2AE140>]}
[0m11:50:19.644537 [info ] [MainThread]: Registered adapter: snowflake=1.11.2
[0m11:50:20.215339 [debug] [MainThread]: checksum: ec27062d18352e9cfca161c166fcc73d2201661a23a3cf3c89196a27f32cfcb8, vars: {}, profile: , target: , version: 1.11.6
[0m11:50:20.438068 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:50:20.440074 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m11:50:20.440074 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:50:20.509750 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '144044ed-a2b1-4724-920c-d18de3ffb45b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020A7FB8CA00>]}
[0m11:50:20.661196 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m11:50:20.661196 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m11:50:20.677151 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '144044ed-a2b1-4724-920c-d18de3ffb45b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020A7F92B8B0>]}
[0m11:50:20.677151 [info ] [MainThread]: Found 6 models, 6 data tests, 1 source, 522 macros
[0m11:50:20.677151 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '144044ed-a2b1-4724-920c-d18de3ffb45b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020A7FB8C400>]}
[0m11:50:20.677151 [info ] [MainThread]: 
[0m11:50:20.677151 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m11:50:20.693003 [info ] [MainThread]: 
[0m11:50:20.696486 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m11:50:20.705663 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m11:50:20.716553 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m11:50:20.733791 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m11:50:20.804530 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m11:50:20.820219 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m11:50:20.821268 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m11:50:20.821268 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m11:50:20.821268 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m11:50:20.823110 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m11:50:20.823842 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:50:20.824425 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:50:20.824425 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:50:21.938539 [debug] [ThreadPool]: SQL status: SUCCESS 5 in 1.113 seconds
[0m11:50:21.954494 [debug] [ThreadPool]: SQL status: SUCCESS 5 in 1.134 seconds
[0m11:50:21.970557 [debug] [ThreadPool]: SQL status: SUCCESS 5 in 1.147 seconds
[0m11:50:21.978653 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__processed'
[0m11:50:22.002466 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__marts'
[0m11:50:22.019666 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__processed"
[0m11:50:22.034228 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__marts"
[0m11:50:22.034228 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__staging'
[0m11:50:22.034228 [debug] [ThreadPool]: On list_INCREMENTALETL__processed: show objects in INCREMENTALETL._processed
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__processed"} */;
[0m11:50:22.034228 [debug] [ThreadPool]: On list_INCREMENTALETL__marts: show objects in INCREMENTALETL._marts
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__marts"} */;
[0m11:50:22.041732 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__staging"
[0m11:50:22.041732 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:50:22.041732 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:50:22.041732 [debug] [ThreadPool]: On list_INCREMENTALETL__staging: show objects in INCREMENTALETL._staging
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__staging"} */;
[0m11:50:22.051491 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:50:22.812874 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.764 seconds
[0m11:50:22.900775 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.862 seconds
[0m11:50:22.938515 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 0.893 seconds
[0m11:50:22.941618 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '144044ed-a2b1-4724-920c-d18de3ffb45b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020A7FC730A0>]}
[0m11:50:22.957449 [debug] [Thread-2 (]: Began running node model.sales_pipelines.stg_sales
[0m11:50:22.973488 [info ] [Thread-2 (]: 1 of 6 START sql incremental model _staging.stg_sales .......................... [RUN]
[0m11:50:22.973488 [debug] [Thread-2 (]: Acquiring new snowflake connection 'model.sales_pipelines.stg_sales'
[0m11:50:22.973488 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.stg_sales
[0m11:50:23.020997 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.stg_sales"
[0m11:50:23.020997 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.stg_sales
[0m11:50:23.094632 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:50:23.094632 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: create or replace  temporary view INCREMENTALETL._staging.stg_sales__dbt_tmp
  
  
  
  
  as (
    

WITH raw_data AS (
    SELECT * 
    FROM INCREMENTALETL.LANDINGZONE.RAW_SALES
),

staged AS (
    SELECT
        -- IDs
        CAST(INVOICENO AS VARCHAR)    AS invoice_no,
        CAST(STOCKCODE AS VARCHAR)    AS stock_code,
        CAST(DESCRIPTION AS VARCHAR)  AS description,

        -- Quantities & Prices
        TRY_CAST(QUANTITY AS INT)      AS quantity,
        TRY_CAST(UNITPRICE AS FLOAT)   AS unit_price,
        TRY_CAST(DISCOUNT AS FLOAT)    AS discount,
        TRY_CAST(SHIPPINGCOST AS FLOAT) AS shipping_cost,
        TRY_CAST(CUSTOMERID AS FLOAT)  AS customer_id,

        -- Convert once
        TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS') AS invoice_date,

        TRIM(UPPER(COUNTRY))            AS country,
        TRIM(UPPER(PAYMENTMETHOD))      AS payment_method,
        TRIM(UPPER(CATEGORY))           AS category,
        TRIM(UPPER(SALESCHANNEL))       AS sales_channel,
        TRIM(UPPER(RETURNSTATUS))       AS return_status,
        TRIM(UPPER(SHIPMENTPROVIDER))   AS shipment_provider,
        TRIM(UPPER(WAREHOUSELOCATION))  AS warehouse_location,
        TRIM(UPPER(ORDERPRIORITY))      AS order_priority,

        CURRENT_TIMESTAMP()             AS stg_loaded_at

    FROM raw_data
),

validated AS (
    SELECT
        *,
        YEAR(invoice_date)  AS invoice_year,
        MONTH(invoice_date) AS invoice_month,

        CASE
            WHEN invoice_date IS NULL           THEN 'INVALID_DATE'
            WHEN quantity IS NULL               THEN 'INVALID_QUANTITY'
            WHEN unit_price IS NULL             THEN 'INVALID_PRICE'
            WHEN shipping_cost < 0              THEN 'NEGATIVE_SHIPPING'
            WHEN discount < 0 OR discount > 1   THEN 'INVALID_DISCOUNT'
            WHEN invoice_no IS NULL             THEN 'NULL_INVOICE'
            ELSE 'VALID'
        END AS data_quality_flag
    FROM staged
)

SELECT *
FROM validated


WHERE NOT EXISTS (
    SELECT 1
    FROM INCREMENTALETL._staging.stg_sales t
    WHERE t.invoice_year  = validated.invoice_year
      AND t.invoice_month = validated.invoice_month
)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m11:50:23.094632 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m11:50:24.095437 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.990 seconds
[0m11:50:24.210545 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:50:24.220656 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:50:24.430224 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.213 seconds
[0m11:50:24.446367 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:50:24.446367 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:50:24.625691 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.170 seconds
[0m11:50:24.673778 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:50:24.673778 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:50:24.812272 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.141 seconds
[0m11:50:24.833655 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:50:24.844270 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table "INCREMENTALETL"."_STAGING"."STG_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:50:25.152088 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.304 seconds
[0m11:50:25.172196 [debug] [Thread-2 (]: 
    In "INCREMENTALETL"."_STAGING"."STG_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m11:50:25.215497 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.stg_sales"
[0m11:50:25.221890 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:50:25.221890 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m11:50:25.382673 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.158 seconds
[0m11:50:25.384799 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:50:25.385771 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: merge into INCREMENTALETL._staging.stg_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._staging.stg_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","DATA_QUALITY_FLAG" = DBT_INTERNAL_SOURCE."DATA_QUALITY_FLAG"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "INVOICE_YEAR", "INVOICE_MONTH", "DATA_QUALITY_FLAG")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "INVOICE_YEAR", "INVOICE_MONTH", "DATA_QUALITY_FLAG")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m11:50:26.369526 [debug] [Thread-2 (]: SQL status: SUCCESS 720 in 0.990 seconds
[0m11:50:26.369526 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:50:26.369526 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:50:26.670731 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.300 seconds
[0m11:50:26.694758 [debug] [Thread-2 (]: Applying DROP to: INCREMENTALETL._staging.stg_sales__dbt_tmp
[0m11:50:26.703701 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:50:26.703701 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: drop view if exists INCREMENTALETL._staging.stg_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:50:26.889024 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.187 seconds
[0m11:50:26.918570 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '144044ed-a2b1-4724-920c-d18de3ffb45b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020A01411180>]}
[0m11:50:26.918570 [info ] [Thread-2 (]: 1 of 6 OK created sql incremental model _staging.stg_sales ..................... [[32mSUCCESS 720[0m in 3.95s]
[0m11:50:26.918570 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.stg_sales
[0m11:50:26.918570 [debug] [Thread-4 (]: Began running node model.sales_pipelines.processed_sales
[0m11:50:26.933770 [info ] [Thread-4 (]: 2 of 6 START sql incremental model _processed.processed_sales .................. [RUN]
[0m11:50:26.933770 [debug] [Thread-4 (]: Acquiring new snowflake connection 'model.sales_pipelines.processed_sales'
[0m11:50:26.936035 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.processed_sales
[0m11:50:26.937813 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.processed_sales"
[0m11:50:26.937813 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.processed_sales
[0m11:50:26.953852 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:50:26.954981 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: create or replace  temporary view INCREMENTALETL._processed.processed_sales__dbt_tmp
  
  
  
  
  as (
    

WITH staging AS (
    SELECT * 
    FROM INCREMENTALETL._staging.stg_sales
),

deduped AS (
    SELECT *,
        ROW_NUMBER() OVER (
            PARTITION BY invoice_no, stock_code
            ORDER BY stg_loaded_at DESC
        ) AS row_num
    FROM staging
),

cleaned AS (
    SELECT
        invoice_no,
        stock_code,
        description,
        quantity,
        unit_price,
        discount,
        shipping_cost,
        customer_id,
        invoice_date,
        invoice_year,
        invoice_month,
        country,
        payment_method,
        category,
        sales_channel,
        return_status,
        shipment_provider,
        warehouse_location,
        order_priority,

        -- Derived metrics
        ROUND(quantity * unit_price, 2) AS gross_amount,
        ROUND(quantity * unit_price * (1 - discount), 2) AS net_amount,
        ROUND(
            quantity * unit_price * (1 - discount)
            + COALESCE(shipping_cost, 0),
        2) AS total_amount,

        CASE WHEN return_status = 'RETURNED' THEN TRUE ELSE FALSE END AS is_returned,
        CASE WHEN customer_id IS NULL THEN TRUE ELSE FALSE END AS is_guest_customer,

        stg_loaded_at,
        CURRENT_TIMESTAMP() AS processed_at

    FROM deduped
    WHERE
        row_num = 1
        AND data_quality_flag = 'VALID'
        AND invoice_date IS NOT NULL
)

SELECT *
FROM cleaned


WHERE NOT EXISTS (
    SELECT 1
    FROM INCREMENTALETL._processed.processed_sales t
    WHERE t.invoice_year  = cleaned.invoice_year
      AND t.invoice_month = cleaned.invoice_month
)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m11:50:26.956289 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m11:50:28.325962 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 1.378 seconds
[0m11:50:28.338255 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:50:28.338255 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:50:28.580071 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.249 seconds
[0m11:50:28.620993 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:50:28.623010 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:50:28.830009 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.205 seconds
[0m11:50:28.841583 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:50:28.841583 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:50:29.038205 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.194 seconds
[0m11:50:29.044102 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:50:29.046373 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:50:29.183949 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.140 seconds
[0m11:50:29.211156 [debug] [Thread-4 (]: 
    In "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m11:50:29.223901 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.processed_sales"
[0m11:50:29.232359 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:50:29.232359 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m11:50:29.442458 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.205 seconds
[0m11:50:29.444404 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:50:29.446144 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: merge into INCREMENTALETL._processed.processed_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._processed.processed_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","GROSS_AMOUNT" = DBT_INTERNAL_SOURCE."GROSS_AMOUNT","NET_AMOUNT" = DBT_INTERNAL_SOURCE."NET_AMOUNT","TOTAL_AMOUNT" = DBT_INTERNAL_SOURCE."TOTAL_AMOUNT","IS_RETURNED" = DBT_INTERNAL_SOURCE."IS_RETURNED","IS_GUEST_CUSTOMER" = DBT_INTERNAL_SOURCE."IS_GUEST_CUSTOMER","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","PROCESSED_AT" = DBT_INTERNAL_SOURCE."PROCESSED_AT"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m11:50:30.353827 [debug] [Thread-4 (]: SQL status: SUCCESS 711 in 0.905 seconds
[0m11:50:30.353827 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:50:30.353827 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:50:30.648096 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.292 seconds
[0m11:50:30.660704 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._processed.processed_sales__dbt_tmp
[0m11:50:30.665276 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:50:30.665276 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: drop view if exists INCREMENTALETL._processed.processed_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:50:30.843308 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.178 seconds
[0m11:50:30.843308 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '144044ed-a2b1-4724-920c-d18de3ffb45b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020A6DCA7FA0>]}
[0m11:50:30.852506 [info ] [Thread-4 (]: 2 of 6 OK created sql incremental model _processed.processed_sales ............. [[32mSUCCESS 711[0m in 3.91s]
[0m11:50:30.855584 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.processed_sales
[0m11:50:30.858143 [debug] [Thread-3 (]: Began running node model.sales_pipelines.dim_customers
[0m11:50:30.859721 [debug] [Thread-2 (]: Began running node model.sales_pipelines.dim_date
[0m11:50:30.859721 [debug] [Thread-5 (]: Began running node model.sales_pipelines.dim_products
[0m11:50:30.863050 [debug] [Thread-4 (]: Began running node model.sales_pipelines.fact_sales
[0m11:50:30.859721 [info ] [Thread-3 (]: 3 of 6 START sql table model _marts.dim_customers .............................. [RUN]
[0m11:50:30.864678 [info ] [Thread-2 (]: 4 of 6 START sql table model _marts.dim_date ................................... [RUN]
[0m11:50:30.867341 [info ] [Thread-5 (]: 5 of 6 START sql table model _marts.dim_products ............................... [RUN]
[0m11:50:30.867341 [info ] [Thread-4 (]: 6 of 6 START sql incremental model _marts.fact_sales ........................... [RUN]
[0m11:50:30.872239 [debug] [Thread-3 (]: Acquiring new snowflake connection 'model.sales_pipelines.dim_customers'
[0m11:50:30.872960 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.sales_pipelines.stg_sales, now model.sales_pipelines.dim_date)
[0m11:50:30.875675 [debug] [Thread-5 (]: Acquiring new snowflake connection 'model.sales_pipelines.dim_products'
[0m11:50:30.875675 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.sales_pipelines.processed_sales, now model.sales_pipelines.fact_sales)
[0m11:50:30.875675 [debug] [Thread-3 (]: Began compiling node model.sales_pipelines.dim_customers
[0m11:50:30.875675 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.dim_date
[0m11:50:30.880775 [debug] [Thread-5 (]: Began compiling node model.sales_pipelines.dim_products
[0m11:50:30.882333 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.fact_sales
[0m11:50:30.887776 [debug] [Thread-3 (]: Writing injected SQL for node "model.sales_pipelines.dim_customers"
[0m11:50:30.896777 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.dim_date"
[0m11:50:30.897938 [debug] [Thread-5 (]: Writing injected SQL for node "model.sales_pipelines.dim_products"
[0m11:50:30.907294 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.fact_sales"
[0m11:50:30.913982 [debug] [Thread-3 (]: Began executing node model.sales_pipelines.dim_customers
[0m11:50:30.915232 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.dim_date
[0m11:50:30.916418 [debug] [Thread-5 (]: Began executing node model.sales_pipelines.dim_products
[0m11:50:30.936369 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.fact_sales
[0m11:50:30.976947 [debug] [Thread-3 (]: Writing runtime sql for node "model.sales_pipelines.dim_customers"
[0m11:50:30.985466 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.dim_date"
[0m11:50:30.993973 [debug] [Thread-5 (]: Writing runtime sql for node "model.sales_pipelines.dim_products"
[0m11:50:31.011278 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:50:31.016344 [debug] [Thread-3 (]: Using snowflake connection "model.sales_pipelines.dim_customers"
[0m11:50:31.016344 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.dim_date"
[0m11:50:31.016344 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: create or replace  temporary view INCREMENTALETL._marts.fact_sales__dbt_tmp
  
  
  
  
  as (
    

SELECT
    invoice_no,
    stock_code,
    customer_id,
    invoice_date::DATE AS date_key,

    quantity,
    unit_price,
    discount,
    shipping_cost,

    gross_amount,
    net_amount,
    total_amount,

    is_returned,
    processed_at

FROM INCREMENTALETL._processed.processed_sales


WHERE processed_at > (SELECT MAX(processed_at) FROM INCREMENTALETL._marts.fact_sales)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */;
[0m11:50:31.025870 [debug] [Thread-5 (]: Using snowflake connection "model.sales_pipelines.dim_products"
[0m11:50:31.025870 [debug] [Thread-3 (]: On model.sales_pipelines.dim_customers: create or replace transient table INCREMENTALETL._marts.dim_customers
    
    
    
    as (

SELECT DISTINCT
    customer_id,
    country,
    payment_method,
    sales_channel,
    is_guest_customer,
    MIN(invoice_date) AS first_purchase_date,
    MAX(invoice_date) AS last_purchase_date,
    COUNT(DISTINCT invoice_no) AS total_orders
FROM INCREMENTALETL._processed.processed_sales
WHERE customer_id IS NOT NULL
GROUP BY
    customer_id,
    country,
    payment_method,
    sales_channel,
    is_guest_customer
    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.dim_customers"} */;
[0m11:50:31.028425 [debug] [Thread-2 (]: On model.sales_pipelines.dim_date: create or replace transient table INCREMENTALETL._marts.dim_date
    
    
    
    as (

SELECT DISTINCT
    invoice_date::DATE AS date_key,
    YEAR(invoice_date)  AS year,
    MONTH(invoice_date) AS month,
    DAY(invoice_date)   AS day,
    DAYNAME(invoice_date) AS day_name,
    WEEK(invoice_date)  AS week_number,
    QUARTER(invoice_date) AS quarter
FROM INCREMENTALETL._processed.processed_sales
    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.dim_date"} */;
[0m11:50:31.028425 [debug] [Thread-5 (]: On model.sales_pipelines.dim_products: create or replace transient table INCREMENTALETL._marts.dim_products
    
    
    
    as (

SELECT DISTINCT
    stock_code,
    description,
    category
FROM INCREMENTALETL._processed.processed_sales
    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.dim_products"} */;
[0m11:50:31.028425 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m11:50:31.041905 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m11:50:31.315268 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.285 seconds
[0m11:50:31.319280 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:50:31.324628 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: describe table INCREMENTALETL._marts.fact_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:50:31.485693 [debug] [Thread-4 (]: SQL status: SUCCESS 13 in 0.165 seconds
[0m11:50:31.498747 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:50:31.498747 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: describe table INCREMENTALETL._marts.fact_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:50:31.654084 [debug] [Thread-4 (]: SQL status: SUCCESS 13 in 0.154 seconds
[0m11:50:31.667221 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:50:31.667221 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: describe table "INCREMENTALETL"."_MARTS"."FACT_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:50:31.709836 [debug] [Thread-2 (]: SQL status: SUCCESS 1582 in 0.671 seconds
[0m11:50:31.720449 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '144044ed-a2b1-4724-920c-d18de3ffb45b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020A01411180>]}
[0m11:50:31.720449 [info ] [Thread-2 (]: 4 of 6 OK created sql table model _marts.dim_date .............................. [[32mSUCCESS 1582[0m in 0.85s]
[0m11:50:31.730131 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.dim_date
[0m11:50:31.821654 [debug] [Thread-4 (]: SQL status: SUCCESS 13 in 0.155 seconds
[0m11:50:31.821654 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.fact_sales"
[0m11:50:31.841735 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:50:31.841735 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */;
[0m11:50:32.016448 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.174 seconds
[0m11:50:32.019555 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:50:32.019555 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: merge into INCREMENTALETL._marts.fact_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._marts.fact_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","DATE_KEY" = DBT_INTERNAL_SOURCE."DATE_KEY","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","GROSS_AMOUNT" = DBT_INTERNAL_SOURCE."GROSS_AMOUNT","NET_AMOUNT" = DBT_INTERNAL_SOURCE."NET_AMOUNT","TOTAL_AMOUNT" = DBT_INTERNAL_SOURCE."TOTAL_AMOUNT","IS_RETURNED" = DBT_INTERNAL_SOURCE."IS_RETURNED","PROCESSED_AT" = DBT_INTERNAL_SOURCE."PROCESSED_AT"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "CUSTOMER_ID", "DATE_KEY", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "PROCESSED_AT")
    values
        ("INVOICE_NO", "STOCK_CODE", "CUSTOMER_ID", "DATE_KEY", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "PROCESSED_AT")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */;
[0m11:50:32.190157 [debug] [Thread-5 (]: SQL status: SUCCESS 26944 in 1.152 seconds
[0m11:50:32.198676 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '144044ed-a2b1-4724-920c-d18de3ffb45b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020A7FB24520>]}
[0m11:50:32.198676 [info ] [Thread-5 (]: 5 of 6 OK created sql table model _marts.dim_products .......................... [[32mSUCCESS 26944[0m in 1.32s]
[0m11:50:32.215341 [debug] [Thread-5 (]: Finished running node model.sales_pipelines.dim_products
[0m11:50:32.320264 [debug] [Thread-3 (]: SQL status: SUCCESS 34103 in 1.286 seconds
[0m11:50:32.335953 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '144044ed-a2b1-4724-920c-d18de3ffb45b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020A013C9FF0>]}
[0m11:50:32.335953 [info ] [Thread-3 (]: 3 of 6 OK created sql table model _marts.dim_customers ......................... [[32mSUCCESS 34103[0m in 1.45s]
[0m11:50:32.342577 [debug] [Thread-3 (]: Finished running node model.sales_pipelines.dim_customers
[0m11:50:32.772621 [debug] [Thread-4 (]: SQL status: SUCCESS 711 in 0.750 seconds
[0m11:50:32.772621 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:50:32.772621 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:50:33.133231 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.354 seconds
[0m11:50:33.139251 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._marts.fact_sales__dbt_tmp
[0m11:50:33.139251 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:50:33.139251 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: drop view if exists INCREMENTALETL._marts.fact_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:50:33.348995 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.211 seconds
[0m11:50:33.365080 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '144044ed-a2b1-4724-920c-d18de3ffb45b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020A01553370>]}
[0m11:50:33.365080 [info ] [Thread-4 (]: 6 of 6 OK created sql incremental model _marts.fact_sales ...................... [[32mSUCCESS 711[0m in 2.49s]
[0m11:50:33.365080 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.fact_sales
[0m11:50:33.384531 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:50:33.388269 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m11:50:33.392723 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m11:50:33.748703 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m11:50:33.755934 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m11:50:34.019719 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m11:50:34.019719 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m11:50:34.369181 [debug] [MainThread]: Connection 'list_INCREMENTALETL__processed' was left open.
[0m11:50:34.369181 [debug] [MainThread]: On list_INCREMENTALETL__processed: Close
[0m11:50:34.657797 [debug] [MainThread]: Connection 'list_INCREMENTALETL__marts' was left open.
[0m11:50:34.657797 [debug] [MainThread]: On list_INCREMENTALETL__marts: Close
[0m11:50:35.049127 [debug] [MainThread]: Connection 'list_INCREMENTALETL__staging' was left open.
[0m11:50:35.065312 [debug] [MainThread]: On list_INCREMENTALETL__staging: Close
[0m11:50:35.309804 [debug] [MainThread]: Connection 'model.sales_pipelines.dim_date' was left open.
[0m11:50:35.309804 [debug] [MainThread]: On model.sales_pipelines.dim_date: Close
[0m11:50:35.698062 [debug] [MainThread]: Connection 'model.sales_pipelines.fact_sales' was left open.
[0m11:50:35.702643 [debug] [MainThread]: On model.sales_pipelines.fact_sales: Close
[0m11:50:36.039804 [debug] [MainThread]: Connection 'model.sales_pipelines.dim_customers' was left open.
[0m11:50:36.039804 [debug] [MainThread]: On model.sales_pipelines.dim_customers: Close
[0m11:50:36.392613 [debug] [MainThread]: Connection 'model.sales_pipelines.dim_products' was left open.
[0m11:50:36.392613 [debug] [MainThread]: On model.sales_pipelines.dim_products: Close
[0m11:50:36.776137 [info ] [MainThread]: 
[0m11:50:36.780232 [info ] [MainThread]: Finished running 3 incremental models, 3 table models in 0 hours 0 minutes and 16.08 seconds (16.08s).
[0m11:50:36.794941 [debug] [MainThread]: Command end result
[0m11:50:36.942527 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m11:50:36.942527 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m11:50:36.958555 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\snowflake-incremental-pipeline\dbt_project\target\run_results.json
[0m11:50:36.958555 [info ] [MainThread]: 
[0m11:50:36.958555 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:50:36.966869 [info ] [MainThread]: 
[0m11:50:36.966869 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=6
[0m11:50:36.969035 [debug] [MainThread]: Command `dbt run` succeeded at 11:50:36.969035 after 18.80 seconds
[0m11:50:36.970335 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020A6D1A7670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020A7F4637F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020A7F461C90>]}
[0m11:50:36.970335 [debug] [MainThread]: Flushing usage events
[0m11:50:38.487115 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:50:53.968666 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028942927640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028943BD73A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028943BD75B0>]}


============================== 11:50:53.972974 | 0998c46c-1a75-4924-8295-8780534b1748 ==============================
[0m11:50:53.972974 [info ] [MainThread]: Running with dbt=1.11.6
[0m11:50:53.972974 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'profiles_dir': 'D:\\snowflake-incremental-pipeline\\dbt_project', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'indirect_selection': 'eager', 'invocation_command': 'dbt run', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'D:\\snowflake-incremental-pipeline\\dbt_project\\logs'}
[0m11:50:55.022115 [debug] [MainThread]: Snowflake adapter: Setting snowflake.connector to ERROR (file logging only)
[0m11:50:55.024242 [debug] [MainThread]: Snowflake adapter: Setting botocore to ERROR (file logging only)
[0m11:50:55.024242 [debug] [MainThread]: Snowflake adapter: Setting boto3 to ERROR (file logging only)
[0m11:50:55.351632 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0998c46c-1a75-4924-8295-8780534b1748', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028943C0D5A0>]}
[0m11:50:55.448599 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '0998c46c-1a75-4924-8295-8780534b1748', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028954BABAF0>]}
[0m11:50:55.450605 [info ] [MainThread]: Registered adapter: snowflake=1.11.2
[0m11:50:56.025099 [debug] [MainThread]: checksum: ec27062d18352e9cfca161c166fcc73d2201661a23a3cf3c89196a27f32cfcb8, vars: {}, profile: , target: , version: 1.11.6
[0m11:50:56.300531 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:50:56.300531 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m11:50:56.305024 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:50:56.390029 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0998c46c-1a75-4924-8295-8780534b1748', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000289562DFE50>]}
[0m11:50:56.540219 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m11:50:56.540219 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m11:50:56.576925 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0998c46c-1a75-4924-8295-8780534b1748', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000289562DD990>]}
[0m11:50:56.576925 [info ] [MainThread]: Found 6 models, 6 data tests, 1 source, 522 macros
[0m11:50:56.578950 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0998c46c-1a75-4924-8295-8780534b1748', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028954C95270>]}
[0m11:50:56.580971 [info ] [MainThread]: 
[0m11:50:56.580971 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m11:50:56.580971 [info ] [MainThread]: 
[0m11:50:56.586287 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m11:50:56.597884 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m11:50:56.609265 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m11:50:56.628257 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m11:50:56.718011 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m11:50:56.718011 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m11:50:56.721307 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m11:50:56.721307 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m11:50:56.721307 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m11:50:56.721307 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m11:50:56.721307 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:50:56.721307 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:50:56.721307 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:50:57.892001 [debug] [ThreadPool]: SQL status: SUCCESS 5 in 1.174 seconds
[0m11:50:57.921903 [debug] [ThreadPool]: SQL status: SUCCESS 5 in 1.199 seconds
[0m11:50:57.929616 [debug] [ThreadPool]: SQL status: SUCCESS 5 in 1.205 seconds
[0m11:50:57.954911 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__processed'
[0m11:50:57.956063 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__marts'
[0m11:50:57.986430 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__staging'
[0m11:50:57.994995 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__processed"
[0m11:50:58.011059 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__marts"
[0m11:50:58.012699 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__staging"
[0m11:50:58.012699 [debug] [ThreadPool]: On list_INCREMENTALETL__processed: show objects in INCREMENTALETL._processed
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__processed"} */;
[0m11:50:58.012699 [debug] [ThreadPool]: On list_INCREMENTALETL__marts: show objects in INCREMENTALETL._marts
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__marts"} */;
[0m11:50:58.012699 [debug] [ThreadPool]: On list_INCREMENTALETL__staging: show objects in INCREMENTALETL._staging
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__staging"} */;
[0m11:50:58.023066 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:50:58.023066 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:50:58.025324 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:50:58.795990 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 0.778 seconds
[0m11:50:58.860266 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.839 seconds
[0m11:50:58.908823 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.891 seconds
[0m11:50:58.924570 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0998c46c-1a75-4924-8295-8780534b1748', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028954C4CD00>]}
[0m11:50:58.942948 [debug] [Thread-2 (]: Began running node model.sales_pipelines.stg_sales
[0m11:50:58.942948 [info ] [Thread-2 (]: 1 of 6 START sql incremental model _staging.stg_sales .......................... [RUN]
[0m11:50:58.951071 [debug] [Thread-2 (]: Acquiring new snowflake connection 'model.sales_pipelines.stg_sales'
[0m11:50:58.951071 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.stg_sales
[0m11:50:59.007802 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.stg_sales"
[0m11:50:59.007802 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.stg_sales
[0m11:50:59.096840 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:50:59.096840 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: create or replace  temporary view INCREMENTALETL._staging.stg_sales__dbt_tmp
  
  
  
  
  as (
    

WITH raw_data AS (
    SELECT * 
    FROM INCREMENTALETL.LANDINGZONE.RAW_SALES
),

staged AS (
    SELECT
        -- IDs
        CAST(INVOICENO AS VARCHAR)    AS invoice_no,
        CAST(STOCKCODE AS VARCHAR)    AS stock_code,
        CAST(DESCRIPTION AS VARCHAR)  AS description,

        -- Quantities & Prices
        TRY_CAST(QUANTITY AS INT)      AS quantity,
        TRY_CAST(UNITPRICE AS FLOAT)   AS unit_price,
        TRY_CAST(DISCOUNT AS FLOAT)    AS discount,
        TRY_CAST(SHIPPINGCOST AS FLOAT) AS shipping_cost,
        TRY_CAST(CUSTOMERID AS FLOAT)  AS customer_id,

        -- Convert once
        TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS') AS invoice_date,

        TRIM(UPPER(COUNTRY))            AS country,
        TRIM(UPPER(PAYMENTMETHOD))      AS payment_method,
        TRIM(UPPER(CATEGORY))           AS category,
        TRIM(UPPER(SALESCHANNEL))       AS sales_channel,
        TRIM(UPPER(RETURNSTATUS))       AS return_status,
        TRIM(UPPER(SHIPMENTPROVIDER))   AS shipment_provider,
        TRIM(UPPER(WAREHOUSELOCATION))  AS warehouse_location,
        TRIM(UPPER(ORDERPRIORITY))      AS order_priority,

        CURRENT_TIMESTAMP()             AS stg_loaded_at

    FROM raw_data
),

validated AS (
    SELECT
        *,
        YEAR(invoice_date)  AS invoice_year,
        MONTH(invoice_date) AS invoice_month,

        CASE
            WHEN invoice_date IS NULL           THEN 'INVALID_DATE'
            WHEN quantity IS NULL               THEN 'INVALID_QUANTITY'
            WHEN unit_price IS NULL             THEN 'INVALID_PRICE'
            WHEN shipping_cost < 0              THEN 'NEGATIVE_SHIPPING'
            WHEN discount < 0 OR discount > 1   THEN 'INVALID_DISCOUNT'
            WHEN invoice_no IS NULL             THEN 'NULL_INVOICE'
            ELSE 'VALID'
        END AS data_quality_flag
    FROM staged
)

SELECT *
FROM validated


WHERE NOT EXISTS (
    SELECT 1
    FROM INCREMENTALETL._staging.stg_sales t
    WHERE t.invoice_year  = validated.invoice_year
      AND t.invoice_month = validated.invoice_month
)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m11:50:59.096840 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m11:50:59.873210 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.770 seconds
[0m11:51:00.002735 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:51:00.005745 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:51:00.147869 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.140 seconds
[0m11:51:00.171496 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:51:00.173030 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:51:00.318571 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.141 seconds
[0m11:51:00.358505 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:51:00.366561 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:51:00.496209 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.131 seconds
[0m11:51:00.516779 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:51:00.516779 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table "INCREMENTALETL"."_STAGING"."STG_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:51:00.655778 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.134 seconds
[0m11:51:00.703081 [debug] [Thread-2 (]: 
    In "INCREMENTALETL"."_STAGING"."STG_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m11:51:00.734960 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.stg_sales"
[0m11:51:00.746431 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:51:00.746431 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m11:51:00.897010 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.149 seconds
[0m11:51:00.897010 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:51:00.897010 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: merge into INCREMENTALETL._staging.stg_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._staging.stg_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","DATA_QUALITY_FLAG" = DBT_INTERNAL_SOURCE."DATA_QUALITY_FLAG"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "INVOICE_YEAR", "INVOICE_MONTH", "DATA_QUALITY_FLAG")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "INVOICE_YEAR", "INVOICE_MONTH", "DATA_QUALITY_FLAG")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m11:51:01.703667 [debug] [Thread-2 (]: SQL status: SUCCESS 744 in 0.798 seconds
[0m11:51:01.703667 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:51:01.703667 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:51:02.020336 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.314 seconds
[0m11:51:02.045434 [debug] [Thread-2 (]: Applying DROP to: INCREMENTALETL._staging.stg_sales__dbt_tmp
[0m11:51:02.055913 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:51:02.055913 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: drop view if exists INCREMENTALETL._staging.stg_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:51:02.241970 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.177 seconds
[0m11:51:02.292359 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0998c46c-1a75-4924-8295-8780534b1748', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028943C68EE0>]}
[0m11:51:02.297652 [info ] [Thread-2 (]: 1 of 6 OK created sql incremental model _staging.stg_sales ..................... [[32mSUCCESS 744[0m in 3.35s]
[0m11:51:02.298704 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.stg_sales
[0m11:51:02.298704 [debug] [Thread-4 (]: Began running node model.sales_pipelines.processed_sales
[0m11:51:02.298704 [info ] [Thread-4 (]: 2 of 6 START sql incremental model _processed.processed_sales .................. [RUN]
[0m11:51:02.304668 [debug] [Thread-4 (]: Acquiring new snowflake connection 'model.sales_pipelines.processed_sales'
[0m11:51:02.307196 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.processed_sales
[0m11:51:02.314068 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.processed_sales"
[0m11:51:02.314068 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.processed_sales
[0m11:51:02.325257 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:51:02.325257 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: create or replace  temporary view INCREMENTALETL._processed.processed_sales__dbt_tmp
  
  
  
  
  as (
    

WITH staging AS (
    SELECT * 
    FROM INCREMENTALETL._staging.stg_sales
),

deduped AS (
    SELECT *,
        ROW_NUMBER() OVER (
            PARTITION BY invoice_no, stock_code
            ORDER BY stg_loaded_at DESC
        ) AS row_num
    FROM staging
),

cleaned AS (
    SELECT
        invoice_no,
        stock_code,
        description,
        quantity,
        unit_price,
        discount,
        shipping_cost,
        customer_id,
        invoice_date,
        invoice_year,
        invoice_month,
        country,
        payment_method,
        category,
        sales_channel,
        return_status,
        shipment_provider,
        warehouse_location,
        order_priority,

        -- Derived metrics
        ROUND(quantity * unit_price, 2) AS gross_amount,
        ROUND(quantity * unit_price * (1 - discount), 2) AS net_amount,
        ROUND(
            quantity * unit_price * (1 - discount)
            + COALESCE(shipping_cost, 0),
        2) AS total_amount,

        CASE WHEN return_status = 'RETURNED' THEN TRUE ELSE FALSE END AS is_returned,
        CASE WHEN customer_id IS NULL THEN TRUE ELSE FALSE END AS is_guest_customer,

        stg_loaded_at,
        CURRENT_TIMESTAMP() AS processed_at

    FROM deduped
    WHERE
        row_num = 1
        AND data_quality_flag = 'VALID'
        AND invoice_date IS NOT NULL
)

SELECT *
FROM cleaned


WHERE NOT EXISTS (
    SELECT 1
    FROM INCREMENTALETL._processed.processed_sales t
    WHERE t.invoice_year  = cleaned.invoice_year
      AND t.invoice_month = cleaned.invoice_month
)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m11:51:02.325257 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m11:51:03.222099 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.896 seconds
[0m11:51:03.242491 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:51:03.242491 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:51:03.452951 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.205 seconds
[0m11:51:03.471546 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:51:03.473354 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:51:03.653167 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.178 seconds
[0m11:51:03.671479 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:51:03.672522 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:51:03.958998 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.290 seconds
[0m11:51:03.983605 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:51:03.988248 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:51:04.172925 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.183 seconds
[0m11:51:04.188536 [debug] [Thread-4 (]: 
    In "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m11:51:04.195401 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.processed_sales"
[0m11:51:04.198053 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:51:04.198053 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m11:51:04.373632 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.183 seconds
[0m11:51:04.373632 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:51:04.391738 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: merge into INCREMENTALETL._processed.processed_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._processed.processed_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","GROSS_AMOUNT" = DBT_INTERNAL_SOURCE."GROSS_AMOUNT","NET_AMOUNT" = DBT_INTERNAL_SOURCE."NET_AMOUNT","TOTAL_AMOUNT" = DBT_INTERNAL_SOURCE."TOTAL_AMOUNT","IS_RETURNED" = DBT_INTERNAL_SOURCE."IS_RETURNED","IS_GUEST_CUSTOMER" = DBT_INTERNAL_SOURCE."IS_GUEST_CUSTOMER","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","PROCESSED_AT" = DBT_INTERNAL_SOURCE."PROCESSED_AT"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m11:51:05.395503 [debug] [Thread-4 (]: SQL status: SUCCESS 725 in 1.002 seconds
[0m11:51:05.395503 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:51:05.395503 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:51:05.797583 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.397 seconds
[0m11:51:05.813353 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._processed.processed_sales__dbt_tmp
[0m11:51:05.822351 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:51:05.822351 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: drop view if exists INCREMENTALETL._processed.processed_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:51:06.010831 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.183 seconds
[0m11:51:06.010831 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0998c46c-1a75-4924-8295-8780534b1748', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000289561C1810>]}
[0m11:51:06.025102 [info ] [Thread-4 (]: 2 of 6 OK created sql incremental model _processed.processed_sales ............. [[32mSUCCESS 725[0m in 3.71s]
[0m11:51:06.031647 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.processed_sales
[0m11:51:06.036919 [debug] [Thread-3 (]: Began running node model.sales_pipelines.dim_customers
[0m11:51:06.036919 [debug] [Thread-2 (]: Began running node model.sales_pipelines.dim_date
[0m11:51:06.041977 [debug] [Thread-5 (]: Began running node model.sales_pipelines.dim_products
[0m11:51:06.044681 [debug] [Thread-4 (]: Began running node model.sales_pipelines.fact_sales
[0m11:51:06.046675 [info ] [Thread-3 (]: 3 of 6 START sql table model _marts.dim_customers .............................. [RUN]
[0m11:51:06.051288 [info ] [Thread-2 (]: 4 of 6 START sql table model _marts.dim_date ................................... [RUN]
[0m11:51:06.058888 [info ] [Thread-5 (]: 5 of 6 START sql table model _marts.dim_products ............................... [RUN]
[0m11:51:06.062614 [info ] [Thread-4 (]: 6 of 6 START sql incremental model _marts.fact_sales ........................... [RUN]
[0m11:51:06.070123 [debug] [Thread-3 (]: Acquiring new snowflake connection 'model.sales_pipelines.dim_customers'
[0m11:51:06.074164 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.sales_pipelines.stg_sales, now model.sales_pipelines.dim_date)
[0m11:51:06.074164 [debug] [Thread-5 (]: Acquiring new snowflake connection 'model.sales_pipelines.dim_products'
[0m11:51:06.079476 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.sales_pipelines.processed_sales, now model.sales_pipelines.fact_sales)
[0m11:51:06.079476 [debug] [Thread-3 (]: Began compiling node model.sales_pipelines.dim_customers
[0m11:51:06.084378 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.dim_date
[0m11:51:06.085072 [debug] [Thread-5 (]: Began compiling node model.sales_pipelines.dim_products
[0m11:51:06.086204 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.fact_sales
[0m11:51:06.091745 [debug] [Thread-3 (]: Writing injected SQL for node "model.sales_pipelines.dim_customers"
[0m11:51:06.099807 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.dim_date"
[0m11:51:06.105423 [debug] [Thread-5 (]: Writing injected SQL for node "model.sales_pipelines.dim_products"
[0m11:51:06.118907 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.fact_sales"
[0m11:51:06.118907 [debug] [Thread-3 (]: Began executing node model.sales_pipelines.dim_customers
[0m11:51:06.122609 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.dim_date
[0m11:51:06.124775 [debug] [Thread-5 (]: Began executing node model.sales_pipelines.dim_products
[0m11:51:06.131613 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.fact_sales
[0m11:51:06.155830 [debug] [Thread-3 (]: Writing runtime sql for node "model.sales_pipelines.dim_customers"
[0m11:51:06.180157 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.dim_date"
[0m11:51:06.188141 [debug] [Thread-5 (]: Writing runtime sql for node "model.sales_pipelines.dim_products"
[0m11:51:06.196268 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:51:06.196268 [debug] [Thread-3 (]: Using snowflake connection "model.sales_pipelines.dim_customers"
[0m11:51:06.204476 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.dim_date"
[0m11:51:06.204476 [debug] [Thread-5 (]: Using snowflake connection "model.sales_pipelines.dim_products"
[0m11:51:06.204476 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: create or replace  temporary view INCREMENTALETL._marts.fact_sales__dbt_tmp
  
  
  
  
  as (
    

SELECT
    invoice_no,
    stock_code,
    customer_id,
    invoice_date::DATE AS date_key,

    quantity,
    unit_price,
    discount,
    shipping_cost,

    gross_amount,
    net_amount,
    total_amount,

    is_returned,
    processed_at

FROM INCREMENTALETL._processed.processed_sales


WHERE processed_at > (SELECT MAX(processed_at) FROM INCREMENTALETL._marts.fact_sales)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */;
[0m11:51:06.204476 [debug] [Thread-3 (]: On model.sales_pipelines.dim_customers: create or replace transient table INCREMENTALETL._marts.dim_customers
    
    
    
    as (

SELECT DISTINCT
    customer_id,
    country,
    payment_method,
    sales_channel,
    is_guest_customer,
    MIN(invoice_date) AS first_purchase_date,
    MAX(invoice_date) AS last_purchase_date,
    COUNT(DISTINCT invoice_no) AS total_orders
FROM INCREMENTALETL._processed.processed_sales
WHERE customer_id IS NOT NULL
GROUP BY
    customer_id,
    country,
    payment_method,
    sales_channel,
    is_guest_customer
    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.dim_customers"} */;
[0m11:51:06.212810 [debug] [Thread-2 (]: On model.sales_pipelines.dim_date: create or replace transient table INCREMENTALETL._marts.dim_date
    
    
    
    as (

SELECT DISTINCT
    invoice_date::DATE AS date_key,
    YEAR(invoice_date)  AS year,
    MONTH(invoice_date) AS month,
    DAY(invoice_date)   AS day,
    DAYNAME(invoice_date) AS day_name,
    WEEK(invoice_date)  AS week_number,
    QUARTER(invoice_date) AS quarter
FROM INCREMENTALETL._processed.processed_sales
    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.dim_date"} */;
[0m11:51:06.212810 [debug] [Thread-5 (]: On model.sales_pipelines.dim_products: create or replace transient table INCREMENTALETL._marts.dim_products
    
    
    
    as (

SELECT DISTINCT
    stock_code,
    description,
    category
FROM INCREMENTALETL._processed.processed_sales
    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.dim_products"} */;
[0m11:51:06.217754 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m11:51:06.224255 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m11:51:06.476248 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.262 seconds
[0m11:51:06.476248 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:51:06.476248 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: describe table INCREMENTALETL._marts.fact_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:51:06.689731 [debug] [Thread-4 (]: SQL status: SUCCESS 13 in 0.202 seconds
[0m11:51:06.706008 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:51:06.708918 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: describe table INCREMENTALETL._marts.fact_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:51:06.867585 [debug] [Thread-2 (]: SQL status: SUCCESS 1613 in 0.647 seconds
[0m11:51:06.885163 [debug] [Thread-4 (]: SQL status: SUCCESS 13 in 0.175 seconds
[0m11:51:06.889932 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0998c46c-1a75-4924-8295-8780534b1748', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028956AD8310>]}
[0m11:51:06.905596 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:51:06.905596 [info ] [Thread-2 (]: 4 of 6 OK created sql table model _marts.dim_date .............................. [[32mSUCCESS 1613[0m in 0.82s]
[0m11:51:06.905596 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: describe table "INCREMENTALETL"."_MARTS"."FACT_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:51:06.913843 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.dim_date
[0m11:51:07.139649 [debug] [Thread-4 (]: SQL status: SUCCESS 13 in 0.225 seconds
[0m11:51:07.149673 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.fact_sales"
[0m11:51:07.165802 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:51:07.165802 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */;
[0m11:51:07.363227 [debug] [Thread-5 (]: SQL status: SUCCESS 27320 in 1.139 seconds
[0m11:51:07.367652 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.198 seconds
[0m11:51:07.381299 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0998c46c-1a75-4924-8295-8780534b1748', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028956BD6D10>]}
[0m11:51:07.382056 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:51:07.388874 [info ] [Thread-5 (]: 5 of 6 OK created sql table model _marts.dim_products .......................... [[32mSUCCESS 27320[0m in 1.31s]
[0m11:51:07.388874 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: merge into INCREMENTALETL._marts.fact_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._marts.fact_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","DATE_KEY" = DBT_INTERNAL_SOURCE."DATE_KEY","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","GROSS_AMOUNT" = DBT_INTERNAL_SOURCE."GROSS_AMOUNT","NET_AMOUNT" = DBT_INTERNAL_SOURCE."NET_AMOUNT","TOTAL_AMOUNT" = DBT_INTERNAL_SOURCE."TOTAL_AMOUNT","IS_RETURNED" = DBT_INTERNAL_SOURCE."IS_RETURNED","PROCESSED_AT" = DBT_INTERNAL_SOURCE."PROCESSED_AT"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "CUSTOMER_ID", "DATE_KEY", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "PROCESSED_AT")
    values
        ("INVOICE_NO", "STOCK_CODE", "CUSTOMER_ID", "DATE_KEY", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "PROCESSED_AT")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */;
[0m11:51:07.398297 [debug] [Thread-5 (]: Finished running node model.sales_pipelines.dim_products
[0m11:51:07.589220 [debug] [Thread-3 (]: SQL status: SUCCESS 34759 in 1.370 seconds
[0m11:51:07.589220 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0998c46c-1a75-4924-8295-8780534b1748', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000289561C25F0>]}
[0m11:51:07.605079 [info ] [Thread-3 (]: 3 of 6 OK created sql table model _marts.dim_customers ......................... [[32mSUCCESS 34759[0m in 1.53s]
[0m11:51:07.605079 [debug] [Thread-3 (]: Finished running node model.sales_pipelines.dim_customers
[0m11:51:08.222238 [debug] [Thread-4 (]: SQL status: SUCCESS 725 in 0.822 seconds
[0m11:51:08.222238 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:51:08.222238 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:51:08.566714 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.340 seconds
[0m11:51:08.582812 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._marts.fact_sales__dbt_tmp
[0m11:51:08.589798 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:51:08.592643 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: drop view if exists INCREMENTALETL._marts.fact_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:51:08.792602 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.198 seconds
[0m11:51:08.804828 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0998c46c-1a75-4924-8295-8780534b1748', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028956B5EEC0>]}
[0m11:51:08.807967 [info ] [Thread-4 (]: 6 of 6 OK created sql incremental model _marts.fact_sales ...................... [[32mSUCCESS 725[0m in 2.73s]
[0m11:51:08.814008 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.fact_sales
[0m11:51:08.817801 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:51:08.819813 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m11:51:08.822155 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m11:51:09.096801 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m11:51:09.096801 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m11:51:09.356091 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m11:51:09.358392 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m11:51:09.767209 [debug] [MainThread]: Connection 'list_INCREMENTALETL__processed' was left open.
[0m11:51:09.769823 [debug] [MainThread]: On list_INCREMENTALETL__processed: Close
[0m11:51:10.110524 [debug] [MainThread]: Connection 'list_INCREMENTALETL__marts' was left open.
[0m11:51:10.110524 [debug] [MainThread]: On list_INCREMENTALETL__marts: Close
[0m11:51:10.374448 [debug] [MainThread]: Connection 'list_INCREMENTALETL__staging' was left open.
[0m11:51:10.374448 [debug] [MainThread]: On list_INCREMENTALETL__staging: Close
[0m11:51:10.718578 [debug] [MainThread]: Connection 'model.sales_pipelines.dim_date' was left open.
[0m11:51:10.720601 [debug] [MainThread]: On model.sales_pipelines.dim_date: Close
[0m11:51:10.959171 [debug] [MainThread]: Connection 'model.sales_pipelines.fact_sales' was left open.
[0m11:51:10.959171 [debug] [MainThread]: On model.sales_pipelines.fact_sales: Close
[0m11:51:11.332108 [debug] [MainThread]: Connection 'model.sales_pipelines.dim_customers' was left open.
[0m11:51:11.332108 [debug] [MainThread]: On model.sales_pipelines.dim_customers: Close
[0m11:51:11.838931 [debug] [MainThread]: Connection 'model.sales_pipelines.dim_products' was left open.
[0m11:51:11.838931 [debug] [MainThread]: On model.sales_pipelines.dim_products: Close
[0m11:51:12.092464 [info ] [MainThread]: 
[0m11:51:12.092464 [info ] [MainThread]: Finished running 3 incremental models, 3 table models in 0 hours 0 minutes and 15.51 seconds (15.51s).
[0m11:51:12.106141 [debug] [MainThread]: Command end result
[0m11:51:12.172097 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m11:51:12.178390 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m11:51:12.190502 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\snowflake-incremental-pipeline\dbt_project\target\run_results.json
[0m11:51:12.190502 [info ] [MainThread]: 
[0m11:51:12.192815 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:51:12.192815 [info ] [MainThread]: 
[0m11:51:12.192815 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=6
[0m11:51:12.192815 [debug] [MainThread]: Command `dbt run` succeeded at 11:51:12.192815 after 18.35 seconds
[0m11:51:12.197879 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028942927640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028943C0D5A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028954BABAF0>]}
[0m11:51:12.197879 [debug] [MainThread]: Flushing usage events
[0m11:51:14.191750 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:51:47.013494 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000200EC147640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000200EE3C73A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000200EE3C75B0>]}


============================== 11:51:47.029669 | 7f488c90-0ef2-4d43-926d-bb0bdca941cd ==============================
[0m11:51:47.029669 [info ] [MainThread]: Running with dbt=1.11.6
[0m11:51:47.029669 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'D:\\snowflake-incremental-pipeline\\dbt_project', 'indirect_selection': 'eager', 'use_colors': 'True', 'invocation_command': 'dbt run', 'use_experimental_parser': 'False', 'log_path': 'D:\\snowflake-incremental-pipeline\\dbt_project\\logs'}
[0m11:51:48.010354 [debug] [MainThread]: Snowflake adapter: Setting snowflake.connector to ERROR (file logging only)
[0m11:51:48.010354 [debug] [MainThread]: Snowflake adapter: Setting botocore to ERROR (file logging only)
[0m11:51:48.012365 [debug] [MainThread]: Snowflake adapter: Setting boto3 to ERROR (file logging only)
[0m11:51:48.312354 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7f488c90-0ef2-4d43-926d-bb0bdca941cd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000200EE3F95A0>]}
[0m11:51:48.390264 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7f488c90-0ef2-4d43-926d-bb0bdca941cd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000200FF39BAF0>]}
[0m11:51:48.390264 [info ] [MainThread]: Registered adapter: snowflake=1.11.2
[0m11:51:48.909592 [debug] [MainThread]: checksum: ec27062d18352e9cfca161c166fcc73d2201661a23a3cf3c89196a27f32cfcb8, vars: {}, profile: , target: , version: 1.11.6
[0m11:51:49.129091 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:51:49.129091 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m11:51:49.129091 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:51:49.210062 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7f488c90-0ef2-4d43-926d-bb0bdca941cd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000200FFB03E50>]}
[0m11:51:49.351572 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m11:51:49.362115 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m11:51:49.385080 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7f488c90-0ef2-4d43-926d-bb0bdca941cd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000200FFB01990>]}
[0m11:51:49.385080 [info ] [MainThread]: Found 6 models, 6 data tests, 1 source, 522 macros
[0m11:51:49.386811 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7f488c90-0ef2-4d43-926d-bb0bdca941cd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000200FF485270>]}
[0m11:51:49.388057 [info ] [MainThread]: 
[0m11:51:49.388057 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m11:51:49.388057 [info ] [MainThread]: 
[0m11:51:49.395421 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m11:51:49.420055 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m11:51:49.436160 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m11:51:49.459451 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m11:51:49.540157 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m11:51:49.540157 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m11:51:49.540157 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m11:51:49.540157 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m11:51:49.540157 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m11:51:49.540157 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m11:51:49.540157 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:51:49.540157 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:51:49.540157 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:51:50.705775 [debug] [ThreadPool]: SQL status: SUCCESS 5 in 1.152 seconds
[0m11:51:50.739061 [debug] [ThreadPool]: SQL status: SUCCESS 5 in 1.188 seconds
[0m11:51:50.775032 [debug] [ThreadPool]: SQL status: SUCCESS 5 in 1.225 seconds
[0m11:51:50.782133 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__staging'
[0m11:51:50.787691 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__processed'
[0m11:51:50.814937 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__marts'
[0m11:51:50.821837 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__staging"
[0m11:51:50.823255 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__processed"
[0m11:51:50.834786 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__marts"
[0m11:51:50.834786 [debug] [ThreadPool]: On list_INCREMENTALETL__staging: show objects in INCREMENTALETL._staging
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__staging"} */;
[0m11:51:50.837059 [debug] [ThreadPool]: On list_INCREMENTALETL__processed: show objects in INCREMENTALETL._processed
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__processed"} */;
[0m11:51:50.837059 [debug] [ThreadPool]: On list_INCREMENTALETL__marts: show objects in INCREMENTALETL._marts
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__marts"} */;
[0m11:51:50.839074 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:51:50.839074 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:51:50.839074 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:51:51.615214 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 0.779 seconds
[0m11:51:51.646773 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.818 seconds
[0m11:51:51.711625 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.873 seconds
[0m11:51:51.715641 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7f488c90-0ef2-4d43-926d-bb0bdca941cd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000200FFB511B0>]}
[0m11:51:51.723677 [debug] [Thread-2 (]: Began running node model.sales_pipelines.stg_sales
[0m11:51:51.725429 [info ] [Thread-2 (]: 1 of 6 START sql incremental model _staging.stg_sales .......................... [RUN]
[0m11:51:51.725429 [debug] [Thread-2 (]: Acquiring new snowflake connection 'model.sales_pipelines.stg_sales'
[0m11:51:51.727438 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.stg_sales
[0m11:51:51.750388 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.stg_sales"
[0m11:51:51.752529 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.stg_sales
[0m11:51:51.828067 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:51:51.843889 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: create or replace  temporary view INCREMENTALETL._staging.stg_sales__dbt_tmp
  
  
  
  
  as (
    

WITH raw_data AS (
    SELECT * 
    FROM INCREMENTALETL.LANDINGZONE.RAW_SALES
),

staged AS (
    SELECT
        -- IDs
        CAST(INVOICENO AS VARCHAR)    AS invoice_no,
        CAST(STOCKCODE AS VARCHAR)    AS stock_code,
        CAST(DESCRIPTION AS VARCHAR)  AS description,

        -- Quantities & Prices
        TRY_CAST(QUANTITY AS INT)      AS quantity,
        TRY_CAST(UNITPRICE AS FLOAT)   AS unit_price,
        TRY_CAST(DISCOUNT AS FLOAT)    AS discount,
        TRY_CAST(SHIPPINGCOST AS FLOAT) AS shipping_cost,
        TRY_CAST(CUSTOMERID AS FLOAT)  AS customer_id,

        -- Convert once
        TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS') AS invoice_date,

        TRIM(UPPER(COUNTRY))            AS country,
        TRIM(UPPER(PAYMENTMETHOD))      AS payment_method,
        TRIM(UPPER(CATEGORY))           AS category,
        TRIM(UPPER(SALESCHANNEL))       AS sales_channel,
        TRIM(UPPER(RETURNSTATUS))       AS return_status,
        TRIM(UPPER(SHIPMENTPROVIDER))   AS shipment_provider,
        TRIM(UPPER(WAREHOUSELOCATION))  AS warehouse_location,
        TRIM(UPPER(ORDERPRIORITY))      AS order_priority,

        CURRENT_TIMESTAMP()             AS stg_loaded_at

    FROM raw_data
),

validated AS (
    SELECT
        *,
        YEAR(invoice_date)  AS invoice_year,
        MONTH(invoice_date) AS invoice_month,

        CASE
            WHEN invoice_date IS NULL           THEN 'INVALID_DATE'
            WHEN quantity IS NULL               THEN 'INVALID_QUANTITY'
            WHEN unit_price IS NULL             THEN 'INVALID_PRICE'
            WHEN shipping_cost < 0              THEN 'NEGATIVE_SHIPPING'
            WHEN discount < 0 OR discount > 1   THEN 'INVALID_DISCOUNT'
            WHEN invoice_no IS NULL             THEN 'NULL_INVOICE'
            ELSE 'VALID'
        END AS data_quality_flag
    FROM staged
)

SELECT *
FROM validated


WHERE NOT EXISTS (
    SELECT 1
    FROM INCREMENTALETL._staging.stg_sales t
    WHERE t.invoice_year  = validated.invoice_year
      AND t.invoice_month = validated.invoice_month
)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m11:51:51.843889 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m11:51:52.860198 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 1.017 seconds
[0m11:51:52.983549 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:51:52.983549 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:51:53.204790 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.219 seconds
[0m11:51:53.220971 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:51:53.220971 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:51:53.418764 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.183 seconds
[0m11:51:53.463084 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:51:53.463084 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:51:54.118177 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.656 seconds
[0m11:51:54.149542 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:51:54.149542 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table "INCREMENTALETL"."_STAGING"."STG_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:51:54.336203 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.186 seconds
[0m11:51:54.384489 [debug] [Thread-2 (]: 
    In "INCREMENTALETL"."_STAGING"."STG_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m11:51:54.416725 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.stg_sales"
[0m11:51:54.425379 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:51:54.425379 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m11:51:54.644844 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.221 seconds
[0m11:51:54.644844 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:51:54.644844 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: merge into INCREMENTALETL._staging.stg_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._staging.stg_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","DATA_QUALITY_FLAG" = DBT_INTERNAL_SOURCE."DATA_QUALITY_FLAG"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "INVOICE_YEAR", "INVOICE_MONTH", "DATA_QUALITY_FLAG")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "INVOICE_YEAR", "INVOICE_MONTH", "DATA_QUALITY_FLAG")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m11:51:55.556792 [debug] [Thread-2 (]: SQL status: SUCCESS 1464 in 0.906 seconds
[0m11:51:55.556792 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:51:55.556792 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:51:55.827359 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.259 seconds
[0m11:51:55.860453 [debug] [Thread-2 (]: Applying DROP to: INCREMENTALETL._staging.stg_sales__dbt_tmp
[0m11:51:55.872254 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:51:55.872254 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: drop view if exists INCREMENTALETL._staging.stg_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:51:56.089039 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.215 seconds
[0m11:51:56.145702 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7f488c90-0ef2-4d43-926d-bb0bdca941cd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020081409150>]}
[0m11:51:56.145702 [info ] [Thread-2 (]: 1 of 6 OK created sql incremental model _staging.stg_sales ..................... [[32mSUCCESS 1464[0m in 4.42s]
[0m11:51:56.147711 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.stg_sales
[0m11:51:56.147711 [debug] [Thread-4 (]: Began running node model.sales_pipelines.processed_sales
[0m11:51:56.147711 [info ] [Thread-4 (]: 2 of 6 START sql incremental model _processed.processed_sales .................. [RUN]
[0m11:51:56.147711 [debug] [Thread-4 (]: Acquiring new snowflake connection 'model.sales_pipelines.processed_sales'
[0m11:51:56.153627 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.processed_sales
[0m11:51:56.155819 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.processed_sales"
[0m11:51:56.161846 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.processed_sales
[0m11:51:56.170047 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:51:56.170047 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: create or replace  temporary view INCREMENTALETL._processed.processed_sales__dbt_tmp
  
  
  
  
  as (
    

WITH staging AS (
    SELECT * 
    FROM INCREMENTALETL._staging.stg_sales
),

deduped AS (
    SELECT *,
        ROW_NUMBER() OVER (
            PARTITION BY invoice_no, stock_code
            ORDER BY stg_loaded_at DESC
        ) AS row_num
    FROM staging
),

cleaned AS (
    SELECT
        invoice_no,
        stock_code,
        description,
        quantity,
        unit_price,
        discount,
        shipping_cost,
        customer_id,
        invoice_date,
        invoice_year,
        invoice_month,
        country,
        payment_method,
        category,
        sales_channel,
        return_status,
        shipment_provider,
        warehouse_location,
        order_priority,

        -- Derived metrics
        ROUND(quantity * unit_price, 2) AS gross_amount,
        ROUND(quantity * unit_price * (1 - discount), 2) AS net_amount,
        ROUND(
            quantity * unit_price * (1 - discount)
            + COALESCE(shipping_cost, 0),
        2) AS total_amount,

        CASE WHEN return_status = 'RETURNED' THEN TRUE ELSE FALSE END AS is_returned,
        CASE WHEN customer_id IS NULL THEN TRUE ELSE FALSE END AS is_guest_customer,

        stg_loaded_at,
        CURRENT_TIMESTAMP() AS processed_at

    FROM deduped
    WHERE
        row_num = 1
        AND data_quality_flag = 'VALID'
        AND invoice_date IS NOT NULL
)

SELECT *
FROM cleaned


WHERE NOT EXISTS (
    SELECT 1
    FROM INCREMENTALETL._processed.processed_sales t
    WHERE t.invoice_year  = cleaned.invoice_year
      AND t.invoice_month = cleaned.invoice_month
)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m11:51:56.170047 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m11:51:57.097683 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.928 seconds
[0m11:51:57.097683 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:51:57.097683 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:51:57.300014 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.195 seconds
[0m11:51:57.329145 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:51:57.329145 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:51:57.505587 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.173 seconds
[0m11:51:57.535739 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:51:57.535739 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:51:57.712620 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.178 seconds
[0m11:51:57.730392 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:51:57.730392 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:51:57.906350 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.175 seconds
[0m11:51:57.923682 [debug] [Thread-4 (]: 
    In "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m11:51:57.923682 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.processed_sales"
[0m11:51:57.939622 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:51:57.941013 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m11:51:58.115653 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.188 seconds
[0m11:51:58.131688 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:51:58.131688 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: merge into INCREMENTALETL._processed.processed_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._processed.processed_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","GROSS_AMOUNT" = DBT_INTERNAL_SOURCE."GROSS_AMOUNT","NET_AMOUNT" = DBT_INTERNAL_SOURCE."NET_AMOUNT","TOTAL_AMOUNT" = DBT_INTERNAL_SOURCE."TOTAL_AMOUNT","IS_RETURNED" = DBT_INTERNAL_SOURCE."IS_RETURNED","IS_GUEST_CUSTOMER" = DBT_INTERNAL_SOURCE."IS_GUEST_CUSTOMER","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","PROCESSED_AT" = DBT_INTERNAL_SOURCE."PROCESSED_AT"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m11:51:59.133428 [debug] [Thread-4 (]: SQL status: SUCCESS 1434 in 1.002 seconds
[0m11:51:59.133428 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:51:59.133428 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:51:59.451197 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.309 seconds
[0m11:51:59.469003 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._processed.processed_sales__dbt_tmp
[0m11:51:59.469003 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:51:59.469003 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: drop view if exists INCREMENTALETL._processed.processed_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:51:59.669591 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.187 seconds
[0m11:51:59.682134 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7f488c90-0ef2-4d43-926d-bb0bdca941cd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000200FFB3C520>]}
[0m11:51:59.687208 [info ] [Thread-4 (]: 2 of 6 OK created sql incremental model _processed.processed_sales ............. [[32mSUCCESS 1434[0m in 3.53s]
[0m11:51:59.691247 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.processed_sales
[0m11:51:59.697302 [debug] [Thread-3 (]: Began running node model.sales_pipelines.dim_customers
[0m11:51:59.697302 [debug] [Thread-2 (]: Began running node model.sales_pipelines.dim_date
[0m11:51:59.701622 [debug] [Thread-5 (]: Began running node model.sales_pipelines.dim_products
[0m11:51:59.701622 [debug] [Thread-4 (]: Began running node model.sales_pipelines.fact_sales
[0m11:51:59.704492 [info ] [Thread-3 (]: 3 of 6 START sql table model _marts.dim_customers .............................. [RUN]
[0m11:51:59.707336 [info ] [Thread-2 (]: 4 of 6 START sql table model _marts.dim_date ................................... [RUN]
[0m11:51:59.710573 [info ] [Thread-5 (]: 5 of 6 START sql table model _marts.dim_products ............................... [RUN]
[0m11:51:59.712220 [info ] [Thread-4 (]: 6 of 6 START sql incremental model _marts.fact_sales ........................... [RUN]
[0m11:51:59.712220 [debug] [Thread-3 (]: Acquiring new snowflake connection 'model.sales_pipelines.dim_customers'
[0m11:51:59.716176 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.sales_pipelines.stg_sales, now model.sales_pipelines.dim_date)
[0m11:51:59.716176 [debug] [Thread-5 (]: Acquiring new snowflake connection 'model.sales_pipelines.dim_products'
[0m11:51:59.720322 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.sales_pipelines.processed_sales, now model.sales_pipelines.fact_sales)
[0m11:51:59.721952 [debug] [Thread-3 (]: Began compiling node model.sales_pipelines.dim_customers
[0m11:51:59.723195 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.dim_date
[0m11:51:59.723195 [debug] [Thread-5 (]: Began compiling node model.sales_pipelines.dim_products
[0m11:51:59.726118 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.fact_sales
[0m11:51:59.733397 [debug] [Thread-3 (]: Writing injected SQL for node "model.sales_pipelines.dim_customers"
[0m11:51:59.739858 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.dim_date"
[0m11:51:59.747273 [debug] [Thread-5 (]: Writing injected SQL for node "model.sales_pipelines.dim_products"
[0m11:51:59.750407 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.fact_sales"
[0m11:51:59.756614 [debug] [Thread-3 (]: Began executing node model.sales_pipelines.dim_customers
[0m11:51:59.759297 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.dim_date
[0m11:51:59.759297 [debug] [Thread-5 (]: Began executing node model.sales_pipelines.dim_products
[0m11:51:59.795944 [debug] [Thread-3 (]: Writing runtime sql for node "model.sales_pipelines.dim_customers"
[0m11:51:59.804040 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.fact_sales
[0m11:51:59.806219 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.dim_date"
[0m11:51:59.811005 [debug] [Thread-5 (]: Writing runtime sql for node "model.sales_pipelines.dim_products"
[0m11:51:59.820871 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:51:59.820871 [debug] [Thread-3 (]: Using snowflake connection "model.sales_pipelines.dim_customers"
[0m11:51:59.820871 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.dim_date"
[0m11:51:59.829073 [debug] [Thread-5 (]: Using snowflake connection "model.sales_pipelines.dim_products"
[0m11:51:59.820871 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: create or replace  temporary view INCREMENTALETL._marts.fact_sales__dbt_tmp
  
  
  
  
  as (
    

SELECT
    invoice_no,
    stock_code,
    customer_id,
    invoice_date::DATE AS date_key,

    quantity,
    unit_price,
    discount,
    shipping_cost,

    gross_amount,
    net_amount,
    total_amount,

    is_returned,
    processed_at

FROM INCREMENTALETL._processed.processed_sales


WHERE processed_at > (SELECT MAX(processed_at) FROM INCREMENTALETL._marts.fact_sales)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */;
[0m11:51:59.830307 [debug] [Thread-3 (]: On model.sales_pipelines.dim_customers: create or replace transient table INCREMENTALETL._marts.dim_customers
    
    
    
    as (

SELECT DISTINCT
    customer_id,
    country,
    payment_method,
    sales_channel,
    is_guest_customer,
    MIN(invoice_date) AS first_purchase_date,
    MAX(invoice_date) AS last_purchase_date,
    COUNT(DISTINCT invoice_no) AS total_orders
FROM INCREMENTALETL._processed.processed_sales
WHERE customer_id IS NOT NULL
GROUP BY
    customer_id,
    country,
    payment_method,
    sales_channel,
    is_guest_customer
    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.dim_customers"} */;
[0m11:51:59.830307 [debug] [Thread-2 (]: On model.sales_pipelines.dim_date: create or replace transient table INCREMENTALETL._marts.dim_date
    
    
    
    as (

SELECT DISTINCT
    invoice_date::DATE AS date_key,
    YEAR(invoice_date)  AS year,
    MONTH(invoice_date) AS month,
    DAY(invoice_date)   AS day,
    DAYNAME(invoice_date) AS day_name,
    WEEK(invoice_date)  AS week_number,
    QUARTER(invoice_date) AS quarter
FROM INCREMENTALETL._processed.processed_sales
    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.dim_date"} */;
[0m11:51:59.830307 [debug] [Thread-5 (]: On model.sales_pipelines.dim_products: create or replace transient table INCREMENTALETL._marts.dim_products
    
    
    
    as (

SELECT DISTINCT
    stock_code,
    description,
    category
FROM INCREMENTALETL._processed.processed_sales
    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.dim_products"} */;
[0m11:51:59.830307 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m11:51:59.838860 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m11:52:00.100033 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.276 seconds
[0m11:52:00.115974 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:52:00.115974 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: describe table INCREMENTALETL._marts.fact_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:52:00.290947 [debug] [Thread-4 (]: SQL status: SUCCESS 13 in 0.164 seconds
[0m11:52:00.314247 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:52:00.314247 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: describe table INCREMENTALETL._marts.fact_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:52:00.463905 [debug] [Thread-4 (]: SQL status: SUCCESS 13 in 0.150 seconds
[0m11:52:00.490366 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:52:00.490366 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: describe table "INCREMENTALETL"."_MARTS"."FACT_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:52:00.572502 [debug] [Thread-2 (]: SQL status: SUCCESS 1674 in 0.746 seconds
[0m11:52:00.608516 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7f488c90-0ef2-4d43-926d-bb0bdca941cd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000200813C6140>]}
[0m11:52:00.608516 [info ] [Thread-2 (]: 4 of 6 OK created sql table model _marts.dim_date .............................. [[32mSUCCESS 1674[0m in 0.89s]
[0m11:52:00.608516 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.dim_date
[0m11:52:00.672804 [debug] [Thread-4 (]: SQL status: SUCCESS 13 in 0.177 seconds
[0m11:52:00.675865 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.fact_sales"
[0m11:52:00.689880 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:52:00.689880 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */;
[0m11:52:00.870524 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.179 seconds
[0m11:52:00.875176 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:52:00.875176 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: merge into INCREMENTALETL._marts.fact_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._marts.fact_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","DATE_KEY" = DBT_INTERNAL_SOURCE."DATE_KEY","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","GROSS_AMOUNT" = DBT_INTERNAL_SOURCE."GROSS_AMOUNT","NET_AMOUNT" = DBT_INTERNAL_SOURCE."NET_AMOUNT","TOTAL_AMOUNT" = DBT_INTERNAL_SOURCE."TOTAL_AMOUNT","IS_RETURNED" = DBT_INTERNAL_SOURCE."IS_RETURNED","PROCESSED_AT" = DBT_INTERNAL_SOURCE."PROCESSED_AT"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "CUSTOMER_ID", "DATE_KEY", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "PROCESSED_AT")
    values
        ("INVOICE_NO", "STOCK_CODE", "CUSTOMER_ID", "DATE_KEY", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "PROCESSED_AT")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */;
[0m11:52:01.135925 [debug] [Thread-5 (]: SQL status: SUCCESS 28031 in 1.296 seconds
[0m11:52:01.148069 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7f488c90-0ef2-4d43-926d-bb0bdca941cd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000200814089A0>]}
[0m11:52:01.151870 [info ] [Thread-5 (]: 5 of 6 OK created sql table model _marts.dim_products .......................... [[32mSUCCESS 28031[0m in 1.43s]
[0m11:52:01.155737 [debug] [Thread-5 (]: Finished running node model.sales_pipelines.dim_products
[0m11:52:01.231983 [debug] [Thread-3 (]: SQL status: SUCCESS 36084 in 1.396 seconds
[0m11:52:01.238963 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7f488c90-0ef2-4d43-926d-bb0bdca941cd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000200FFBFB6A0>]}
[0m11:52:01.247326 [info ] [Thread-3 (]: 3 of 6 OK created sql table model _marts.dim_customers ......................... [[32mSUCCESS 36084[0m in 1.53s]
[0m11:52:01.254048 [debug] [Thread-3 (]: Finished running node model.sales_pipelines.dim_customers
[0m11:52:01.605252 [debug] [Thread-4 (]: SQL status: SUCCESS 1434 in 0.726 seconds
[0m11:52:01.612093 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:52:01.612093 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:52:02.205910 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.602 seconds
[0m11:52:02.238143 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._marts.fact_sales__dbt_tmp
[0m11:52:02.238143 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:52:02.238143 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: drop view if exists INCREMENTALETL._marts.fact_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:52:02.437442 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.194 seconds
[0m11:52:02.447190 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7f488c90-0ef2-4d43-926d-bb0bdca941cd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002008139DA50>]}
[0m11:52:02.447190 [info ] [Thread-4 (]: 6 of 6 OK created sql incremental model _marts.fact_sales ...................... [[32mSUCCESS 1434[0m in 2.73s]
[0m11:52:02.464424 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.fact_sales
[0m11:52:02.464424 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:52:02.464424 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m11:52:02.464424 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m11:52:02.770830 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m11:52:02.770830 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m11:52:03.241068 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m11:52:03.241068 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m11:52:03.565277 [debug] [MainThread]: Connection 'list_INCREMENTALETL__staging' was left open.
[0m11:52:03.565277 [debug] [MainThread]: On list_INCREMENTALETL__staging: Close
[0m11:52:04.018301 [debug] [MainThread]: Connection 'list_INCREMENTALETL__processed' was left open.
[0m11:52:04.018301 [debug] [MainThread]: On list_INCREMENTALETL__processed: Close
[0m11:52:04.387032 [debug] [MainThread]: Connection 'list_INCREMENTALETL__marts' was left open.
[0m11:52:04.387032 [debug] [MainThread]: On list_INCREMENTALETL__marts: Close
[0m11:52:04.697603 [debug] [MainThread]: Connection 'model.sales_pipelines.dim_date' was left open.
[0m11:52:04.697603 [debug] [MainThread]: On model.sales_pipelines.dim_date: Close
[0m11:52:04.982892 [debug] [MainThread]: Connection 'model.sales_pipelines.fact_sales' was left open.
[0m11:52:04.995168 [debug] [MainThread]: On model.sales_pipelines.fact_sales: Close
[0m11:52:05.402537 [debug] [MainThread]: Connection 'model.sales_pipelines.dim_customers' was left open.
[0m11:52:05.402537 [debug] [MainThread]: On model.sales_pipelines.dim_customers: Close
[0m11:52:05.720364 [debug] [MainThread]: Connection 'model.sales_pipelines.dim_products' was left open.
[0m11:52:05.720364 [debug] [MainThread]: On model.sales_pipelines.dim_products: Close
[0m11:52:06.114603 [info ] [MainThread]: 
[0m11:52:06.114603 [info ] [MainThread]: Finished running 3 incremental models, 3 table models in 0 hours 0 minutes and 16.72 seconds (16.72s).
[0m11:52:06.120657 [debug] [MainThread]: Command end result
[0m11:52:06.174446 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m11:52:06.174446 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m11:52:06.197180 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\snowflake-incremental-pipeline\dbt_project\target\run_results.json
[0m11:52:06.197180 [info ] [MainThread]: 
[0m11:52:06.200354 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:52:06.202240 [info ] [MainThread]: 
[0m11:52:06.202240 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=6
[0m11:52:06.204163 [debug] [MainThread]: Command `dbt run` succeeded at 11:52:06.204163 after 19.31 seconds
[0m11:52:06.206342 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000200EC147640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000200FF3D3400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002008148D240>]}
[0m11:52:06.207321 [debug] [MainThread]: Flushing usage events
[0m11:52:07.456147 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:52:22.557681 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C3E62C76D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C3E7572620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C3E75730D0>]}


============================== 11:52:22.563662 | 63a1eb50-f5b7-4487-9c92-fce4870bb411 ==============================
[0m11:52:22.563662 [info ] [MainThread]: Running with dbt=1.11.6
[0m11:52:22.563662 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'profiles_dir': 'D:\\snowflake-incremental-pipeline\\dbt_project', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'use_colors': 'True', 'indirect_selection': 'eager', 'invocation_command': 'dbt run', 'use_experimental_parser': 'False', 'log_path': 'D:\\snowflake-incremental-pipeline\\dbt_project\\logs'}
[0m11:52:23.554165 [debug] [MainThread]: Snowflake adapter: Setting snowflake.connector to ERROR (file logging only)
[0m11:52:23.557115 [debug] [MainThread]: Snowflake adapter: Setting botocore to ERROR (file logging only)
[0m11:52:23.557115 [debug] [MainThread]: Snowflake adapter: Setting boto3 to ERROR (file logging only)
[0m11:52:23.898566 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '63a1eb50-f5b7-4487-9c92-fce4870bb411', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C3E406EB00>]}
[0m11:52:23.985002 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '63a1eb50-f5b7-4487-9c92-fce4870bb411', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C3E6D3B1C0>]}
[0m11:52:23.985002 [info ] [MainThread]: Registered adapter: snowflake=1.11.2
[0m11:52:24.532779 [debug] [MainThread]: checksum: ec27062d18352e9cfca161c166fcc73d2201661a23a3cf3c89196a27f32cfcb8, vars: {}, profile: , target: , version: 1.11.6
[0m11:52:24.766205 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:52:24.766205 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m11:52:24.766205 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:52:24.845717 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '63a1eb50-f5b7-4487-9c92-fce4870bb411', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C3F9C78A60>]}
[0m11:52:24.988424 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m11:52:24.988424 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m11:52:25.016047 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '63a1eb50-f5b7-4487-9c92-fce4870bb411', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C3F9BF1600>]}
[0m11:52:25.016047 [info ] [MainThread]: Found 6 models, 6 data tests, 1 source, 522 macros
[0m11:52:25.021526 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '63a1eb50-f5b7-4487-9c92-fce4870bb411', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C3F9C789A0>]}
[0m11:52:25.023414 [info ] [MainThread]: 
[0m11:52:25.023414 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m11:52:25.026514 [info ] [MainThread]: 
[0m11:52:25.027676 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m11:52:25.036978 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m11:52:25.042308 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m11:52:25.059756 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m11:52:25.145813 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m11:52:25.145813 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m11:52:25.145813 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m11:52:25.145813 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m11:52:25.145813 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m11:52:25.145813 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m11:52:25.145813 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:52:25.145813 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:52:25.145813 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:52:26.337221 [debug] [ThreadPool]: SQL status: SUCCESS 5 in 1.191 seconds
[0m11:52:26.337221 [debug] [ThreadPool]: SQL status: SUCCESS 5 in 1.195 seconds
[0m11:52:26.350392 [debug] [ThreadPool]: SQL status: SUCCESS 5 in 1.212 seconds
[0m11:52:26.380252 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__staging'
[0m11:52:26.384307 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__processed'
[0m11:52:26.413821 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__marts'
[0m11:52:26.425123 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__staging"
[0m11:52:26.432378 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__processed"
[0m11:52:26.440677 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__marts"
[0m11:52:26.442724 [debug] [ThreadPool]: On list_INCREMENTALETL__staging: show objects in INCREMENTALETL._staging
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__staging"} */;
[0m11:52:26.445034 [debug] [ThreadPool]: On list_INCREMENTALETL__processed: show objects in INCREMENTALETL._processed
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__processed"} */;
[0m11:52:26.446042 [debug] [ThreadPool]: On list_INCREMENTALETL__marts: show objects in INCREMENTALETL._marts
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__marts"} */;
[0m11:52:26.447081 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:52:26.447081 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:52:26.447081 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:52:27.573016 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 1.132 seconds
[0m11:52:27.591151 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 1.148 seconds
[0m11:52:27.601282 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 1.161 seconds
[0m11:52:27.617069 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '63a1eb50-f5b7-4487-9c92-fce4870bb411', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C3F8737C10>]}
[0m11:52:27.632800 [debug] [Thread-2 (]: Began running node model.sales_pipelines.stg_sales
[0m11:52:27.632800 [info ] [Thread-2 (]: 1 of 6 START sql incremental model _staging.stg_sales .......................... [RUN]
[0m11:52:27.638240 [debug] [Thread-2 (]: Acquiring new snowflake connection 'model.sales_pipelines.stg_sales'
[0m11:52:27.638240 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.stg_sales
[0m11:52:27.674214 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.stg_sales"
[0m11:52:27.674214 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.stg_sales
[0m11:52:27.870922 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:52:27.870922 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: create or replace  temporary view INCREMENTALETL._staging.stg_sales__dbt_tmp
  
  
  
  
  as (
    

WITH raw_data AS (
    SELECT * 
    FROM INCREMENTALETL.LANDINGZONE.RAW_SALES
),

staged AS (
    SELECT
        -- IDs
        CAST(INVOICENO AS VARCHAR)    AS invoice_no,
        CAST(STOCKCODE AS VARCHAR)    AS stock_code,
        CAST(DESCRIPTION AS VARCHAR)  AS description,

        -- Quantities & Prices
        TRY_CAST(QUANTITY AS INT)      AS quantity,
        TRY_CAST(UNITPRICE AS FLOAT)   AS unit_price,
        TRY_CAST(DISCOUNT AS FLOAT)    AS discount,
        TRY_CAST(SHIPPINGCOST AS FLOAT) AS shipping_cost,
        TRY_CAST(CUSTOMERID AS FLOAT)  AS customer_id,

        -- Convert once
        TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS') AS invoice_date,

        TRIM(UPPER(COUNTRY))            AS country,
        TRIM(UPPER(PAYMENTMETHOD))      AS payment_method,
        TRIM(UPPER(CATEGORY))           AS category,
        TRIM(UPPER(SALESCHANNEL))       AS sales_channel,
        TRIM(UPPER(RETURNSTATUS))       AS return_status,
        TRIM(UPPER(SHIPMENTPROVIDER))   AS shipment_provider,
        TRIM(UPPER(WAREHOUSELOCATION))  AS warehouse_location,
        TRIM(UPPER(ORDERPRIORITY))      AS order_priority,

        CURRENT_TIMESTAMP()             AS stg_loaded_at

    FROM raw_data
),

validated AS (
    SELECT
        *,
        YEAR(invoice_date)  AS invoice_year,
        MONTH(invoice_date) AS invoice_month,

        CASE
            WHEN invoice_date IS NULL           THEN 'INVALID_DATE'
            WHEN quantity IS NULL               THEN 'INVALID_QUANTITY'
            WHEN unit_price IS NULL             THEN 'INVALID_PRICE'
            WHEN shipping_cost < 0              THEN 'NEGATIVE_SHIPPING'
            WHEN discount < 0 OR discount > 1   THEN 'INVALID_DISCOUNT'
            WHEN invoice_no IS NULL             THEN 'NULL_INVOICE'
            ELSE 'VALID'
        END AS data_quality_flag
    FROM staged
)

SELECT *
FROM validated


WHERE NOT EXISTS (
    SELECT 1
    FROM INCREMENTALETL._staging.stg_sales t
    WHERE t.invoice_year  = validated.invoice_year
      AND t.invoice_month = validated.invoice_month
)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m11:52:27.870922 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m11:52:28.802973 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.932 seconds
[0m11:52:28.839107 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:52:28.839107 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:52:29.054309 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.210 seconds
[0m11:52:29.077392 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:52:29.077392 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:52:29.354550 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.275 seconds
[0m11:52:29.406686 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:52:29.406686 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:52:29.557454 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.154 seconds
[0m11:52:29.587345 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:52:29.590695 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table "INCREMENTALETL"."_STAGING"."STG_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:52:29.743493 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.150 seconds
[0m11:52:29.791298 [debug] [Thread-2 (]: 
    In "INCREMENTALETL"."_STAGING"."STG_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m11:52:29.823141 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.stg_sales"
[0m11:52:29.823141 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:52:29.823141 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m11:52:29.989500 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.163 seconds
[0m11:52:29.989500 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:52:30.001155 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: merge into INCREMENTALETL._staging.stg_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._staging.stg_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","DATA_QUALITY_FLAG" = DBT_INTERNAL_SOURCE."DATA_QUALITY_FLAG"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "INVOICE_YEAR", "INVOICE_MONTH", "DATA_QUALITY_FLAG")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "INVOICE_YEAR", "INVOICE_MONTH", "DATA_QUALITY_FLAG")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m11:52:30.843839 [debug] [Thread-2 (]: SQL status: SUCCESS 744 in 0.839 seconds
[0m11:52:30.844968 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:52:30.844968 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:52:31.090694 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.242 seconds
[0m11:52:31.117363 [debug] [Thread-2 (]: Applying DROP to: INCREMENTALETL._staging.stg_sales__dbt_tmp
[0m11:52:31.133372 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:52:31.133372 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: drop view if exists INCREMENTALETL._staging.stg_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:52:31.325208 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.195 seconds
[0m11:52:31.380811 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '63a1eb50-f5b7-4487-9c92-fce4870bb411', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C3F9C13670>]}
[0m11:52:31.380811 [info ] [Thread-2 (]: 1 of 6 OK created sql incremental model _staging.stg_sales ..................... [[32mSUCCESS 744[0m in 3.75s]
[0m11:52:31.380811 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.stg_sales
[0m11:52:31.391394 [debug] [Thread-4 (]: Began running node model.sales_pipelines.processed_sales
[0m11:52:31.394579 [info ] [Thread-4 (]: 2 of 6 START sql incremental model _processed.processed_sales .................. [RUN]
[0m11:52:31.394579 [debug] [Thread-4 (]: Acquiring new snowflake connection 'model.sales_pipelines.processed_sales'
[0m11:52:31.394579 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.processed_sales
[0m11:52:31.399604 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.processed_sales"
[0m11:52:31.408418 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.processed_sales
[0m11:52:31.417350 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:52:31.420460 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: create or replace  temporary view INCREMENTALETL._processed.processed_sales__dbt_tmp
  
  
  
  
  as (
    

WITH staging AS (
    SELECT * 
    FROM INCREMENTALETL._staging.stg_sales
),

deduped AS (
    SELECT *,
        ROW_NUMBER() OVER (
            PARTITION BY invoice_no, stock_code
            ORDER BY stg_loaded_at DESC
        ) AS row_num
    FROM staging
),

cleaned AS (
    SELECT
        invoice_no,
        stock_code,
        description,
        quantity,
        unit_price,
        discount,
        shipping_cost,
        customer_id,
        invoice_date,
        invoice_year,
        invoice_month,
        country,
        payment_method,
        category,
        sales_channel,
        return_status,
        shipment_provider,
        warehouse_location,
        order_priority,

        -- Derived metrics
        ROUND(quantity * unit_price, 2) AS gross_amount,
        ROUND(quantity * unit_price * (1 - discount), 2) AS net_amount,
        ROUND(
            quantity * unit_price * (1 - discount)
            + COALESCE(shipping_cost, 0),
        2) AS total_amount,

        CASE WHEN return_status = 'RETURNED' THEN TRUE ELSE FALSE END AS is_returned,
        CASE WHEN customer_id IS NULL THEN TRUE ELSE FALSE END AS is_guest_customer,

        stg_loaded_at,
        CURRENT_TIMESTAMP() AS processed_at

    FROM deduped
    WHERE
        row_num = 1
        AND data_quality_flag = 'VALID'
        AND invoice_date IS NOT NULL
)

SELECT *
FROM cleaned


WHERE NOT EXISTS (
    SELECT 1
    FROM INCREMENTALETL._processed.processed_sales t
    WHERE t.invoice_year  = cleaned.invoice_year
      AND t.invoice_month = cleaned.invoice_month
)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m11:52:31.420460 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m11:52:32.325511 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.906 seconds
[0m11:52:32.344020 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:52:32.344020 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:52:32.501847 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.147 seconds
[0m11:52:32.524572 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:52:32.526435 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:52:32.675856 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.150 seconds
[0m11:52:32.705671 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:52:32.707828 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:52:32.851042 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.141 seconds
[0m11:52:32.878869 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:52:32.880883 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:52:33.031676 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.155 seconds
[0m11:52:33.075844 [debug] [Thread-4 (]: 
    In "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m11:52:33.087028 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.processed_sales"
[0m11:52:33.098014 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:52:33.100053 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m11:52:33.248988 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.149 seconds
[0m11:52:33.253171 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:52:33.253171 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: merge into INCREMENTALETL._processed.processed_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._processed.processed_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","GROSS_AMOUNT" = DBT_INTERNAL_SOURCE."GROSS_AMOUNT","NET_AMOUNT" = DBT_INTERNAL_SOURCE."NET_AMOUNT","TOTAL_AMOUNT" = DBT_INTERNAL_SOURCE."TOTAL_AMOUNT","IS_RETURNED" = DBT_INTERNAL_SOURCE."IS_RETURNED","IS_GUEST_CUSTOMER" = DBT_INTERNAL_SOURCE."IS_GUEST_CUSTOMER","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","PROCESSED_AT" = DBT_INTERNAL_SOURCE."PROCESSED_AT"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m11:52:34.270471 [debug] [Thread-4 (]: SQL status: SUCCESS 725 in 1.013 seconds
[0m11:52:34.270471 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:52:34.270471 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:52:34.689932 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.407 seconds
[0m11:52:34.707472 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._processed.processed_sales__dbt_tmp
[0m11:52:34.711274 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:52:34.715912 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: drop view if exists INCREMENTALETL._processed.processed_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:52:34.890250 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.173 seconds
[0m11:52:34.893339 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '63a1eb50-f5b7-4487-9c92-fce4870bb411', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C3F9CD50F0>]}
[0m11:52:34.893339 [info ] [Thread-4 (]: 2 of 6 OK created sql incremental model _processed.processed_sales ............. [[32mSUCCESS 725[0m in 3.50s]
[0m11:52:34.893339 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.processed_sales
[0m11:52:34.901167 [debug] [Thread-3 (]: Began running node model.sales_pipelines.dim_customers
[0m11:52:34.903572 [debug] [Thread-2 (]: Began running node model.sales_pipelines.dim_date
[0m11:52:34.903572 [debug] [Thread-5 (]: Began running node model.sales_pipelines.dim_products
[0m11:52:34.903572 [debug] [Thread-4 (]: Began running node model.sales_pipelines.fact_sales
[0m11:52:34.903572 [info ] [Thread-3 (]: 3 of 6 START sql table model _marts.dim_customers .............................. [RUN]
[0m11:52:34.908985 [info ] [Thread-2 (]: 4 of 6 START sql table model _marts.dim_date ................................... [RUN]
[0m11:52:34.910371 [info ] [Thread-5 (]: 5 of 6 START sql table model _marts.dim_products ............................... [RUN]
[0m11:52:34.913050 [info ] [Thread-4 (]: 6 of 6 START sql incremental model _marts.fact_sales ........................... [RUN]
[0m11:52:34.914209 [debug] [Thread-3 (]: Acquiring new snowflake connection 'model.sales_pipelines.dim_customers'
[0m11:52:34.917272 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.sales_pipelines.stg_sales, now model.sales_pipelines.dim_date)
[0m11:52:34.919901 [debug] [Thread-5 (]: Acquiring new snowflake connection 'model.sales_pipelines.dim_products'
[0m11:52:34.919901 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.sales_pipelines.processed_sales, now model.sales_pipelines.fact_sales)
[0m11:52:34.923282 [debug] [Thread-3 (]: Began compiling node model.sales_pipelines.dim_customers
[0m11:52:34.925299 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.dim_date
[0m11:52:34.961914 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.dim_date"
[0m11:52:34.965940 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.dim_date
[0m11:52:34.927315 [debug] [Thread-5 (]: Began compiling node model.sales_pipelines.dim_products
[0m11:52:34.929330 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.fact_sales
[0m11:52:34.999626 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.fact_sales"
[0m11:52:34.985234 [debug] [Thread-5 (]: Writing injected SQL for node "model.sales_pipelines.dim_products"
[0m11:52:35.003652 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.fact_sales
[0m11:52:35.005165 [debug] [Thread-5 (]: Began executing node model.sales_pipelines.dim_products
[0m11:52:34.948249 [debug] [Thread-3 (]: Writing injected SQL for node "model.sales_pipelines.dim_customers"
[0m11:52:35.077314 [debug] [Thread-5 (]: Writing runtime sql for node "model.sales_pipelines.dim_products"
[0m11:52:35.091520 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.dim_date"
[0m11:52:35.091520 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:52:35.093550 [debug] [Thread-3 (]: Began executing node model.sales_pipelines.dim_customers
[0m11:52:35.093550 [debug] [Thread-5 (]: Using snowflake connection "model.sales_pipelines.dim_products"
[0m11:52:35.096421 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: create or replace  temporary view INCREMENTALETL._marts.fact_sales__dbt_tmp
  
  
  
  
  as (
    

SELECT
    invoice_no,
    stock_code,
    customer_id,
    invoice_date::DATE AS date_key,

    quantity,
    unit_price,
    discount,
    shipping_cost,

    gross_amount,
    net_amount,
    total_amount,

    is_returned,
    processed_at

FROM INCREMENTALETL._processed.processed_sales


WHERE processed_at > (SELECT MAX(processed_at) FROM INCREMENTALETL._marts.fact_sales)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */;
[0m11:52:35.096421 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.dim_date"
[0m11:52:35.103109 [debug] [Thread-3 (]: Writing runtime sql for node "model.sales_pipelines.dim_customers"
[0m11:52:35.103109 [debug] [Thread-5 (]: On model.sales_pipelines.dim_products: create or replace transient table INCREMENTALETL._marts.dim_products
    
    
    
    as (

SELECT DISTINCT
    stock_code,
    description,
    category
FROM INCREMENTALETL._processed.processed_sales
    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.dim_products"} */;
[0m11:52:35.108770 [debug] [Thread-2 (]: On model.sales_pipelines.dim_date: create or replace transient table INCREMENTALETL._marts.dim_date
    
    
    
    as (

SELECT DISTINCT
    invoice_date::DATE AS date_key,
    YEAR(invoice_date)  AS year,
    MONTH(invoice_date) AS month,
    DAY(invoice_date)   AS day,
    DAYNAME(invoice_date) AS day_name,
    WEEK(invoice_date)  AS week_number,
    QUARTER(invoice_date) AS quarter
FROM INCREMENTALETL._processed.processed_sales
    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.dim_date"} */;
[0m11:52:35.109843 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m11:52:35.111050 [debug] [Thread-3 (]: Using snowflake connection "model.sales_pipelines.dim_customers"
[0m11:52:35.117100 [debug] [Thread-3 (]: On model.sales_pipelines.dim_customers: create or replace transient table INCREMENTALETL._marts.dim_customers
    
    
    
    as (

SELECT DISTINCT
    customer_id,
    country,
    payment_method,
    sales_channel,
    is_guest_customer,
    MIN(invoice_date) AS first_purchase_date,
    MAX(invoice_date) AS last_purchase_date,
    COUNT(DISTINCT invoice_no) AS total_orders
FROM INCREMENTALETL._processed.processed_sales
WHERE customer_id IS NOT NULL
GROUP BY
    customer_id,
    country,
    payment_method,
    sales_channel,
    is_guest_customer
    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.dim_customers"} */;
[0m11:52:35.118243 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m11:52:35.365648 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.260 seconds
[0m11:52:35.365648 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:52:35.365648 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: describe table INCREMENTALETL._marts.fact_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:52:35.524021 [debug] [Thread-4 (]: SQL status: SUCCESS 13 in 0.150 seconds
[0m11:52:35.534329 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:52:35.534329 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: describe table INCREMENTALETL._marts.fact_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:52:35.673755 [debug] [Thread-4 (]: SQL status: SUCCESS 13 in 0.136 seconds
[0m11:52:35.699069 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:52:35.702669 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: describe table "INCREMENTALETL"."_MARTS"."FACT_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:52:35.860828 [debug] [Thread-4 (]: SQL status: SUCCESS 13 in 0.155 seconds
[0m11:52:35.891558 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.fact_sales"
[0m11:52:35.891558 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:52:35.891558 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */;
[0m11:52:36.070670 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.169 seconds
[0m11:52:36.073983 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:52:36.075705 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: merge into INCREMENTALETL._marts.fact_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._marts.fact_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","DATE_KEY" = DBT_INTERNAL_SOURCE."DATE_KEY","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","GROSS_AMOUNT" = DBT_INTERNAL_SOURCE."GROSS_AMOUNT","NET_AMOUNT" = DBT_INTERNAL_SOURCE."NET_AMOUNT","TOTAL_AMOUNT" = DBT_INTERNAL_SOURCE."TOTAL_AMOUNT","IS_RETURNED" = DBT_INTERNAL_SOURCE."IS_RETURNED","PROCESSED_AT" = DBT_INTERNAL_SOURCE."PROCESSED_AT"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "CUSTOMER_ID", "DATE_KEY", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "PROCESSED_AT")
    values
        ("INVOICE_NO", "STOCK_CODE", "CUSTOMER_ID", "DATE_KEY", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "PROCESSED_AT")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */;
[0m11:52:36.430964 [debug] [Thread-5 (]: SQL status: SUCCESS 28379 in 1.321 seconds
[0m11:52:36.449282 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '63a1eb50-f5b7-4487-9c92-fce4870bb411', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C3F9CD0A60>]}
[0m11:52:36.449282 [info ] [Thread-5 (]: 5 of 6 OK created sql table model _marts.dim_products .......................... [[32mSUCCESS 28379[0m in 1.53s]
[0m11:52:36.465159 [debug] [Thread-5 (]: Finished running node model.sales_pipelines.dim_products
[0m11:52:36.527501 [debug] [Thread-3 (]: SQL status: SUCCESS 36754 in 1.411 seconds
[0m11:52:36.527501 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '63a1eb50-f5b7-4487-9c92-fce4870bb411', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C3FA589B70>]}
[0m11:52:36.537661 [info ] [Thread-3 (]: 3 of 6 OK created sql table model _marts.dim_customers ......................... [[32mSUCCESS 36754[0m in 1.61s]
[0m11:52:36.541127 [debug] [Thread-3 (]: Finished running node model.sales_pipelines.dim_customers
[0m11:52:36.666584 [debug] [Thread-4 (]: SQL status: SUCCESS 725 in 0.590 seconds
[0m11:52:36.666584 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:52:36.666584 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:52:36.920445 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.245 seconds
[0m11:52:36.941363 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._marts.fact_sales__dbt_tmp
[0m11:52:36.952457 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:52:36.954037 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: drop view if exists INCREMENTALETL._marts.fact_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:52:37.125198 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.168 seconds
[0m11:52:37.137964 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '63a1eb50-f5b7-4487-9c92-fce4870bb411', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C3E415F8E0>]}
[0m11:52:37.142094 [info ] [Thread-4 (]: 6 of 6 OK created sql incremental model _marts.fact_sales ...................... [[32mSUCCESS 725[0m in 2.22s]
[0m11:52:37.142094 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.fact_sales
[0m11:52:40.721599 [debug] [Thread-2 (]: SQL status: SUCCESS 1705 in 5.613 seconds
[0m11:52:40.734454 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '63a1eb50-f5b7-4487-9c92-fce4870bb411', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C3FA589420>]}
[0m11:52:40.734454 [info ] [Thread-2 (]: 4 of 6 OK created sql table model _marts.dim_date .............................. [[32mSUCCESS 1705[0m in 5.82s]
[0m11:52:40.734454 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.dim_date
[0m11:52:40.755045 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:52:40.757818 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m11:52:40.760005 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m11:52:41.051812 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m11:52:41.059425 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m11:52:41.296976 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m11:52:41.296976 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m11:52:41.644470 [debug] [MainThread]: Connection 'list_INCREMENTALETL__staging' was left open.
[0m11:52:41.644470 [debug] [MainThread]: On list_INCREMENTALETL__staging: Close
[0m11:52:41.941232 [debug] [MainThread]: Connection 'list_INCREMENTALETL__processed' was left open.
[0m11:52:41.954597 [debug] [MainThread]: On list_INCREMENTALETL__processed: Close
[0m11:52:42.230627 [debug] [MainThread]: Connection 'list_INCREMENTALETL__marts' was left open.
[0m11:52:42.230627 [debug] [MainThread]: On list_INCREMENTALETL__marts: Close
[0m11:52:42.536233 [debug] [MainThread]: Connection 'model.sales_pipelines.dim_date' was left open.
[0m11:52:42.536233 [debug] [MainThread]: On model.sales_pipelines.dim_date: Close
[0m11:52:42.792929 [debug] [MainThread]: Connection 'model.sales_pipelines.fact_sales' was left open.
[0m11:52:42.792929 [debug] [MainThread]: On model.sales_pipelines.fact_sales: Close
[0m11:52:43.033799 [debug] [MainThread]: Connection 'model.sales_pipelines.dim_customers' was left open.
[0m11:52:43.033799 [debug] [MainThread]: On model.sales_pipelines.dim_customers: Close
[0m11:52:43.388307 [debug] [MainThread]: Connection 'model.sales_pipelines.dim_products' was left open.
[0m11:52:43.388307 [debug] [MainThread]: On model.sales_pipelines.dim_products: Close
[0m11:52:43.630612 [info ] [MainThread]: 
[0m11:52:43.630612 [info ] [MainThread]: Finished running 3 incremental models, 3 table models in 0 hours 0 minutes and 18.60 seconds (18.60s).
[0m11:52:43.640191 [debug] [MainThread]: Command end result
[0m11:52:43.691882 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m11:52:43.703623 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m11:52:43.708298 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\snowflake-incremental-pipeline\dbt_project\target\run_results.json
[0m11:52:43.708298 [info ] [MainThread]: 
[0m11:52:43.720263 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:52:43.721125 [info ] [MainThread]: 
[0m11:52:43.722401 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=6
[0m11:52:43.726161 [debug] [MainThread]: Command `dbt run` succeeded at 11:52:43.726161 after 21.29 seconds
[0m11:52:43.727271 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C3E62C76D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C3F9C61C90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C3FA607F70>]}
[0m11:52:43.727271 [debug] [MainThread]: Flushing usage events
[0m11:52:45.631909 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:53:56.892978 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C82DA76A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C85021000>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C850223B0>]}


============================== 11:53:56.892978 | 7aac2662-7c7b-418e-8f78-9f974629a9e1 ==============================
[0m11:53:56.892978 [info ] [MainThread]: Running with dbt=1.11.6
[0m11:53:56.892978 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'D:\\snowflake-incremental-pipeline\\dbt_project', 'invocation_command': 'dbt run', 'indirect_selection': 'eager', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'D:\\snowflake-incremental-pipeline\\dbt_project\\logs'}
[0m11:53:58.238412 [debug] [MainThread]: Snowflake adapter: Setting snowflake.connector to ERROR (file logging only)
[0m11:53:58.238412 [debug] [MainThread]: Snowflake adapter: Setting botocore to ERROR (file logging only)
[0m11:53:58.243538 [debug] [MainThread]: Snowflake adapter: Setting boto3 to ERROR (file logging only)
[0m11:53:58.671455 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7aac2662-7c7b-418e-8f78-9f974629a9e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C850652A0>]}
[0m11:53:58.775416 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7aac2662-7c7b-418e-8f78-9f974629a9e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C850894B0>]}
[0m11:53:58.775416 [info ] [MainThread]: Registered adapter: snowflake=1.11.2
[0m11:53:59.466946 [debug] [MainThread]: checksum: ec27062d18352e9cfca161c166fcc73d2201661a23a3cf3c89196a27f32cfcb8, vars: {}, profile: , target: , version: 1.11.6
[0m11:53:59.775296 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:53:59.776444 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m11:53:59.777561 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:53:59.871130 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7aac2662-7c7b-418e-8f78-9f974629a9e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C97728A30>]}
[0m11:54:00.069375 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m11:54:00.070725 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m11:54:00.093526 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7aac2662-7c7b-418e-8f78-9f974629a9e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C97728E20>]}
[0m11:54:00.093526 [info ] [MainThread]: Found 6 models, 6 data tests, 1 source, 522 macros
[0m11:54:00.093526 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7aac2662-7c7b-418e-8f78-9f974629a9e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C97729480>]}
[0m11:54:00.104618 [info ] [MainThread]: 
[0m11:54:00.104618 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m11:54:00.108099 [info ] [MainThread]: 
[0m11:54:00.109713 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m11:54:00.122103 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m11:54:00.133262 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m11:54:00.162466 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m11:54:00.258683 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m11:54:00.258683 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m11:54:00.266871 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m11:54:00.266871 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m11:54:00.268022 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m11:54:00.268022 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m11:54:00.268022 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:54:00.271247 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:54:00.272508 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:54:01.535011 [debug] [ThreadPool]: SQL status: SUCCESS 5 in 1.265 seconds
[0m11:54:01.542632 [debug] [ThreadPool]: SQL status: SUCCESS 5 in 1.272 seconds
[0m11:54:14.579682 [debug] [ThreadPool]: SQL status: SUCCESS 5 in 14.309 seconds
[0m11:54:14.586087 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__marts'
[0m11:54:14.590115 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__processed'
[0m11:54:14.609232 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__staging'
[0m11:54:14.681981 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__processed"
[0m11:54:14.679877 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__marts"
[0m11:54:14.696596 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__staging"
[0m11:54:14.700319 [debug] [ThreadPool]: On list_INCREMENTALETL__processed: show objects in INCREMENTALETL._processed
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__processed"} */;
[0m11:54:14.700319 [debug] [ThreadPool]: On list_INCREMENTALETL__marts: show objects in INCREMENTALETL._marts
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__marts"} */;
[0m11:54:14.700319 [debug] [ThreadPool]: On list_INCREMENTALETL__staging: show objects in INCREMENTALETL._staging
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__staging"} */;
[0m11:54:14.700319 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:54:14.707164 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:54:14.707164 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:54:15.638796 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 0.933 seconds
[0m11:54:15.694783 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 1.000 seconds
[0m11:54:15.750516 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 1.046 seconds
[0m11:54:15.766967 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7aac2662-7c7b-418e-8f78-9f974629a9e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C978155A0>]}
[0m11:54:15.785296 [debug] [Thread-2 (]: Began running node model.sales_pipelines.stg_sales
[0m11:54:15.791111 [info ] [Thread-2 (]: 1 of 6 START sql incremental model _staging.stg_sales .......................... [RUN]
[0m11:54:15.799516 [debug] [Thread-2 (]: Acquiring new snowflake connection 'model.sales_pipelines.stg_sales'
[0m11:54:15.799516 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.stg_sales
[0m11:54:15.887553 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.stg_sales"
[0m11:54:15.893665 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.stg_sales
[0m11:54:15.984105 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:54:15.992228 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: create or replace  temporary view INCREMENTALETL._staging.stg_sales__dbt_tmp
  
  
  
  
  as (
    

WITH raw_data AS (
    SELECT * 
    FROM INCREMENTALETL.LANDINGZONE.RAW_SALES
),

staged AS (
    SELECT
        -- IDs
        CAST(INVOICENO AS VARCHAR)    AS invoice_no,
        CAST(STOCKCODE AS VARCHAR)    AS stock_code,
        CAST(DESCRIPTION AS VARCHAR)  AS description,

        -- Quantities & Prices
        TRY_CAST(QUANTITY AS INT)      AS quantity,
        TRY_CAST(UNITPRICE AS FLOAT)   AS unit_price,
        TRY_CAST(DISCOUNT AS FLOAT)    AS discount,
        TRY_CAST(SHIPPINGCOST AS FLOAT) AS shipping_cost,
        TRY_CAST(CUSTOMERID AS FLOAT)  AS customer_id,

        -- Convert once
        TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS') AS invoice_date,

        TRIM(UPPER(COUNTRY))            AS country,
        TRIM(UPPER(PAYMENTMETHOD))      AS payment_method,
        TRIM(UPPER(CATEGORY))           AS category,
        TRIM(UPPER(SALESCHANNEL))       AS sales_channel,
        TRIM(UPPER(RETURNSTATUS))       AS return_status,
        TRIM(UPPER(SHIPMENTPROVIDER))   AS shipment_provider,
        TRIM(UPPER(WAREHOUSELOCATION))  AS warehouse_location,
        TRIM(UPPER(ORDERPRIORITY))      AS order_priority,

        CURRENT_TIMESTAMP()             AS stg_loaded_at

    FROM raw_data
),

validated AS (
    SELECT
        *,
        YEAR(invoice_date)  AS invoice_year,
        MONTH(invoice_date) AS invoice_month,

        CASE
            WHEN invoice_date IS NULL           THEN 'INVALID_DATE'
            WHEN quantity IS NULL               THEN 'INVALID_QUANTITY'
            WHEN unit_price IS NULL             THEN 'INVALID_PRICE'
            WHEN shipping_cost < 0              THEN 'NEGATIVE_SHIPPING'
            WHEN discount < 0 OR discount > 1   THEN 'INVALID_DISCOUNT'
            WHEN invoice_no IS NULL             THEN 'NULL_INVOICE'
            ELSE 'VALID'
        END AS data_quality_flag
    FROM staged
)

SELECT *
FROM validated


WHERE NOT EXISTS (
    SELECT 1
    FROM INCREMENTALETL._staging.stg_sales t
    WHERE t.invoice_year  = validated.invoice_year
      AND t.invoice_month = validated.invoice_month
)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m11:54:15.993572 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m11:54:16.761557 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.773 seconds
[0m11:54:16.879373 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:54:16.879373 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:54:17.028476 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.148 seconds
[0m11:54:17.060023 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:54:17.060023 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:54:17.210026 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.152 seconds
[0m11:54:17.256560 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:54:17.256560 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:54:17.393014 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.129 seconds
[0m11:54:17.420142 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:54:17.420142 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table "INCREMENTALETL"."_STAGING"."STG_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:54:17.593757 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.167 seconds
[0m11:54:17.641381 [debug] [Thread-2 (]: 
    In "INCREMENTALETL"."_STAGING"."STG_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m11:54:17.673451 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.stg_sales"
[0m11:54:17.682584 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:54:17.682584 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m11:54:17.829644 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.149 seconds
[0m11:54:17.829644 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:54:17.837415 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: merge into INCREMENTALETL._staging.stg_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._staging.stg_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","DATA_QUALITY_FLAG" = DBT_INTERNAL_SOURCE."DATA_QUALITY_FLAG"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "INVOICE_YEAR", "INVOICE_MONTH", "DATA_QUALITY_FLAG")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "INVOICE_YEAR", "INVOICE_MONTH", "DATA_QUALITY_FLAG")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m11:54:18.713663 [debug] [Thread-2 (]: SQL status: SUCCESS 1464 in 0.878 seconds
[0m11:54:18.713663 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:54:18.713663 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:54:19.123823 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.406 seconds
[0m11:54:19.164603 [debug] [Thread-2 (]: Applying DROP to: INCREMENTALETL._staging.stg_sales__dbt_tmp
[0m11:54:19.164603 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:54:19.164603 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: drop view if exists INCREMENTALETL._staging.stg_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:54:19.336421 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.159 seconds
[0m11:54:19.382237 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7aac2662-7c7b-418e-8f78-9f974629a9e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C97FADAB0>]}
[0m11:54:19.382237 [info ] [Thread-2 (]: 1 of 6 OK created sql incremental model _staging.stg_sales ..................... [[32mSUCCESS 1464[0m in 3.59s]
[0m11:54:19.382237 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.stg_sales
[0m11:54:19.394191 [debug] [Thread-4 (]: Began running node model.sales_pipelines.processed_sales
[0m11:54:19.395606 [info ] [Thread-4 (]: 2 of 6 START sql incremental model _processed.processed_sales .................. [RUN]
[0m11:54:19.396817 [debug] [Thread-4 (]: Acquiring new snowflake connection 'model.sales_pipelines.processed_sales'
[0m11:54:19.398328 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.processed_sales
[0m11:54:19.411448 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.processed_sales"
[0m11:54:19.413534 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.processed_sales
[0m11:54:19.418809 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:54:19.423363 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: create or replace  temporary view INCREMENTALETL._processed.processed_sales__dbt_tmp
  
  
  
  
  as (
    

WITH staging AS (
    SELECT * 
    FROM INCREMENTALETL._staging.stg_sales
),

deduped AS (
    SELECT *,
        ROW_NUMBER() OVER (
            PARTITION BY invoice_no, stock_code
            ORDER BY stg_loaded_at DESC
        ) AS row_num
    FROM staging
),

cleaned AS (
    SELECT
        invoice_no,
        stock_code,
        description,
        quantity,
        unit_price,
        discount,
        shipping_cost,
        customer_id,
        invoice_date,
        invoice_year,
        invoice_month,
        country,
        payment_method,
        category,
        sales_channel,
        return_status,
        shipment_provider,
        warehouse_location,
        order_priority,

        -- Derived metrics
        ROUND(quantity * unit_price, 2) AS gross_amount,
        ROUND(quantity * unit_price * (1 - discount), 2) AS net_amount,
        ROUND(
            quantity * unit_price * (1 - discount)
            + COALESCE(shipping_cost, 0),
        2) AS total_amount,

        CASE WHEN return_status = 'RETURNED' THEN TRUE ELSE FALSE END AS is_returned,
        CASE WHEN customer_id IS NULL THEN TRUE ELSE FALSE END AS is_guest_customer,

        stg_loaded_at,
        CURRENT_TIMESTAMP() AS processed_at

    FROM deduped
    WHERE
        row_num = 1
        AND data_quality_flag = 'VALID'
        AND invoice_date IS NOT NULL
)

SELECT *
FROM cleaned


WHERE NOT EXISTS (
    SELECT 1
    FROM INCREMENTALETL._processed.processed_sales t
    WHERE t.invoice_year  = cleaned.invoice_year
      AND t.invoice_month = cleaned.invoice_month
)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m11:54:19.423363 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m11:54:20.277177 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.858 seconds
[0m11:54:20.286786 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:54:20.293820 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:54:20.448229 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.158 seconds
[0m11:54:20.478983 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:54:20.478983 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:54:20.645001 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.166 seconds
[0m11:54:20.675897 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:54:20.676743 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:54:20.865931 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.190 seconds
[0m11:54:20.893138 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:54:20.893138 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:54:21.047188 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.149 seconds
[0m11:54:21.063179 [debug] [Thread-4 (]: 
    In "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m11:54:21.079351 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.processed_sales"
[0m11:54:21.079351 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:54:21.079351 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m11:54:21.272334 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.185 seconds
[0m11:54:21.272334 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:54:21.272334 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: merge into INCREMENTALETL._processed.processed_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._processed.processed_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","GROSS_AMOUNT" = DBT_INTERNAL_SOURCE."GROSS_AMOUNT","NET_AMOUNT" = DBT_INTERNAL_SOURCE."NET_AMOUNT","TOTAL_AMOUNT" = DBT_INTERNAL_SOURCE."TOTAL_AMOUNT","IS_RETURNED" = DBT_INTERNAL_SOURCE."IS_RETURNED","IS_GUEST_CUSTOMER" = DBT_INTERNAL_SOURCE."IS_GUEST_CUSTOMER","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","PROCESSED_AT" = DBT_INTERNAL_SOURCE."PROCESSED_AT"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m11:54:22.511129 [debug] [Thread-4 (]: SQL status: SUCCESS 1446 in 1.223 seconds
[0m11:54:22.511129 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:54:22.511129 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:54:22.797065 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.281 seconds
[0m11:54:22.797065 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._processed.processed_sales__dbt_tmp
[0m11:54:22.797065 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:54:22.797065 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: drop view if exists INCREMENTALETL._processed.processed_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:54:23.049002 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.241 seconds
[0m11:54:23.057995 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7aac2662-7c7b-418e-8f78-9f974629a9e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C82BF8790>]}
[0m11:54:23.057995 [info ] [Thread-4 (]: 2 of 6 OK created sql incremental model _processed.processed_sales ............. [[32mSUCCESS 1446[0m in 3.66s]
[0m11:54:23.057995 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.processed_sales
[0m11:54:23.075084 [debug] [Thread-2 (]: Began running node model.sales_pipelines.dim_date
[0m11:54:23.076729 [debug] [Thread-5 (]: Began running node model.sales_pipelines.dim_products
[0m11:54:23.079015 [debug] [Thread-3 (]: Began running node model.sales_pipelines.dim_customers
[0m11:54:23.079015 [debug] [Thread-4 (]: Began running node model.sales_pipelines.fact_sales
[0m11:54:23.082186 [info ] [Thread-2 (]: 4 of 6 START sql table model _marts.dim_date ................................... [RUN]
[0m11:54:23.082186 [info ] [Thread-5 (]: 5 of 6 START sql table model _marts.dim_products ............................... [RUN]
[0m11:54:23.082186 [info ] [Thread-3 (]: 3 of 6 START sql table model _marts.dim_customers .............................. [RUN]
[0m11:54:23.089787 [info ] [Thread-4 (]: 6 of 6 START sql incremental model _marts.fact_sales ........................... [RUN]
[0m11:54:23.092449 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.sales_pipelines.stg_sales, now model.sales_pipelines.dim_date)
[0m11:54:23.094603 [debug] [Thread-5 (]: Acquiring new snowflake connection 'model.sales_pipelines.dim_products'
[0m11:54:23.098722 [debug] [Thread-3 (]: Acquiring new snowflake connection 'model.sales_pipelines.dim_customers'
[0m11:54:23.098722 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.sales_pipelines.processed_sales, now model.sales_pipelines.fact_sales)
[0m11:54:23.098722 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.dim_date
[0m11:54:23.102732 [debug] [Thread-5 (]: Began compiling node model.sales_pipelines.dim_products
[0m11:54:23.102732 [debug] [Thread-3 (]: Began compiling node model.sales_pipelines.dim_customers
[0m11:54:23.102732 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.fact_sales
[0m11:54:23.109157 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.dim_date"
[0m11:54:23.116375 [debug] [Thread-5 (]: Writing injected SQL for node "model.sales_pipelines.dim_products"
[0m11:54:23.126958 [debug] [Thread-3 (]: Writing injected SQL for node "model.sales_pipelines.dim_customers"
[0m11:54:23.128259 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.fact_sales"
[0m11:54:23.128259 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.dim_date
[0m11:54:23.128259 [debug] [Thread-5 (]: Began executing node model.sales_pipelines.dim_products
[0m11:54:23.142503 [debug] [Thread-3 (]: Began executing node model.sales_pipelines.dim_customers
[0m11:54:23.176923 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.dim_date"
[0m11:54:23.219232 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.dim_date"
[0m11:54:23.176923 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.fact_sales
[0m11:54:23.211180 [debug] [Thread-3 (]: Writing runtime sql for node "model.sales_pipelines.dim_customers"
[0m11:54:23.196582 [debug] [Thread-5 (]: Writing runtime sql for node "model.sales_pipelines.dim_products"
[0m11:54:23.221244 [debug] [Thread-2 (]: On model.sales_pipelines.dim_date: create or replace transient table INCREMENTALETL._marts.dim_date
    
    
    
    as (

SELECT DISTINCT
    invoice_date::DATE AS date_key,
    YEAR(invoice_date)  AS year,
    MONTH(invoice_date) AS month,
    DAY(invoice_date)   AS day,
    DAYNAME(invoice_date) AS day_name,
    WEEK(invoice_date)  AS week_number,
    QUARTER(invoice_date) AS quarter
FROM INCREMENTALETL._processed.processed_sales
    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.dim_date"} */;
[0m11:54:23.239350 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:54:23.242499 [debug] [Thread-5 (]: Using snowflake connection "model.sales_pipelines.dim_products"
[0m11:54:23.250545 [debug] [Thread-3 (]: Using snowflake connection "model.sales_pipelines.dim_customers"
[0m11:54:23.258592 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: create or replace  temporary view INCREMENTALETL._marts.fact_sales__dbt_tmp
  
  
  
  
  as (
    

SELECT
    invoice_no,
    stock_code,
    customer_id,
    invoice_date::DATE AS date_key,

    quantity,
    unit_price,
    discount,
    shipping_cost,

    gross_amount,
    net_amount,
    total_amount,

    is_returned,
    processed_at

FROM INCREMENTALETL._processed.processed_sales


WHERE processed_at > (SELECT MAX(processed_at) FROM INCREMENTALETL._marts.fact_sales)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */;
[0m11:54:23.260604 [debug] [Thread-5 (]: On model.sales_pipelines.dim_products: create or replace transient table INCREMENTALETL._marts.dim_products
    
    
    
    as (

SELECT DISTINCT
    stock_code,
    description,
    category
FROM INCREMENTALETL._processed.processed_sales
    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.dim_products"} */;
[0m11:54:23.260604 [debug] [Thread-3 (]: On model.sales_pipelines.dim_customers: create or replace transient table INCREMENTALETL._marts.dim_customers
    
    
    
    as (

SELECT DISTINCT
    customer_id,
    country,
    payment_method,
    sales_channel,
    is_guest_customer,
    MIN(invoice_date) AS first_purchase_date,
    MAX(invoice_date) AS last_purchase_date,
    COUNT(DISTINCT invoice_no) AS total_orders
FROM INCREMENTALETL._processed.processed_sales
WHERE customer_id IS NOT NULL
GROUP BY
    customer_id,
    country,
    payment_method,
    sales_channel,
    is_guest_customer
    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.dim_customers"} */;
[0m11:54:23.265213 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m11:54:23.265213 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m11:54:23.512298 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.253 seconds
[0m11:54:23.528147 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:54:23.528147 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: describe table INCREMENTALETL._marts.fact_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:54:23.700461 [debug] [Thread-4 (]: SQL status: SUCCESS 13 in 0.163 seconds
[0m11:54:23.710553 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:54:23.710553 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: describe table INCREMENTALETL._marts.fact_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:54:23.860304 [debug] [Thread-4 (]: SQL status: SUCCESS 13 in 0.143 seconds
[0m11:54:23.860304 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:54:23.860304 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: describe table "INCREMENTALETL"."_MARTS"."FACT_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:54:23.893606 [debug] [Thread-2 (]: SQL status: SUCCESS 1766 in 0.649 seconds
[0m11:54:23.905735 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7aac2662-7c7b-418e-8f78-9f974629a9e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C9782F190>]}
[0m11:54:23.910518 [info ] [Thread-2 (]: 4 of 6 OK created sql table model _marts.dim_date .............................. [[32mSUCCESS 1766[0m in 0.82s]
[0m11:54:23.910518 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.dim_date
[0m11:54:24.017552 [debug] [Thread-4 (]: SQL status: SUCCESS 13 in 0.143 seconds
[0m11:54:24.027204 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.fact_sales"
[0m11:54:24.034100 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:54:24.034100 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */;
[0m11:54:24.204195 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.167 seconds
[0m11:54:24.204195 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:54:24.204195 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: merge into INCREMENTALETL._marts.fact_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._marts.fact_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","DATE_KEY" = DBT_INTERNAL_SOURCE."DATE_KEY","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","GROSS_AMOUNT" = DBT_INTERNAL_SOURCE."GROSS_AMOUNT","NET_AMOUNT" = DBT_INTERNAL_SOURCE."NET_AMOUNT","TOTAL_AMOUNT" = DBT_INTERNAL_SOURCE."TOTAL_AMOUNT","IS_RETURNED" = DBT_INTERNAL_SOURCE."IS_RETURNED","PROCESSED_AT" = DBT_INTERNAL_SOURCE."PROCESSED_AT"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "CUSTOMER_ID", "DATE_KEY", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "PROCESSED_AT")
    values
        ("INVOICE_NO", "STOCK_CODE", "CUSTOMER_ID", "DATE_KEY", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "PROCESSED_AT")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */;
[0m11:54:25.318170 [debug] [Thread-3 (]: SQL status: SUCCESS 38068 in 2.063 seconds
[0m11:54:25.333572 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7aac2662-7c7b-418e-8f78-9f974629a9e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C97FAD2D0>]}
[0m11:54:25.333572 [info ] [Thread-3 (]: 3 of 6 OK created sql table model _marts.dim_customers ......................... [[32mSUCCESS 38068[0m in 2.24s]
[0m11:54:25.351653 [debug] [Thread-3 (]: Finished running node model.sales_pipelines.dim_customers
[0m11:54:25.378177 [debug] [Thread-4 (]: SQL status: SUCCESS 1446 in 1.171 seconds
[0m11:54:25.387491 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:54:25.391980 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:54:25.776664 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.392 seconds
[0m11:54:25.807351 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._marts.fact_sales__dbt_tmp
[0m11:54:25.808982 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:54:25.808982 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: drop view if exists INCREMENTALETL._marts.fact_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:54:25.994930 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.182 seconds
[0m11:54:26.016084 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7aac2662-7c7b-418e-8f78-9f974629a9e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C97F1F010>]}
[0m11:54:26.016084 [info ] [Thread-4 (]: 6 of 6 OK created sql incremental model _marts.fact_sales ...................... [[32mSUCCESS 1446[0m in 2.91s]
[0m11:54:26.025115 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.fact_sales
[0m11:54:31.715043 [debug] [Thread-5 (]: SQL status: SUCCESS 29051 in 8.459 seconds
[0m11:54:31.734365 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7aac2662-7c7b-418e-8f78-9f974629a9e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C97781DB0>]}
[0m11:54:31.743414 [info ] [Thread-5 (]: 5 of 6 OK created sql table model _marts.dim_products .......................... [[32mSUCCESS 29051[0m in 8.64s]
[0m11:54:31.749213 [debug] [Thread-5 (]: Finished running node model.sales_pipelines.dim_products
[0m11:54:31.754607 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:54:31.756814 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m11:54:31.757771 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m11:54:32.055821 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m11:54:32.055821 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m11:54:32.315203 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m11:54:32.315203 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m11:54:32.535568 [debug] [MainThread]: Connection 'list_INCREMENTALETL__marts' was left open.
[0m11:54:32.535568 [debug] [MainThread]: On list_INCREMENTALETL__marts: Close
[0m11:54:32.843259 [debug] [MainThread]: Connection 'list_INCREMENTALETL__processed' was left open.
[0m11:54:32.851217 [debug] [MainThread]: On list_INCREMENTALETL__processed: Close
[0m11:54:33.157880 [debug] [MainThread]: Connection 'list_INCREMENTALETL__staging' was left open.
[0m11:54:33.159889 [debug] [MainThread]: On list_INCREMENTALETL__staging: Close
[0m11:54:33.568625 [debug] [MainThread]: Connection 'model.sales_pipelines.dim_date' was left open.
[0m11:54:33.570527 [debug] [MainThread]: On model.sales_pipelines.dim_date: Close
[0m11:54:33.822979 [debug] [MainThread]: Connection 'model.sales_pipelines.fact_sales' was left open.
[0m11:54:33.822979 [debug] [MainThread]: On model.sales_pipelines.fact_sales: Close
[0m11:54:34.095128 [debug] [MainThread]: Connection 'model.sales_pipelines.dim_products' was left open.
[0m11:54:34.097145 [debug] [MainThread]: On model.sales_pipelines.dim_products: Close
[0m11:54:34.412767 [debug] [MainThread]: Connection 'model.sales_pipelines.dim_customers' was left open.
[0m11:54:34.414785 [debug] [MainThread]: On model.sales_pipelines.dim_customers: Close
[0m11:54:34.724352 [info ] [MainThread]: 
[0m11:54:34.724352 [info ] [MainThread]: Finished running 3 incremental models, 3 table models in 0 hours 0 minutes and 34.61 seconds (34.61s).
[0m11:54:34.730938 [debug] [MainThread]: Command end result
[0m11:54:34.796036 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m11:54:34.796036 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m11:54:34.811335 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\snowflake-incremental-pipeline\dbt_project\target\run_results.json
[0m11:54:34.813673 [info ] [MainThread]: 
[0m11:54:34.816508 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:54:34.818089 [info ] [MainThread]: 
[0m11:54:34.818089 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=6
[0m11:54:34.822881 [debug] [MainThread]: Command `dbt run` succeeded at 11:54:34.822881 after 38.12 seconds
[0m11:54:34.824936 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C82DA76A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C977120B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C964FBCD0>]}
[0m11:54:34.824936 [debug] [MainThread]: Flushing usage events
[0m11:54:36.612595 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:54:44.533254 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000215883876A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002158A605000>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002158A6063B0>]}


============================== 11:54:44.533254 | 9048e6bb-e0e5-45ed-b418-9c4089a0eea7 ==============================
[0m11:54:44.533254 [info ] [MainThread]: Running with dbt=1.11.6
[0m11:54:44.533254 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'D:\\snowflake-incremental-pipeline\\dbt_project', 'use_colors': 'True', 'invocation_command': 'dbt run', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'D:\\snowflake-incremental-pipeline\\dbt_project\\logs'}
[0m11:54:45.593752 [debug] [MainThread]: Snowflake adapter: Setting snowflake.connector to ERROR (file logging only)
[0m11:54:45.593752 [debug] [MainThread]: Snowflake adapter: Setting botocore to ERROR (file logging only)
[0m11:54:45.609710 [debug] [MainThread]: Snowflake adapter: Setting boto3 to ERROR (file logging only)
[0m11:54:45.954626 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9048e6bb-e0e5-45ed-b418-9c4089a0eea7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002158A6452A0>]}
[0m11:54:46.031937 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9048e6bb-e0e5-45ed-b418-9c4089a0eea7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002158A6694B0>]}
[0m11:54:46.031937 [info ] [MainThread]: Registered adapter: snowflake=1.11.2
[0m11:54:46.576950 [debug] [MainThread]: checksum: ec27062d18352e9cfca161c166fcc73d2201661a23a3cf3c89196a27f32cfcb8, vars: {}, profile: , target: , version: 1.11.6
[0m11:54:46.859739 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:54:46.860373 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m11:54:46.860373 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:54:46.946738 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9048e6bb-e0e5-45ed-b418-9c4089a0eea7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002159CD0CA30>]}
[0m11:54:47.110159 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m11:54:47.110159 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m11:54:47.136826 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9048e6bb-e0e5-45ed-b418-9c4089a0eea7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002159CD0CE20>]}
[0m11:54:47.136826 [info ] [MainThread]: Found 6 models, 6 data tests, 1 source, 522 macros
[0m11:54:47.136826 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9048e6bb-e0e5-45ed-b418-9c4089a0eea7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002159CD0D480>]}
[0m11:54:47.143787 [info ] [MainThread]: 
[0m11:54:47.145674 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m11:54:47.147230 [info ] [MainThread]: 
[0m11:54:47.147230 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m11:54:47.159722 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m11:54:47.178308 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m11:54:47.200214 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m11:54:47.289919 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m11:54:47.289919 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m11:54:47.289919 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m11:54:47.289919 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m11:54:47.292998 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m11:54:47.292998 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m11:54:47.294468 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:54:47.295216 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:54:47.295998 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:54:49.214580 [debug] [ThreadPool]: SQL status: SUCCESS 5 in 1.928 seconds
[0m11:54:49.230637 [debug] [ThreadPool]: SQL status: SUCCESS 5 in 1.948 seconds
[0m11:54:49.727809 [debug] [ThreadPool]: SQL status: SUCCESS 5 in 2.433 seconds
[0m11:54:49.739639 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__marts'
[0m11:54:49.759915 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__staging'
[0m11:54:49.791985 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__processed'
[0m11:54:49.823811 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__marts"
[0m11:54:49.823811 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__staging"
[0m11:54:49.839848 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__processed"
[0m11:54:49.839848 [debug] [ThreadPool]: On list_INCREMENTALETL__marts: show objects in INCREMENTALETL._marts
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__marts"} */;
[0m11:54:49.839848 [debug] [ThreadPool]: On list_INCREMENTALETL__staging: show objects in INCREMENTALETL._staging
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__staging"} */;
[0m11:54:49.855800 [debug] [ThreadPool]: On list_INCREMENTALETL__processed: show objects in INCREMENTALETL._processed
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__processed"} */;
[0m11:54:49.855800 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:54:49.855800 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:54:49.855800 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:54:50.722976 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.860 seconds
[0m11:54:50.736378 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.870 seconds
[0m11:54:50.786632 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 0.926 seconds
[0m11:54:50.795697 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9048e6bb-e0e5-45ed-b418-9c4089a0eea7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002159CDFB5E0>]}
[0m11:54:50.805268 [debug] [Thread-2 (]: Began running node model.sales_pipelines.stg_sales
[0m11:54:50.807280 [info ] [Thread-2 (]: 1 of 6 START sql incremental model _staging.stg_sales .......................... [RUN]
[0m11:54:50.809292 [debug] [Thread-2 (]: Acquiring new snowflake connection 'model.sales_pipelines.stg_sales'
[0m11:54:50.811304 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.stg_sales
[0m11:54:50.860927 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.stg_sales"
[0m11:54:50.862968 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.stg_sales
[0m11:54:50.992389 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:54:50.993064 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: create or replace  temporary view INCREMENTALETL._staging.stg_sales__dbt_tmp
  
  
  
  
  as (
    

WITH raw_data AS (
    SELECT * 
    FROM INCREMENTALETL.LANDINGZONE.RAW_SALES
),

staged AS (
    SELECT
        -- IDs
        CAST(INVOICENO AS VARCHAR)    AS invoice_no,
        CAST(STOCKCODE AS VARCHAR)    AS stock_code,
        CAST(DESCRIPTION AS VARCHAR)  AS description,

        -- Quantities & Prices
        TRY_CAST(QUANTITY AS INT)      AS quantity,
        TRY_CAST(UNITPRICE AS FLOAT)   AS unit_price,
        TRY_CAST(DISCOUNT AS FLOAT)    AS discount,
        TRY_CAST(SHIPPINGCOST AS FLOAT) AS shipping_cost,
        TRY_CAST(CUSTOMERID AS FLOAT)  AS customer_id,

        -- Convert once
        TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS') AS invoice_date,

        TRIM(UPPER(COUNTRY))            AS country,
        TRIM(UPPER(PAYMENTMETHOD))      AS payment_method,
        TRIM(UPPER(CATEGORY))           AS category,
        TRIM(UPPER(SALESCHANNEL))       AS sales_channel,
        TRIM(UPPER(RETURNSTATUS))       AS return_status,
        TRIM(UPPER(SHIPMENTPROVIDER))   AS shipment_provider,
        TRIM(UPPER(WAREHOUSELOCATION))  AS warehouse_location,
        TRIM(UPPER(ORDERPRIORITY))      AS order_priority,

        CURRENT_TIMESTAMP()             AS stg_loaded_at

    FROM raw_data
),

validated AS (
    SELECT
        *,
        YEAR(invoice_date)  AS invoice_year,
        MONTH(invoice_date) AS invoice_month,

        CASE
            WHEN invoice_date IS NULL           THEN 'INVALID_DATE'
            WHEN quantity IS NULL               THEN 'INVALID_QUANTITY'
            WHEN unit_price IS NULL             THEN 'INVALID_PRICE'
            WHEN shipping_cost < 0              THEN 'NEGATIVE_SHIPPING'
            WHEN discount < 0 OR discount > 1   THEN 'INVALID_DISCOUNT'
            WHEN invoice_no IS NULL             THEN 'NULL_INVOICE'
            ELSE 'VALID'
        END AS data_quality_flag
    FROM staged
)

SELECT *
FROM validated


WHERE NOT EXISTS (
    SELECT 1
    FROM INCREMENTALETL._staging.stg_sales t
    WHERE t.invoice_year  = validated.invoice_year
      AND t.invoice_month = validated.invoice_month
)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m11:54:50.993064 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m11:54:53.613940 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 2.631 seconds
[0m11:54:53.741872 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:54:53.741872 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:54:53.934609 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.189 seconds
[0m11:54:53.950502 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:54:53.950502 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:54:54.158106 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.202 seconds
[0m11:54:54.193272 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:54:54.197136 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:54:54.377319 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.178 seconds
[0m11:54:54.411870 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:54:54.413425 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table "INCREMENTALETL"."_STAGING"."STG_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:54:54.579863 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.162 seconds
[0m11:54:54.617610 [debug] [Thread-2 (]: 
    In "INCREMENTALETL"."_STAGING"."STG_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m11:54:54.659025 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.stg_sales"
[0m11:54:54.663936 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:54:54.663936 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m11:54:54.854939 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.199 seconds
[0m11:54:54.870801 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:54:54.870801 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: merge into INCREMENTALETL._staging.stg_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._staging.stg_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","DATA_QUALITY_FLAG" = DBT_INTERNAL_SOURCE."DATA_QUALITY_FLAG"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "INVOICE_YEAR", "INVOICE_MONTH", "DATA_QUALITY_FLAG")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "INVOICE_YEAR", "INVOICE_MONTH", "DATA_QUALITY_FLAG")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m11:54:55.706533 [debug] [Thread-2 (]: SQL status: SUCCESS 720 in 0.829 seconds
[0m11:54:55.706533 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:54:55.706533 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:54:55.997155 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.290 seconds
[0m11:54:56.029151 [debug] [Thread-2 (]: Applying DROP to: INCREMENTALETL._staging.stg_sales__dbt_tmp
[0m11:54:56.045042 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:54:56.045042 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: drop view if exists INCREMENTALETL._staging.stg_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:54:56.243305 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.200 seconds
[0m11:54:56.311125 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9048e6bb-e0e5-45ed-b418-9c4089a0eea7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002159D5895D0>]}
[0m11:54:56.311125 [info ] [Thread-2 (]: 1 of 6 OK created sql incremental model _staging.stg_sales ..................... [[32mSUCCESS 720[0m in 5.50s]
[0m11:54:56.311125 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.stg_sales
[0m11:54:56.314954 [debug] [Thread-4 (]: Began running node model.sales_pipelines.processed_sales
[0m11:54:56.314954 [info ] [Thread-4 (]: 2 of 6 START sql incremental model _processed.processed_sales .................. [RUN]
[0m11:54:56.317583 [debug] [Thread-4 (]: Acquiring new snowflake connection 'model.sales_pipelines.processed_sales'
[0m11:54:56.317583 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.processed_sales
[0m11:54:56.321434 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.processed_sales"
[0m11:54:56.329081 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.processed_sales
[0m11:54:56.329081 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:54:56.329081 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: create or replace  temporary view INCREMENTALETL._processed.processed_sales__dbt_tmp
  
  
  
  
  as (
    

WITH staging AS (
    SELECT * 
    FROM INCREMENTALETL._staging.stg_sales
),

deduped AS (
    SELECT *,
        ROW_NUMBER() OVER (
            PARTITION BY invoice_no, stock_code
            ORDER BY stg_loaded_at DESC
        ) AS row_num
    FROM staging
),

cleaned AS (
    SELECT
        invoice_no,
        stock_code,
        description,
        quantity,
        unit_price,
        discount,
        shipping_cost,
        customer_id,
        invoice_date,
        invoice_year,
        invoice_month,
        country,
        payment_method,
        category,
        sales_channel,
        return_status,
        shipment_provider,
        warehouse_location,
        order_priority,

        -- Derived metrics
        ROUND(quantity * unit_price, 2) AS gross_amount,
        ROUND(quantity * unit_price * (1 - discount), 2) AS net_amount,
        ROUND(
            quantity * unit_price * (1 - discount)
            + COALESCE(shipping_cost, 0),
        2) AS total_amount,

        CASE WHEN return_status = 'RETURNED' THEN TRUE ELSE FALSE END AS is_returned,
        CASE WHEN customer_id IS NULL THEN TRUE ELSE FALSE END AS is_guest_customer,

        stg_loaded_at,
        CURRENT_TIMESTAMP() AS processed_at

    FROM deduped
    WHERE
        row_num = 1
        AND data_quality_flag = 'VALID'
        AND invoice_date IS NOT NULL
)

SELECT *
FROM cleaned


WHERE NOT EXISTS (
    SELECT 1
    FROM INCREMENTALETL._processed.processed_sales t
    WHERE t.invoice_year  = cleaned.invoice_year
      AND t.invoice_month = cleaned.invoice_month
)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m11:54:56.329081 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m11:54:57.232915 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.893 seconds
[0m11:54:57.249528 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:54:57.249528 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:54:57.397837 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.142 seconds
[0m11:54:57.411051 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:54:57.411051 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:54:57.546031 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.133 seconds
[0m11:54:57.578173 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:54:57.578173 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:54:57.711870 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.130 seconds
[0m11:54:57.726021 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:54:57.726021 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:54:57.851911 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.127 seconds
[0m11:54:57.887298 [debug] [Thread-4 (]: 
    In "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m11:54:57.893524 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.processed_sales"
[0m11:54:57.893524 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:54:57.901603 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m11:54:58.054775 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.164 seconds
[0m11:54:58.054775 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:54:58.054775 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: merge into INCREMENTALETL._processed.processed_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._processed.processed_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","GROSS_AMOUNT" = DBT_INTERNAL_SOURCE."GROSS_AMOUNT","NET_AMOUNT" = DBT_INTERNAL_SOURCE."NET_AMOUNT","TOTAL_AMOUNT" = DBT_INTERNAL_SOURCE."TOTAL_AMOUNT","IS_RETURNED" = DBT_INTERNAL_SOURCE."IS_RETURNED","IS_GUEST_CUSTOMER" = DBT_INTERNAL_SOURCE."IS_GUEST_CUSTOMER","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","PROCESSED_AT" = DBT_INTERNAL_SOURCE."PROCESSED_AT"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m11:54:59.073635 [debug] [Thread-4 (]: SQL status: SUCCESS 706 in 1.003 seconds
[0m11:54:59.073635 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:54:59.073635 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:54:59.327231 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.244 seconds
[0m11:54:59.328297 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._processed.processed_sales__dbt_tmp
[0m11:54:59.328297 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:54:59.328297 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: drop view if exists INCREMENTALETL._processed.processed_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:54:59.502524 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.165 seconds
[0m11:54:59.502524 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9048e6bb-e0e5-45ed-b418-9c4089a0eea7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002159CDFA890>]}
[0m11:54:59.518560 [info ] [Thread-4 (]: 2 of 6 OK created sql incremental model _processed.processed_sales ............. [[32mSUCCESS 706[0m in 3.18s]
[0m11:54:59.518560 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.processed_sales
[0m11:54:59.518560 [debug] [Thread-3 (]: Began running node model.sales_pipelines.dim_customers
[0m11:54:59.518560 [debug] [Thread-2 (]: Began running node model.sales_pipelines.dim_date
[0m11:54:59.534676 [debug] [Thread-4 (]: Began running node model.sales_pipelines.fact_sales
[0m11:54:59.518560 [debug] [Thread-5 (]: Began running node model.sales_pipelines.dim_products
[0m11:54:59.540135 [info ] [Thread-3 (]: 3 of 6 START sql table model _marts.dim_customers .............................. [RUN]
[0m11:54:59.544130 [info ] [Thread-2 (]: 4 of 6 START sql table model _marts.dim_date ................................... [RUN]
[0m11:54:59.547970 [info ] [Thread-4 (]: 6 of 6 START sql incremental model _marts.fact_sales ........................... [RUN]
[0m11:54:59.553935 [info ] [Thread-5 (]: 5 of 6 START sql table model _marts.dim_products ............................... [RUN]
[0m11:54:59.559196 [debug] [Thread-3 (]: Acquiring new snowflake connection 'model.sales_pipelines.dim_customers'
[0m11:54:59.560696 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.sales_pipelines.stg_sales, now model.sales_pipelines.dim_date)
[0m11:54:59.564636 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.sales_pipelines.processed_sales, now model.sales_pipelines.fact_sales)
[0m11:54:59.566673 [debug] [Thread-5 (]: Acquiring new snowflake connection 'model.sales_pipelines.dim_products'
[0m11:54:59.568694 [debug] [Thread-3 (]: Began compiling node model.sales_pipelines.dim_customers
[0m11:54:59.568694 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.dim_date
[0m11:54:59.568694 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.fact_sales
[0m11:54:59.568694 [debug] [Thread-5 (]: Began compiling node model.sales_pipelines.dim_products
[0m11:54:59.580523 [debug] [Thread-3 (]: Writing injected SQL for node "model.sales_pipelines.dim_customers"
[0m11:54:59.594052 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.dim_date"
[0m11:54:59.600375 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.fact_sales"
[0m11:54:59.605421 [debug] [Thread-5 (]: Writing injected SQL for node "model.sales_pipelines.dim_products"
[0m11:54:59.610847 [debug] [Thread-3 (]: Began executing node model.sales_pipelines.dim_customers
[0m11:54:59.613168 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.dim_date
[0m11:54:59.613168 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.fact_sales
[0m11:54:59.626714 [debug] [Thread-5 (]: Began executing node model.sales_pipelines.dim_products
[0m11:54:59.673665 [debug] [Thread-3 (]: Writing runtime sql for node "model.sales_pipelines.dim_customers"
[0m11:54:59.673665 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.dim_date"
[0m11:54:59.681702 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:54:59.681702 [debug] [Thread-5 (]: Writing runtime sql for node "model.sales_pipelines.dim_products"
[0m11:54:59.690178 [debug] [Thread-3 (]: Using snowflake connection "model.sales_pipelines.dim_customers"
[0m11:54:59.690178 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: create or replace  temporary view INCREMENTALETL._marts.fact_sales__dbt_tmp
  
  
  
  
  as (
    

SELECT
    invoice_no,
    stock_code,
    customer_id,
    invoice_date::DATE AS date_key,

    quantity,
    unit_price,
    discount,
    shipping_cost,

    gross_amount,
    net_amount,
    total_amount,

    is_returned,
    processed_at

FROM INCREMENTALETL._processed.processed_sales


WHERE processed_at > (SELECT MAX(processed_at) FROM INCREMENTALETL._marts.fact_sales)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */;
[0m11:54:59.694391 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.dim_date"
[0m11:54:59.694391 [debug] [Thread-3 (]: On model.sales_pipelines.dim_customers: create or replace transient table INCREMENTALETL._marts.dim_customers
    
    
    
    as (

SELECT DISTINCT
    customer_id,
    country,
    payment_method,
    sales_channel,
    is_guest_customer,
    MIN(invoice_date) AS first_purchase_date,
    MAX(invoice_date) AS last_purchase_date,
    COUNT(DISTINCT invoice_no) AS total_orders
FROM INCREMENTALETL._processed.processed_sales
WHERE customer_id IS NOT NULL
GROUP BY
    customer_id,
    country,
    payment_method,
    sales_channel,
    is_guest_customer
    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.dim_customers"} */;
[0m11:54:59.696766 [debug] [Thread-5 (]: Using snowflake connection "model.sales_pipelines.dim_products"
[0m11:54:59.698515 [debug] [Thread-2 (]: On model.sales_pipelines.dim_date: create or replace transient table INCREMENTALETL._marts.dim_date
    
    
    
    as (

SELECT DISTINCT
    invoice_date::DATE AS date_key,
    YEAR(invoice_date)  AS year,
    MONTH(invoice_date) AS month,
    DAY(invoice_date)   AS day,
    DAYNAME(invoice_date) AS day_name,
    WEEK(invoice_date)  AS week_number,
    QUARTER(invoice_date) AS quarter
FROM INCREMENTALETL._processed.processed_sales
    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.dim_date"} */;
[0m11:54:59.700803 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m11:54:59.700803 [debug] [Thread-5 (]: On model.sales_pipelines.dim_products: create or replace transient table INCREMENTALETL._marts.dim_products
    
    
    
    as (

SELECT DISTINCT
    stock_code,
    description,
    category
FROM INCREMENTALETL._processed.processed_sales
    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.dim_products"} */;
[0m11:54:59.703371 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m11:54:59.914865 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.219 seconds
[0m11:54:59.914865 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:54:59.914865 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: describe table INCREMENTALETL._marts.fact_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:55:00.059209 [debug] [Thread-4 (]: SQL status: SUCCESS 13 in 0.146 seconds
[0m11:55:00.075048 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:55:00.075048 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: describe table INCREMENTALETL._marts.fact_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:55:00.217916 [debug] [Thread-4 (]: SQL status: SUCCESS 13 in 0.130 seconds
[0m11:55:00.244381 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:55:00.250662 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: describe table "INCREMENTALETL"."_MARTS"."FACT_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:55:00.332439 [debug] [Thread-2 (]: SQL status: SUCCESS 1796 in 0.630 seconds
[0m11:55:00.352072 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9048e6bb-e0e5-45ed-b418-9c4089a0eea7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002159B767670>]}
[0m11:55:00.352072 [info ] [Thread-2 (]: 4 of 6 OK created sql table model _marts.dim_date .............................. [[32mSUCCESS 1796[0m in 0.79s]
[0m11:55:00.361452 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.dim_date
[0m11:55:00.383492 [debug] [Thread-4 (]: SQL status: SUCCESS 13 in 0.130 seconds
[0m11:55:00.398895 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.fact_sales"
[0m11:55:00.406603 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:55:00.408616 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */;
[0m11:55:00.552203 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.146 seconds
[0m11:55:00.562051 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:55:00.562051 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: merge into INCREMENTALETL._marts.fact_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._marts.fact_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","DATE_KEY" = DBT_INTERNAL_SOURCE."DATE_KEY","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","GROSS_AMOUNT" = DBT_INTERNAL_SOURCE."GROSS_AMOUNT","NET_AMOUNT" = DBT_INTERNAL_SOURCE."NET_AMOUNT","TOTAL_AMOUNT" = DBT_INTERNAL_SOURCE."TOTAL_AMOUNT","IS_RETURNED" = DBT_INTERNAL_SOURCE."IS_RETURNED","PROCESSED_AT" = DBT_INTERNAL_SOURCE."PROCESSED_AT"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "CUSTOMER_ID", "DATE_KEY", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "PROCESSED_AT")
    values
        ("INVOICE_NO", "STOCK_CODE", "CUSTOMER_ID", "DATE_KEY", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "PROCESSED_AT")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */;
[0m11:55:00.908219 [debug] [Thread-5 (]: SQL status: SUCCESS 29391 in 1.202 seconds
[0m11:55:00.915682 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9048e6bb-e0e5-45ed-b418-9c4089a0eea7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002158A6062F0>]}
[0m11:55:00.927334 [info ] [Thread-5 (]: 5 of 6 OK created sql table model _marts.dim_products .......................... [[32mSUCCESS 29391[0m in 1.35s]
[0m11:55:00.931978 [debug] [Thread-5 (]: Finished running node model.sales_pipelines.dim_products
[0m11:55:00.951778 [debug] [Thread-3 (]: SQL status: SUCCESS 38693 in 1.250 seconds
[0m11:55:00.966052 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9048e6bb-e0e5-45ed-b418-9c4089a0eea7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002159D502B30>]}
[0m11:55:00.972550 [info ] [Thread-3 (]: 3 of 6 OK created sql table model _marts.dim_customers ......................... [[32mSUCCESS 38693[0m in 1.41s]
[0m11:55:00.978323 [debug] [Thread-3 (]: Finished running node model.sales_pipelines.dim_customers
[0m11:55:01.306922 [debug] [Thread-4 (]: SQL status: SUCCESS 706 in 0.749 seconds
[0m11:55:01.321330 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:55:01.321330 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:55:01.578229 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.253 seconds
[0m11:55:01.602163 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._marts.fact_sales__dbt_tmp
[0m11:55:01.608051 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:55:01.610171 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: drop view if exists INCREMENTALETL._marts.fact_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:55:01.780628 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.168 seconds
[0m11:55:01.793723 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9048e6bb-e0e5-45ed-b418-9c4089a0eea7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002159D66C1F0>]}
[0m11:55:01.795580 [info ] [Thread-4 (]: 6 of 6 OK created sql incremental model _marts.fact_sales ...................... [[32mSUCCESS 706[0m in 2.23s]
[0m11:55:01.801289 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.fact_sales
[0m11:55:01.807949 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:55:01.809884 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m11:55:01.811023 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m11:55:02.042625 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m11:55:02.042625 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m11:55:02.353572 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m11:55:02.357622 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m11:55:02.651957 [debug] [MainThread]: Connection 'list_INCREMENTALETL__marts' was left open.
[0m11:55:02.651957 [debug] [MainThread]: On list_INCREMENTALETL__marts: Close
[0m11:55:02.945892 [debug] [MainThread]: Connection 'list_INCREMENTALETL__staging' was left open.
[0m11:55:02.962197 [debug] [MainThread]: On list_INCREMENTALETL__staging: Close
[0m11:55:03.262379 [debug] [MainThread]: Connection 'list_INCREMENTALETL__processed' was left open.
[0m11:55:03.262379 [debug] [MainThread]: On list_INCREMENTALETL__processed: Close
[0m11:55:03.570141 [debug] [MainThread]: Connection 'model.sales_pipelines.dim_date' was left open.
[0m11:55:03.580714 [debug] [MainThread]: On model.sales_pipelines.dim_date: Close
[0m11:55:03.876627 [debug] [MainThread]: Connection 'model.sales_pipelines.fact_sales' was left open.
[0m11:55:03.876627 [debug] [MainThread]: On model.sales_pipelines.fact_sales: Close
[0m11:55:04.144285 [debug] [MainThread]: Connection 'model.sales_pipelines.dim_customers' was left open.
[0m11:55:04.146310 [debug] [MainThread]: On model.sales_pipelines.dim_customers: Close
[0m11:55:04.407522 [debug] [MainThread]: Connection 'model.sales_pipelines.dim_products' was left open.
[0m11:55:04.409543 [debug] [MainThread]: On model.sales_pipelines.dim_products: Close
[0m11:55:04.643869 [info ] [MainThread]: 
[0m11:55:04.656615 [info ] [MainThread]: Finished running 3 incremental models, 3 table models in 0 hours 0 minutes and 17.50 seconds (17.50s).
[0m11:55:04.656615 [debug] [MainThread]: Command end result
[0m11:55:04.729635 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m11:55:04.732257 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m11:55:04.744045 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\snowflake-incremental-pipeline\dbt_project\target\run_results.json
[0m11:55:04.744045 [info ] [MainThread]: 
[0m11:55:04.744045 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:55:04.744045 [info ] [MainThread]: 
[0m11:55:04.744045 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=6
[0m11:55:04.744045 [debug] [MainThread]: Command `dbt run` succeeded at 11:55:04.744045 after 20.35 seconds
[0m11:55:04.752495 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000215883876A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021587D8F6D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002158A6694B0>]}
[0m11:55:04.754203 [debug] [MainThread]: Flushing usage events
[0m11:55:06.127363 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:56:04.594054 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CF52EE76A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CF54195000>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CF541963B0>]}


============================== 11:56:04.601815 | e982d628-6bed-4abd-9256-80981693acf9 ==============================
[0m11:56:04.601815 [info ] [MainThread]: Running with dbt=1.11.6
[0m11:56:04.601815 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'profiles_dir': 'D:\\snowflake-incremental-pipeline\\dbt_project', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'invocation_command': 'dbt run', 'use_colors': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'D:\\snowflake-incremental-pipeline\\dbt_project\\logs'}
[0m11:56:05.666602 [debug] [MainThread]: Snowflake adapter: Setting snowflake.connector to ERROR (file logging only)
[0m11:56:05.666602 [debug] [MainThread]: Snowflake adapter: Setting botocore to ERROR (file logging only)
[0m11:56:05.666602 [debug] [MainThread]: Snowflake adapter: Setting boto3 to ERROR (file logging only)
[0m11:56:06.021942 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e982d628-6bed-4abd-9256-80981693acf9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CF541D52A0>]}
[0m11:56:06.100543 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e982d628-6bed-4abd-9256-80981693acf9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CF541F94B0>]}
[0m11:56:06.100543 [info ] [MainThread]: Registered adapter: snowflake=1.11.2
[0m11:56:06.626780 [debug] [MainThread]: checksum: ec27062d18352e9cfca161c166fcc73d2201661a23a3cf3c89196a27f32cfcb8, vars: {}, profile: , target: , version: 1.11.6
[0m11:56:06.886385 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:56:06.886385 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m11:56:06.886385 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:56:06.963235 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e982d628-6bed-4abd-9256-80981693acf9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CF66898A30>]}
[0m11:56:07.135694 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m11:56:07.135694 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m11:56:07.156597 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e982d628-6bed-4abd-9256-80981693acf9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CF66898E20>]}
[0m11:56:07.156597 [info ] [MainThread]: Found 6 models, 6 data tests, 1 source, 522 macros
[0m11:56:07.156597 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e982d628-6bed-4abd-9256-80981693acf9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CF66899480>]}
[0m11:56:07.169537 [info ] [MainThread]: 
[0m11:56:07.169537 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m11:56:07.171785 [info ] [MainThread]: 
[0m11:56:07.172883 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m11:56:07.184281 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m11:56:07.186090 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m11:56:07.212326 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m11:56:07.300533 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m11:56:07.300533 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m11:56:07.300533 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m11:56:07.308555 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m11:56:07.308555 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m11:56:07.308555 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m11:56:07.311677 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:56:07.311677 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:56:07.311677 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:56:08.590047 [debug] [ThreadPool]: SQL status: SUCCESS 5 in 1.291 seconds
[0m11:56:08.606168 [debug] [ThreadPool]: SQL status: SUCCESS 5 in 1.295 seconds
[0m11:56:08.606168 [debug] [ThreadPool]: SQL status: SUCCESS 5 in 1.301 seconds
[0m11:56:08.634584 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__processed'
[0m11:56:08.638159 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__staging'
[0m11:56:08.670031 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__marts'
[0m11:56:08.701625 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__processed"
[0m11:56:08.705648 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__staging"
[0m11:56:08.717772 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__marts"
[0m11:56:08.717772 [debug] [ThreadPool]: On list_INCREMENTALETL__processed: show objects in INCREMENTALETL._processed
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__processed"} */;
[0m11:56:08.717772 [debug] [ThreadPool]: On list_INCREMENTALETL__staging: show objects in INCREMENTALETL._staging
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__staging"} */;
[0m11:56:08.717772 [debug] [ThreadPool]: On list_INCREMENTALETL__marts: show objects in INCREMENTALETL._marts
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__marts"} */;
[0m11:56:08.717772 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:56:08.727423 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:56:08.727423 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:56:09.589523 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.878 seconds
[0m11:56:09.654102 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.938 seconds
[0m11:56:09.669822 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 0.950 seconds
[0m11:56:09.689877 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e982d628-6bed-4abd-9256-80981693acf9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CF668DC970>]}
[0m11:56:09.701538 [debug] [Thread-2 (]: Began running node model.sales_pipelines.stg_sales
[0m11:56:09.701538 [info ] [Thread-2 (]: 1 of 6 START sql incremental model _staging.stg_sales .......................... [RUN]
[0m11:56:09.717466 [debug] [Thread-2 (]: Acquiring new snowflake connection 'model.sales_pipelines.stg_sales'
[0m11:56:09.717466 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.stg_sales
[0m11:56:09.765139 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.stg_sales"
[0m11:56:09.765139 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.stg_sales
[0m11:56:09.848212 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:56:09.848212 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: create or replace  temporary view INCREMENTALETL._staging.stg_sales__dbt_tmp
  
  
  
  
  as (
    

WITH raw_data AS (
    SELECT * 
    FROM INCREMENTALETL.LANDINGZONE.RAW_SALES
),

staged AS (
    SELECT
        -- IDs
        CAST(INVOICENO AS VARCHAR)    AS invoice_no,
        CAST(STOCKCODE AS VARCHAR)    AS stock_code,
        CAST(DESCRIPTION AS VARCHAR)  AS description,

        -- Quantities & Prices
        TRY_CAST(QUANTITY AS INT)      AS quantity,
        TRY_CAST(UNITPRICE AS FLOAT)   AS unit_price,
        TRY_CAST(DISCOUNT AS FLOAT)    AS discount,
        TRY_CAST(SHIPPINGCOST AS FLOAT) AS shipping_cost,
        TRY_CAST(CUSTOMERID AS FLOAT)  AS customer_id,

        -- Convert once
        TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS') AS invoice_date,

        TRIM(UPPER(COUNTRY))            AS country,
        TRIM(UPPER(PAYMENTMETHOD))      AS payment_method,
        TRIM(UPPER(CATEGORY))           AS category,
        TRIM(UPPER(SALESCHANNEL))       AS sales_channel,
        TRIM(UPPER(RETURNSTATUS))       AS return_status,
        TRIM(UPPER(SHIPMENTPROVIDER))   AS shipment_provider,
        TRIM(UPPER(WAREHOUSELOCATION))  AS warehouse_location,
        TRIM(UPPER(ORDERPRIORITY))      AS order_priority,

        CURRENT_TIMESTAMP()             AS stg_loaded_at

    FROM raw_data
),

validated AS (
    SELECT
        *,
        YEAR(invoice_date)  AS invoice_year,
        MONTH(invoice_date) AS invoice_month,

        CASE
            WHEN invoice_date IS NULL           THEN 'INVALID_DATE'
            WHEN quantity IS NULL               THEN 'INVALID_QUANTITY'
            WHEN unit_price IS NULL             THEN 'INVALID_PRICE'
            WHEN shipping_cost < 0              THEN 'NEGATIVE_SHIPPING'
            WHEN discount < 0 OR discount > 1   THEN 'INVALID_DISCOUNT'
            WHEN invoice_no IS NULL             THEN 'NULL_INVOICE'
            ELSE 'VALID'
        END AS data_quality_flag
    FROM staged
)

SELECT *
FROM validated


WHERE NOT EXISTS (
    SELECT 1
    FROM INCREMENTALETL._staging.stg_sales t
    WHERE t.invoice_year  = validated.invoice_year
      AND t.invoice_month = validated.invoice_month
)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m11:56:09.848212 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m11:56:10.798959 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.939 seconds
[0m11:56:10.831472 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:56:10.831472 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:56:10.991093 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.155 seconds
[0m11:56:11.007169 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:56:11.007169 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:56:11.164423 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.153 seconds
[0m11:56:11.205753 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:56:11.205753 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:56:11.559468 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.349 seconds
[0m11:56:11.559468 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:56:11.559468 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table "INCREMENTALETL"."_STAGING"."STG_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:56:11.742109 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.166 seconds
[0m11:56:11.783737 [debug] [Thread-2 (]: 
    In "INCREMENTALETL"."_STAGING"."STG_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m11:56:11.816030 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.stg_sales"
[0m11:56:11.816030 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:56:11.816030 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m11:56:12.006461 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.177 seconds
[0m11:56:12.006461 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:56:12.006461 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: merge into INCREMENTALETL._staging.stg_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._staging.stg_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","DATA_QUALITY_FLAG" = DBT_INTERNAL_SOURCE."DATA_QUALITY_FLAG"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "INVOICE_YEAR", "INVOICE_MONTH", "DATA_QUALITY_FLAG")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "INVOICE_YEAR", "INVOICE_MONTH", "DATA_QUALITY_FLAG")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m11:56:13.003612 [debug] [Thread-2 (]: SQL status: SUCCESS 3624 in 0.994 seconds
[0m11:56:13.003612 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:56:13.003612 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:56:13.406133 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.395 seconds
[0m11:56:13.412198 [debug] [Thread-2 (]: Applying DROP to: INCREMENTALETL._staging.stg_sales__dbt_tmp
[0m11:56:13.428252 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:56:13.428252 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: drop view if exists INCREMENTALETL._staging.stg_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:56:14.422905 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.999 seconds
[0m11:56:14.492231 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e982d628-6bed-4abd-9256-80981693acf9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CF54228EE0>]}
[0m11:56:14.494240 [info ] [Thread-2 (]: 1 of 6 OK created sql incremental model _staging.stg_sales ..................... [[32mSUCCESS 3624[0m in 4.77s]
[0m11:56:14.494240 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.stg_sales
[0m11:56:14.494240 [debug] [Thread-4 (]: Began running node model.sales_pipelines.processed_sales
[0m11:56:14.497385 [info ] [Thread-4 (]: 2 of 6 START sql incremental model _processed.processed_sales .................. [RUN]
[0m11:56:14.498654 [debug] [Thread-4 (]: Acquiring new snowflake connection 'model.sales_pipelines.processed_sales'
[0m11:56:14.498654 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.processed_sales
[0m11:56:14.504677 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.processed_sales"
[0m11:56:14.506684 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.processed_sales
[0m11:56:14.515143 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:56:14.516259 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: create or replace  temporary view INCREMENTALETL._processed.processed_sales__dbt_tmp
  
  
  
  
  as (
    

WITH staging AS (
    SELECT * 
    FROM INCREMENTALETL._staging.stg_sales
),

deduped AS (
    SELECT *,
        ROW_NUMBER() OVER (
            PARTITION BY invoice_no, stock_code
            ORDER BY stg_loaded_at DESC
        ) AS row_num
    FROM staging
),

cleaned AS (
    SELECT
        invoice_no,
        stock_code,
        description,
        quantity,
        unit_price,
        discount,
        shipping_cost,
        customer_id,
        invoice_date,
        invoice_year,
        invoice_month,
        country,
        payment_method,
        category,
        sales_channel,
        return_status,
        shipment_provider,
        warehouse_location,
        order_priority,

        -- Derived metrics
        ROUND(quantity * unit_price, 2) AS gross_amount,
        ROUND(quantity * unit_price * (1 - discount), 2) AS net_amount,
        ROUND(
            quantity * unit_price * (1 - discount)
            + COALESCE(shipping_cost, 0),
        2) AS total_amount,

        CASE WHEN return_status = 'RETURNED' THEN TRUE ELSE FALSE END AS is_returned,
        CASE WHEN customer_id IS NULL THEN TRUE ELSE FALSE END AS is_guest_customer,

        stg_loaded_at,
        CURRENT_TIMESTAMP() AS processed_at

    FROM deduped
    WHERE
        row_num = 1
        AND data_quality_flag = 'VALID'
        AND invoice_date IS NOT NULL
)

SELECT *
FROM cleaned


WHERE NOT EXISTS (
    SELECT 1
    FROM INCREMENTALETL._processed.processed_sales t
    WHERE t.invoice_year  = cleaned.invoice_year
      AND t.invoice_month = cleaned.invoice_month
)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m11:56:14.516259 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m11:56:16.212313 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 1.697 seconds
[0m11:56:16.212313 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:56:16.212313 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:56:16.392003 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.167 seconds
[0m11:56:16.407746 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:56:16.407746 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:56:16.576126 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.153 seconds
[0m11:56:16.584238 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:56:16.584238 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:56:16.740724 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.151 seconds
[0m11:56:16.762690 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:56:16.764713 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:56:16.992861 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.225 seconds
[0m11:56:17.020864 [debug] [Thread-4 (]: 
    In "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m11:56:17.028910 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.processed_sales"
[0m11:56:17.035742 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:56:17.035742 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m11:56:17.225535 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.186 seconds
[0m11:56:17.225535 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:56:17.225535 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: merge into INCREMENTALETL._processed.processed_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._processed.processed_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","GROSS_AMOUNT" = DBT_INTERNAL_SOURCE."GROSS_AMOUNT","NET_AMOUNT" = DBT_INTERNAL_SOURCE."NET_AMOUNT","TOTAL_AMOUNT" = DBT_INTERNAL_SOURCE."TOTAL_AMOUNT","IS_RETURNED" = DBT_INTERNAL_SOURCE."IS_RETURNED","IS_GUEST_CUSTOMER" = DBT_INTERNAL_SOURCE."IS_GUEST_CUSTOMER","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","PROCESSED_AT" = DBT_INTERNAL_SOURCE."PROCESSED_AT"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m11:56:18.286191 [debug] [Thread-4 (]: SQL status: SUCCESS 3548 in 1.058 seconds
[0m11:56:18.286191 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:56:18.298730 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:56:18.561614 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.269 seconds
[0m11:56:18.574909 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._processed.processed_sales__dbt_tmp
[0m11:56:18.588776 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:56:18.588776 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: drop view if exists INCREMENTALETL._processed.processed_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:56:18.787447 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.194 seconds
[0m11:56:18.798078 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e982d628-6bed-4abd-9256-80981693acf9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CF66980910>]}
[0m11:56:18.798078 [info ] [Thread-4 (]: 2 of 6 OK created sql incremental model _processed.processed_sales ............. [[32mSUCCESS 3548[0m in 4.30s]
[0m11:56:18.812063 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.processed_sales
[0m11:56:18.813914 [debug] [Thread-3 (]: Began running node model.sales_pipelines.dim_customers
[0m11:56:18.818194 [debug] [Thread-2 (]: Began running node model.sales_pipelines.dim_date
[0m11:56:18.821101 [debug] [Thread-5 (]: Began running node model.sales_pipelines.dim_products
[0m11:56:18.821101 [debug] [Thread-4 (]: Began running node model.sales_pipelines.fact_sales
[0m11:56:18.821101 [info ] [Thread-3 (]: 3 of 6 START sql table model _marts.dim_customers .............................. [RUN]
[0m11:56:18.832086 [info ] [Thread-2 (]: 4 of 6 START sql table model _marts.dim_date ................................... [RUN]
[0m11:56:18.838387 [info ] [Thread-5 (]: 5 of 6 START sql table model _marts.dim_products ............................... [RUN]
[0m11:56:18.838387 [info ] [Thread-4 (]: 6 of 6 START sql incremental model _marts.fact_sales ........................... [RUN]
[0m11:56:18.846011 [debug] [Thread-3 (]: Acquiring new snowflake connection 'model.sales_pipelines.dim_customers'
[0m11:56:18.852920 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.sales_pipelines.stg_sales, now model.sales_pipelines.dim_date)
[0m11:56:18.856854 [debug] [Thread-5 (]: Acquiring new snowflake connection 'model.sales_pipelines.dim_products'
[0m11:56:18.858926 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.sales_pipelines.processed_sales, now model.sales_pipelines.fact_sales)
[0m11:56:18.858926 [debug] [Thread-3 (]: Began compiling node model.sales_pipelines.dim_customers
[0m11:56:18.863083 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.dim_date
[0m11:56:18.863083 [debug] [Thread-5 (]: Began compiling node model.sales_pipelines.dim_products
[0m11:56:18.865543 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.fact_sales
[0m11:56:18.865543 [debug] [Thread-3 (]: Writing injected SQL for node "model.sales_pipelines.dim_customers"
[0m11:56:18.882408 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.dim_date"
[0m11:56:18.882408 [debug] [Thread-5 (]: Writing injected SQL for node "model.sales_pipelines.dim_products"
[0m11:56:18.898949 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.fact_sales"
[0m11:56:18.898949 [debug] [Thread-3 (]: Began executing node model.sales_pipelines.dim_customers
[0m11:56:18.898949 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.dim_date
[0m11:56:18.903940 [debug] [Thread-5 (]: Began executing node model.sales_pipelines.dim_products
[0m11:56:18.905186 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.fact_sales
[0m11:56:18.946211 [debug] [Thread-3 (]: Writing runtime sql for node "model.sales_pipelines.dim_customers"
[0m11:56:18.946211 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.dim_date"
[0m11:56:18.959491 [debug] [Thread-5 (]: Writing runtime sql for node "model.sales_pipelines.dim_products"
[0m11:56:18.963317 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:56:18.972243 [debug] [Thread-3 (]: Using snowflake connection "model.sales_pipelines.dim_customers"
[0m11:56:18.974122 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.dim_date"
[0m11:56:18.974122 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: create or replace  temporary view INCREMENTALETL._marts.fact_sales__dbt_tmp
  
  
  
  
  as (
    

SELECT
    invoice_no,
    stock_code,
    customer_id,
    invoice_date::DATE AS date_key,

    quantity,
    unit_price,
    discount,
    shipping_cost,

    gross_amount,
    net_amount,
    total_amount,

    is_returned,
    processed_at

FROM INCREMENTALETL._processed.processed_sales


WHERE processed_at > (SELECT MAX(processed_at) FROM INCREMENTALETL._marts.fact_sales)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */;
[0m11:56:18.974122 [debug] [Thread-5 (]: Using snowflake connection "model.sales_pipelines.dim_products"
[0m11:56:18.978444 [debug] [Thread-3 (]: On model.sales_pipelines.dim_customers: create or replace transient table INCREMENTALETL._marts.dim_customers
    
    
    
    as (

SELECT DISTINCT
    customer_id,
    country,
    payment_method,
    sales_channel,
    is_guest_customer,
    MIN(invoice_date) AS first_purchase_date,
    MAX(invoice_date) AS last_purchase_date,
    COUNT(DISTINCT invoice_no) AS total_orders
FROM INCREMENTALETL._processed.processed_sales
WHERE customer_id IS NOT NULL
GROUP BY
    customer_id,
    country,
    payment_method,
    sales_channel,
    is_guest_customer
    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.dim_customers"} */;
[0m11:56:18.980530 [debug] [Thread-2 (]: On model.sales_pipelines.dim_date: create or replace transient table INCREMENTALETL._marts.dim_date
    
    
    
    as (

SELECT DISTINCT
    invoice_date::DATE AS date_key,
    YEAR(invoice_date)  AS year,
    MONTH(invoice_date) AS month,
    DAY(invoice_date)   AS day,
    DAYNAME(invoice_date) AS day_name,
    WEEK(invoice_date)  AS week_number,
    QUARTER(invoice_date) AS quarter
FROM INCREMENTALETL._processed.processed_sales
    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.dim_date"} */;
[0m11:56:18.982537 [debug] [Thread-5 (]: On model.sales_pipelines.dim_products: create or replace transient table INCREMENTALETL._marts.dim_products
    
    
    
    as (

SELECT DISTINCT
    stock_code,
    description,
    category
FROM INCREMENTALETL._processed.processed_sales
    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.dim_products"} */;
[0m11:56:18.984575 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m11:56:18.988982 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m11:56:19.236516 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.264 seconds
[0m11:56:19.250616 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:56:19.250616 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: describe table INCREMENTALETL._marts.fact_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:56:19.455759 [debug] [Thread-4 (]: SQL status: SUCCESS 13 in 0.201 seconds
[0m11:56:19.480215 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:56:19.492704 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: describe table INCREMENTALETL._marts.fact_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:56:19.617562 [debug] [Thread-2 (]: SQL status: SUCCESS 1947 in 0.639 seconds
[0m11:56:19.646954 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e982d628-6bed-4abd-9256-80981693acf9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CF66EED2A0>]}
[0m11:56:19.646954 [info ] [Thread-2 (]: 4 of 6 OK created sql table model _marts.dim_date .............................. [[32mSUCCESS 1947[0m in 0.80s]
[0m11:56:19.646954 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.dim_date
[0m11:56:19.679582 [debug] [Thread-4 (]: SQL status: SUCCESS 13 in 0.158 seconds
[0m11:56:19.698847 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:56:19.702840 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: describe table "INCREMENTALETL"."_MARTS"."FACT_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:56:19.864434 [debug] [Thread-4 (]: SQL status: SUCCESS 13 in 0.161 seconds
[0m11:56:19.876801 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.fact_sales"
[0m11:56:19.891887 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:56:19.891887 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */;
[0m11:56:20.059130 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.167 seconds
[0m11:56:20.059130 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:56:20.059130 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: merge into INCREMENTALETL._marts.fact_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._marts.fact_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","DATE_KEY" = DBT_INTERNAL_SOURCE."DATE_KEY","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","GROSS_AMOUNT" = DBT_INTERNAL_SOURCE."GROSS_AMOUNT","NET_AMOUNT" = DBT_INTERNAL_SOURCE."NET_AMOUNT","TOTAL_AMOUNT" = DBT_INTERNAL_SOURCE."TOTAL_AMOUNT","IS_RETURNED" = DBT_INTERNAL_SOURCE."IS_RETURNED","PROCESSED_AT" = DBT_INTERNAL_SOURCE."PROCESSED_AT"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "CUSTOMER_ID", "DATE_KEY", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "PROCESSED_AT")
    values
        ("INVOICE_NO", "STOCK_CODE", "CUSTOMER_ID", "DATE_KEY", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "PROCESSED_AT")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */;
[0m11:56:20.313870 [debug] [Thread-5 (]: SQL status: SUCCESS 30997 in 1.324 seconds
[0m11:56:20.325023 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e982d628-6bed-4abd-9256-80981693acf9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CF66811930>]}
[0m11:56:20.331101 [info ] [Thread-5 (]: 5 of 6 OK created sql table model _marts.dim_products .......................... [[32mSUCCESS 30997[0m in 1.47s]
[0m11:56:20.334679 [debug] [Thread-5 (]: Finished running node model.sales_pipelines.dim_products
[0m11:56:20.576897 [debug] [Thread-3 (]: SQL status: SUCCESS 41916 in 1.592 seconds
[0m11:56:20.576897 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e982d628-6bed-4abd-9256-80981693acf9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CF66EEC6D0>]}
[0m11:56:20.594711 [info ] [Thread-3 (]: 3 of 6 OK created sql table model _marts.dim_customers ......................... [[32mSUCCESS 41916[0m in 1.73s]
[0m11:56:20.601934 [debug] [Thread-3 (]: Finished running node model.sales_pipelines.dim_customers
[0m11:56:20.722293 [debug] [Thread-4 (]: SQL status: SUCCESS 3548 in 0.654 seconds
[0m11:56:20.722293 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:56:20.722293 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:56:21.185079 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.454 seconds
[0m11:56:21.197035 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._marts.fact_sales__dbt_tmp
[0m11:56:21.209346 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:56:21.209346 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: drop view if exists INCREMENTALETL._marts.fact_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:56:21.414245 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.203 seconds
[0m11:56:21.431288 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e982d628-6bed-4abd-9256-80981693acf9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CF670BA4D0>]}
[0m11:56:21.431288 [info ] [Thread-4 (]: 6 of 6 OK created sql incremental model _marts.fact_sales ...................... [[32mSUCCESS 3548[0m in 2.57s]
[0m11:56:21.441147 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.fact_sales
[0m11:56:21.447495 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:56:21.448609 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m11:56:21.449674 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m11:56:21.733457 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m11:56:21.733457 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m11:56:22.014002 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m11:56:22.027043 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m11:56:22.329839 [debug] [MainThread]: Connection 'list_INCREMENTALETL__processed' was left open.
[0m11:56:22.329839 [debug] [MainThread]: On list_INCREMENTALETL__processed: Close
[0m11:56:22.725702 [debug] [MainThread]: Connection 'list_INCREMENTALETL__staging' was left open.
[0m11:56:22.725702 [debug] [MainThread]: On list_INCREMENTALETL__staging: Close
[0m11:56:23.097434 [debug] [MainThread]: Connection 'list_INCREMENTALETL__marts' was left open.
[0m11:56:23.099208 [debug] [MainThread]: On list_INCREMENTALETL__marts: Close
[0m11:56:23.438446 [debug] [MainThread]: Connection 'model.sales_pipelines.dim_date' was left open.
[0m11:56:23.438446 [debug] [MainThread]: On model.sales_pipelines.dim_date: Close
[0m11:56:23.762810 [debug] [MainThread]: Connection 'model.sales_pipelines.fact_sales' was left open.
[0m11:56:23.762810 [debug] [MainThread]: On model.sales_pipelines.fact_sales: Close
[0m11:56:24.096280 [debug] [MainThread]: Connection 'model.sales_pipelines.dim_customers' was left open.
[0m11:56:24.096280 [debug] [MainThread]: On model.sales_pipelines.dim_customers: Close
[0m11:56:24.466051 [debug] [MainThread]: Connection 'model.sales_pipelines.dim_products' was left open.
[0m11:56:24.466051 [debug] [MainThread]: On model.sales_pipelines.dim_products: Close
[0m11:56:24.771901 [info ] [MainThread]: 
[0m11:56:24.771901 [info ] [MainThread]: Finished running 3 incremental models, 3 table models in 0 hours 0 minutes and 17.60 seconds (17.60s).
[0m11:56:24.771901 [debug] [MainThread]: Command end result
[0m11:56:24.829149 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m11:56:24.829149 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m11:56:24.847416 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\snowflake-incremental-pipeline\dbt_project\target\run_results.json
[0m11:56:24.847416 [info ] [MainThread]: 
[0m11:56:24.855234 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:56:24.855234 [info ] [MainThread]: 
[0m11:56:24.855234 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=6
[0m11:56:24.859060 [debug] [MainThread]: Command `dbt run` succeeded at 11:56:24.859060 after 20.39 seconds
[0m11:56:24.860234 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CF52EE76A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CF668820B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CF532ADE40>]}
[0m11:56:24.861631 [debug] [MainThread]: Flushing usage events
[0m11:56:26.328513 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:57:13.503451 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DDDDAB76D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DDDED62620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DDDED630D0>]}


============================== 11:57:13.507470 | a5cf51fe-c141-4ef2-b4f2-a3e743dea8ad ==============================
[0m11:57:13.507470 [info ] [MainThread]: Running with dbt=1.11.6
[0m11:57:13.509220 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'profiles_dir': 'D:\\snowflake-incremental-pipeline\\dbt_project', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'invocation_command': 'dbt run', 'use_colors': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'D:\\snowflake-incremental-pipeline\\dbt_project\\logs'}
[0m11:57:14.479996 [debug] [MainThread]: Snowflake adapter: Setting snowflake.connector to ERROR (file logging only)
[0m11:57:14.479996 [debug] [MainThread]: Snowflake adapter: Setting botocore to ERROR (file logging only)
[0m11:57:14.479996 [debug] [MainThread]: Snowflake adapter: Setting boto3 to ERROR (file logging only)
[0m11:57:14.846325 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a5cf51fe-c141-4ef2-b4f2-a3e743dea8ad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DDDB85EB00>]}
[0m11:57:14.924653 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a5cf51fe-c141-4ef2-b4f2-a3e743dea8ad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DDDE52B1C0>]}
[0m11:57:14.924653 [info ] [MainThread]: Registered adapter: snowflake=1.11.2
[0m11:57:15.442211 [debug] [MainThread]: checksum: ec27062d18352e9cfca161c166fcc73d2201661a23a3cf3c89196a27f32cfcb8, vars: {}, profile: , target: , version: 1.11.6
[0m11:57:15.674386 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:57:15.674386 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m11:57:15.674386 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:57:15.757567 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a5cf51fe-c141-4ef2-b4f2-a3e743dea8ad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DDF1468A60>]}
[0m11:57:15.903681 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m11:57:15.905688 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m11:57:15.925492 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a5cf51fe-c141-4ef2-b4f2-a3e743dea8ad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DDF13E1600>]}
[0m11:57:15.925492 [info ] [MainThread]: Found 6 models, 6 data tests, 1 source, 522 macros
[0m11:57:15.927241 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a5cf51fe-c141-4ef2-b4f2-a3e743dea8ad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DDF14689A0>]}
[0m11:57:15.928836 [info ] [MainThread]: 
[0m11:57:15.928836 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m11:57:15.933177 [info ] [MainThread]: 
[0m11:57:15.933883 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m11:57:15.943412 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m11:57:15.951947 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m11:57:15.964841 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m11:57:16.052379 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m11:57:16.052379 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m11:57:16.052379 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m11:57:16.052379 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m11:57:16.052379 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m11:57:16.052379 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m11:57:16.052379 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:57:16.052379 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:57:16.052379 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:57:17.821447 [debug] [ThreadPool]: SQL status: SUCCESS 5 in 1.761 seconds
[0m11:57:17.832164 [debug] [ThreadPool]: SQL status: SUCCESS 5 in 1.766 seconds
[0m11:57:17.842526 [debug] [ThreadPool]: SQL status: SUCCESS 5 in 1.777 seconds
[0m11:57:17.857711 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__processed'
[0m11:57:17.879551 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__staging'
[0m11:57:17.911303 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__processed"
[0m11:57:17.911303 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__marts'
[0m11:57:17.929370 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__staging"
[0m11:57:17.929370 [debug] [ThreadPool]: On list_INCREMENTALETL__processed: show objects in INCREMENTALETL._processed
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__processed"} */;
[0m11:57:17.943322 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__marts"
[0m11:57:17.943322 [debug] [ThreadPool]: On list_INCREMENTALETL__staging: show objects in INCREMENTALETL._staging
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__staging"} */;
[0m11:57:17.943322 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:57:17.943322 [debug] [ThreadPool]: On list_INCREMENTALETL__marts: show objects in INCREMENTALETL._marts
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__marts"} */;
[0m11:57:17.943322 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:57:17.959332 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:57:18.790271 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.833 seconds
[0m11:57:18.845461 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.891 seconds
[0m11:57:18.856945 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 0.896 seconds
[0m11:57:18.863853 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a5cf51fe-c141-4ef2-b4f2-a3e743dea8ad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DDEFF27C10>]}
[0m11:57:18.879123 [debug] [Thread-2 (]: Began running node model.sales_pipelines.stg_sales
[0m11:57:18.879123 [info ] [Thread-2 (]: 1 of 6 START sql incremental model _staging.stg_sales .......................... [RUN]
[0m11:57:18.879123 [debug] [Thread-2 (]: Acquiring new snowflake connection 'model.sales_pipelines.stg_sales'
[0m11:57:18.888941 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.stg_sales
[0m11:57:18.929934 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.stg_sales"
[0m11:57:18.930346 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.stg_sales
[0m11:57:18.999377 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:57:18.999377 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: create or replace  temporary view INCREMENTALETL._staging.stg_sales__dbt_tmp
  
  
  
  
  as (
    

WITH raw_data AS (
    SELECT * 
    FROM INCREMENTALETL.LANDINGZONE.RAW_SALES
),

staged AS (
    SELECT
        -- IDs
        CAST(INVOICENO AS VARCHAR)    AS invoice_no,
        CAST(STOCKCODE AS VARCHAR)    AS stock_code,
        CAST(DESCRIPTION AS VARCHAR)  AS description,

        -- Quantities & Prices
        TRY_CAST(QUANTITY AS INT)      AS quantity,
        TRY_CAST(UNITPRICE AS FLOAT)   AS unit_price,
        TRY_CAST(DISCOUNT AS FLOAT)    AS discount,
        TRY_CAST(SHIPPINGCOST AS FLOAT) AS shipping_cost,
        TRY_CAST(CUSTOMERID AS FLOAT)  AS customer_id,

        -- Convert once
        TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS') AS invoice_date,

        TRIM(UPPER(COUNTRY))            AS country,
        TRIM(UPPER(PAYMENTMETHOD))      AS payment_method,
        TRIM(UPPER(CATEGORY))           AS category,
        TRIM(UPPER(SALESCHANNEL))       AS sales_channel,
        TRIM(UPPER(RETURNSTATUS))       AS return_status,
        TRIM(UPPER(SHIPMENTPROVIDER))   AS shipment_provider,
        TRIM(UPPER(WAREHOUSELOCATION))  AS warehouse_location,
        TRIM(UPPER(ORDERPRIORITY))      AS order_priority,

        CURRENT_TIMESTAMP()             AS stg_loaded_at

    FROM raw_data
),

validated AS (
    SELECT
        *,
        YEAR(invoice_date)  AS invoice_year,
        MONTH(invoice_date) AS invoice_month,

        CASE
            WHEN invoice_date IS NULL           THEN 'INVALID_DATE'
            WHEN quantity IS NULL               THEN 'INVALID_QUANTITY'
            WHEN unit_price IS NULL             THEN 'INVALID_PRICE'
            WHEN shipping_cost < 0              THEN 'NEGATIVE_SHIPPING'
            WHEN discount < 0 OR discount > 1   THEN 'INVALID_DISCOUNT'
            WHEN invoice_no IS NULL             THEN 'NULL_INVOICE'
            ELSE 'VALID'
        END AS data_quality_flag
    FROM staged
)

SELECT *
FROM validated


WHERE NOT EXISTS (
    SELECT 1
    FROM INCREMENTALETL._staging.stg_sales t
    WHERE t.invoice_year  = validated.invoice_year
      AND t.invoice_month = validated.invoice_month
)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m11:57:18.999377 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m11:57:19.798520 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.789 seconds
[0m11:57:19.921758 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:57:19.923767 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:57:20.094802 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.172 seconds
[0m11:57:20.110585 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:57:20.110585 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:57:20.239178 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.129 seconds
[0m11:57:20.303655 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:57:20.303655 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:57:20.445580 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.138 seconds
[0m11:57:20.458156 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:57:20.458156 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table "INCREMENTALETL"."_STAGING"."STG_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:57:20.602270 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.133 seconds
[0m11:57:20.633995 [debug] [Thread-2 (]: 
    In "INCREMENTALETL"."_STAGING"."STG_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m11:57:20.681212 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.stg_sales"
[0m11:57:20.686781 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:57:20.686781 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m11:57:20.848427 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.160 seconds
[0m11:57:20.851036 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:57:20.851036 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: merge into INCREMENTALETL._staging.stg_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._staging.stg_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","DATA_QUALITY_FLAG" = DBT_INTERNAL_SOURCE."DATA_QUALITY_FLAG"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "INVOICE_YEAR", "INVOICE_MONTH", "DATA_QUALITY_FLAG")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "INVOICE_YEAR", "INVOICE_MONTH", "DATA_QUALITY_FLAG")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m11:57:21.792272 [debug] [Thread-2 (]: SQL status: SUCCESS 3054 in 0.934 seconds
[0m11:57:21.792272 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:57:21.792272 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:57:22.061971 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.264 seconds
[0m11:57:22.089568 [debug] [Thread-2 (]: Applying DROP to: INCREMENTALETL._staging.stg_sales__dbt_tmp
[0m11:57:22.105449 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m11:57:22.105449 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: drop view if exists INCREMENTALETL._staging.stg_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m11:57:22.274502 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.159 seconds
[0m11:57:22.326384 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a5cf51fe-c141-4ef2-b4f2-a3e743dea8ad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DDF1CC4A00>]}
[0m11:57:22.328392 [info ] [Thread-2 (]: 1 of 6 OK created sql incremental model _staging.stg_sales ..................... [[32mSUCCESS 3054[0m in 3.45s]
[0m11:57:22.330141 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.stg_sales
[0m11:57:22.331928 [debug] [Thread-4 (]: Began running node model.sales_pipelines.processed_sales
[0m11:57:22.332836 [info ] [Thread-4 (]: 2 of 6 START sql incremental model _processed.processed_sales .................. [RUN]
[0m11:57:22.333823 [debug] [Thread-4 (]: Acquiring new snowflake connection 'model.sales_pipelines.processed_sales'
[0m11:57:22.335195 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.processed_sales
[0m11:57:22.335195 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.processed_sales"
[0m11:57:22.335195 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.processed_sales
[0m11:57:22.348605 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:57:22.353185 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: create or replace  temporary view INCREMENTALETL._processed.processed_sales__dbt_tmp
  
  
  
  
  as (
    

WITH staging AS (
    SELECT * 
    FROM INCREMENTALETL._staging.stg_sales
),

deduped AS (
    SELECT *,
        ROW_NUMBER() OVER (
            PARTITION BY invoice_no, stock_code
            ORDER BY stg_loaded_at DESC
        ) AS row_num
    FROM staging
),

cleaned AS (
    SELECT
        invoice_no,
        stock_code,
        description,
        quantity,
        unit_price,
        discount,
        shipping_cost,
        customer_id,
        invoice_date,
        invoice_year,
        invoice_month,
        country,
        payment_method,
        category,
        sales_channel,
        return_status,
        shipment_provider,
        warehouse_location,
        order_priority,

        -- Derived metrics
        ROUND(quantity * unit_price, 2) AS gross_amount,
        ROUND(quantity * unit_price * (1 - discount), 2) AS net_amount,
        ROUND(
            quantity * unit_price * (1 - discount)
            + COALESCE(shipping_cost, 0),
        2) AS total_amount,

        CASE WHEN return_status = 'RETURNED' THEN TRUE ELSE FALSE END AS is_returned,
        CASE WHEN customer_id IS NULL THEN TRUE ELSE FALSE END AS is_guest_customer,

        stg_loaded_at,
        CURRENT_TIMESTAMP() AS processed_at

    FROM deduped
    WHERE
        row_num = 1
        AND data_quality_flag = 'VALID'
        AND invoice_date IS NOT NULL
)

SELECT *
FROM cleaned


WHERE NOT EXISTS (
    SELECT 1
    FROM INCREMENTALETL._processed.processed_sales t
    WHERE t.invoice_year  = cleaned.invoice_year
      AND t.invoice_month = cleaned.invoice_month
)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m11:57:22.353185 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m11:57:23.239240 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.894 seconds
[0m11:57:23.255111 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:57:23.255111 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:57:23.437644 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.194 seconds
[0m11:57:23.469818 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:57:23.469818 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:57:23.757915 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.275 seconds
[0m11:57:23.764215 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:57:23.764215 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:57:23.925948 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.154 seconds
[0m11:57:23.945590 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:57:23.945590 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:57:24.167887 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.213 seconds
[0m11:57:24.167887 [debug] [Thread-4 (]: 
    In "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m11:57:24.183931 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.processed_sales"
[0m11:57:24.187943 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:57:24.187943 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m11:57:24.363596 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.183 seconds
[0m11:57:24.377013 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:57:24.377013 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: merge into INCREMENTALETL._processed.processed_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._processed.processed_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","GROSS_AMOUNT" = DBT_INTERNAL_SOURCE."GROSS_AMOUNT","NET_AMOUNT" = DBT_INTERNAL_SOURCE."NET_AMOUNT","TOTAL_AMOUNT" = DBT_INTERNAL_SOURCE."TOTAL_AMOUNT","IS_RETURNED" = DBT_INTERNAL_SOURCE."IS_RETURNED","IS_GUEST_CUSTOMER" = DBT_INTERNAL_SOURCE."IS_GUEST_CUSTOMER","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","PROCESSED_AT" = DBT_INTERNAL_SOURCE."PROCESSED_AT"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m11:57:25.492622 [debug] [Thread-4 (]: SQL status: SUCCESS 2985 in 1.115 seconds
[0m11:57:25.492622 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:57:25.492622 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:57:25.792640 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.303 seconds
[0m11:57:25.807716 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._processed.processed_sales__dbt_tmp
[0m11:57:25.807716 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m11:57:25.807716 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: drop view if exists INCREMENTALETL._processed.processed_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m11:57:26.002692 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.192 seconds
[0m11:57:26.018265 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a5cf51fe-c141-4ef2-b4f2-a3e743dea8ad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DDDC939720>]}
[0m11:57:26.018265 [info ] [Thread-4 (]: 2 of 6 OK created sql incremental model _processed.processed_sales ............. [[32mSUCCESS 2985[0m in 3.68s]
[0m11:57:26.027831 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.processed_sales
[0m11:57:26.032380 [debug] [Thread-3 (]: Began running node model.sales_pipelines.dim_customers
[0m11:57:26.035902 [debug] [Thread-2 (]: Began running node model.sales_pipelines.dim_date
[0m11:57:26.039014 [debug] [Thread-5 (]: Began running node model.sales_pipelines.dim_products
[0m11:57:26.040963 [debug] [Thread-4 (]: Began running node model.sales_pipelines.fact_sales
[0m11:57:26.040963 [info ] [Thread-3 (]: 3 of 6 START sql table model _marts.dim_customers .............................. [RUN]
[0m11:57:26.048613 [info ] [Thread-2 (]: 4 of 6 START sql table model _marts.dim_date ................................... [RUN]
[0m11:57:26.051093 [info ] [Thread-5 (]: 5 of 6 START sql table model _marts.dim_products ............................... [RUN]
[0m11:57:26.053648 [info ] [Thread-4 (]: 6 of 6 START sql incremental model _marts.fact_sales ........................... [RUN]
[0m11:57:26.057112 [debug] [Thread-3 (]: Acquiring new snowflake connection 'model.sales_pipelines.dim_customers'
[0m11:57:26.057112 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.sales_pipelines.stg_sales, now model.sales_pipelines.dim_date)
[0m11:57:26.057112 [debug] [Thread-5 (]: Acquiring new snowflake connection 'model.sales_pipelines.dim_products'
[0m11:57:26.061526 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.sales_pipelines.processed_sales, now model.sales_pipelines.fact_sales)
[0m11:57:26.063104 [debug] [Thread-3 (]: Began compiling node model.sales_pipelines.dim_customers
[0m11:57:26.065499 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.dim_date
[0m11:57:26.067432 [debug] [Thread-5 (]: Began compiling node model.sales_pipelines.dim_products
[0m11:57:26.067432 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.fact_sales
[0m11:57:26.075812 [debug] [Thread-3 (]: Writing injected SQL for node "model.sales_pipelines.dim_customers"
[0m11:57:26.082187 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.dim_date"
[0m11:57:26.091713 [debug] [Thread-5 (]: Writing injected SQL for node "model.sales_pipelines.dim_products"
[0m11:57:26.097237 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.fact_sales"
[0m11:57:26.097237 [debug] [Thread-3 (]: Began executing node model.sales_pipelines.dim_customers
[0m11:57:26.097237 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.dim_date
[0m11:57:26.097237 [debug] [Thread-5 (]: Began executing node model.sales_pipelines.dim_products
[0m11:57:26.148001 [debug] [Thread-3 (]: Writing runtime sql for node "model.sales_pipelines.dim_customers"
[0m11:57:26.150974 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.dim_date"
[0m11:57:26.155062 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.fact_sales
[0m11:57:26.155062 [debug] [Thread-5 (]: Writing runtime sql for node "model.sales_pipelines.dim_products"
[0m11:57:26.164564 [debug] [Thread-3 (]: Using snowflake connection "model.sales_pipelines.dim_customers"
[0m11:57:26.170834 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:57:26.170834 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.dim_date"
[0m11:57:26.170834 [debug] [Thread-3 (]: On model.sales_pipelines.dim_customers: create or replace transient table INCREMENTALETL._marts.dim_customers
    
    
    
    as (

SELECT DISTINCT
    customer_id,
    country,
    payment_method,
    sales_channel,
    is_guest_customer,
    MIN(invoice_date) AS first_purchase_date,
    MAX(invoice_date) AS last_purchase_date,
    COUNT(DISTINCT invoice_no) AS total_orders
FROM INCREMENTALETL._processed.processed_sales
WHERE customer_id IS NOT NULL
GROUP BY
    customer_id,
    country,
    payment_method,
    sales_channel,
    is_guest_customer
    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.dim_customers"} */;
[0m11:57:26.170834 [debug] [Thread-5 (]: Using snowflake connection "model.sales_pipelines.dim_products"
[0m11:57:26.181470 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: create or replace  temporary view INCREMENTALETL._marts.fact_sales__dbt_tmp
  
  
  
  
  as (
    

SELECT
    invoice_no,
    stock_code,
    customer_id,
    invoice_date::DATE AS date_key,

    quantity,
    unit_price,
    discount,
    shipping_cost,

    gross_amount,
    net_amount,
    total_amount,

    is_returned,
    processed_at

FROM INCREMENTALETL._processed.processed_sales


WHERE processed_at > (SELECT MAX(processed_at) FROM INCREMENTALETL._marts.fact_sales)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */;
[0m11:57:26.183215 [debug] [Thread-2 (]: On model.sales_pipelines.dim_date: create or replace transient table INCREMENTALETL._marts.dim_date
    
    
    
    as (

SELECT DISTINCT
    invoice_date::DATE AS date_key,
    YEAR(invoice_date)  AS year,
    MONTH(invoice_date) AS month,
    DAY(invoice_date)   AS day,
    DAYNAME(invoice_date) AS day_name,
    WEEK(invoice_date)  AS week_number,
    QUARTER(invoice_date) AS quarter
FROM INCREMENTALETL._processed.processed_sales
    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.dim_date"} */;
[0m11:57:26.183215 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m11:57:26.183215 [debug] [Thread-5 (]: On model.sales_pipelines.dim_products: create or replace transient table INCREMENTALETL._marts.dim_products
    
    
    
    as (

SELECT DISTINCT
    stock_code,
    description,
    category
FROM INCREMENTALETL._processed.processed_sales
    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.dim_products"} */;
[0m11:57:26.186966 [debug] [Thread-5 (]: Opening a new connection, currently in state init
[0m11:57:26.427117 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.253 seconds
[0m11:57:26.442826 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:57:26.442826 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: describe table INCREMENTALETL._marts.fact_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:57:26.606742 [debug] [Thread-4 (]: SQL status: SUCCESS 13 in 0.160 seconds
[0m11:57:26.606742 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:57:26.618417 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: describe table INCREMENTALETL._marts.fact_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:57:26.794181 [debug] [Thread-4 (]: SQL status: SUCCESS 13 in 0.167 seconds
[0m11:57:26.804312 [debug] [Thread-2 (]: SQL status: SUCCESS 2075 in 0.618 seconds
[0m11:57:26.820222 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:57:26.858102 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a5cf51fe-c141-4ef2-b4f2-a3e743dea8ad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DDF1C95090>]}
[0m11:57:26.860122 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: describe table "INCREMENTALETL"."_MARTS"."FACT_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:57:26.860122 [info ] [Thread-2 (]: 4 of 6 OK created sql table model _marts.dim_date .............................. [[32mSUCCESS 2075[0m in 0.80s]
[0m11:57:26.875343 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.dim_date
[0m11:57:27.020276 [debug] [Thread-4 (]: SQL status: SUCCESS 13 in 0.153 seconds
[0m11:57:27.036367 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.fact_sales"
[0m11:57:27.048526 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:57:27.052078 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */;
[0m11:57:27.214026 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.159 seconds
[0m11:57:27.214026 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:57:27.214026 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: merge into INCREMENTALETL._marts.fact_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._marts.fact_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","DATE_KEY" = DBT_INTERNAL_SOURCE."DATE_KEY","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","GROSS_AMOUNT" = DBT_INTERNAL_SOURCE."GROSS_AMOUNT","NET_AMOUNT" = DBT_INTERNAL_SOURCE."NET_AMOUNT","TOTAL_AMOUNT" = DBT_INTERNAL_SOURCE."TOTAL_AMOUNT","IS_RETURNED" = DBT_INTERNAL_SOURCE."IS_RETURNED","PROCESSED_AT" = DBT_INTERNAL_SOURCE."PROCESSED_AT"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "CUSTOMER_ID", "DATE_KEY", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "PROCESSED_AT")
    values
        ("INVOICE_NO", "STOCK_CODE", "CUSTOMER_ID", "DATE_KEY", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "PROCESSED_AT")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */;
[0m11:57:27.574206 [debug] [Thread-5 (]: SQL status: SUCCESS 32293 in 1.390 seconds
[0m11:57:27.587727 [debug] [Thread-5 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a5cf51fe-c141-4ef2-b4f2-a3e743dea8ad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DDF1D00580>]}
[0m11:57:27.587727 [info ] [Thread-5 (]: 5 of 6 OK created sql table model _marts.dim_products .......................... [[32mSUCCESS 32293[0m in 1.53s]
[0m11:57:27.587727 [debug] [Thread-5 (]: Finished running node model.sales_pipelines.dim_products
[0m11:57:27.614740 [debug] [Thread-3 (]: SQL status: SUCCESS 44640 in 1.430 seconds
[0m11:57:27.616638 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a5cf51fe-c141-4ef2-b4f2-a3e743dea8ad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DDF13CF0D0>]}
[0m11:57:27.616638 [info ] [Thread-3 (]: 3 of 6 OK created sql table model _marts.dim_customers ......................... [[32mSUCCESS 44640[0m in 1.56s]
[0m11:57:27.632255 [debug] [Thread-3 (]: Finished running node model.sales_pipelines.dim_customers
[0m11:57:27.952812 [debug] [Thread-4 (]: SQL status: SUCCESS 2985 in 0.737 seconds
[0m11:57:27.952812 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:57:27.965491 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:57:28.968329 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 1.011 seconds
[0m11:57:28.984452 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._marts.fact_sales__dbt_tmp
[0m11:57:28.984452 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.fact_sales"
[0m11:57:28.991281 [debug] [Thread-4 (]: On model.sales_pipelines.fact_sales: drop view if exists INCREMENTALETL._marts.fact_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.fact_sales"} */
[0m11:57:29.278447 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.287 seconds
[0m11:57:29.290572 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a5cf51fe-c141-4ef2-b4f2-a3e743dea8ad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DDF1E1BBB0>]}
[0m11:57:29.290572 [info ] [Thread-4 (]: 6 of 6 OK created sql incremental model _marts.fact_sales ...................... [[32mSUCCESS 2985[0m in 3.23s]
[0m11:57:29.299492 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.fact_sales
[0m11:57:29.306661 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:57:29.308598 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m11:57:29.311923 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m11:57:34.615241 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m11:57:34.615241 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m11:57:34.861746 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m11:57:34.861746 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m11:57:35.223814 [debug] [MainThread]: Connection 'list_INCREMENTALETL__processed' was left open.
[0m11:57:35.223814 [debug] [MainThread]: On list_INCREMENTALETL__processed: Close
[0m11:57:35.641372 [debug] [MainThread]: Connection 'list_INCREMENTALETL__staging' was left open.
[0m11:57:35.645421 [debug] [MainThread]: On list_INCREMENTALETL__staging: Close
[0m11:57:35.923125 [debug] [MainThread]: Connection 'list_INCREMENTALETL__marts' was left open.
[0m11:57:35.923125 [debug] [MainThread]: On list_INCREMENTALETL__marts: Close
[0m11:57:36.244779 [debug] [MainThread]: Connection 'model.sales_pipelines.dim_date' was left open.
[0m11:57:36.244779 [debug] [MainThread]: On model.sales_pipelines.dim_date: Close
[0m11:57:36.521466 [debug] [MainThread]: Connection 'model.sales_pipelines.fact_sales' was left open.
[0m11:57:36.521466 [debug] [MainThread]: On model.sales_pipelines.fact_sales: Close
[0m11:57:36.862915 [debug] [MainThread]: Connection 'model.sales_pipelines.dim_customers' was left open.
[0m11:57:36.862915 [debug] [MainThread]: On model.sales_pipelines.dim_customers: Close
[0m11:57:37.184851 [debug] [MainThread]: Connection 'model.sales_pipelines.dim_products' was left open.
[0m11:57:37.184851 [debug] [MainThread]: On model.sales_pipelines.dim_products: Close
[0m11:57:37.473911 [info ] [MainThread]: 
[0m11:57:37.473911 [info ] [MainThread]: Finished running 3 incremental models, 3 table models in 0 hours 0 minutes and 21.54 seconds (21.54s).
[0m11:57:37.487237 [debug] [MainThread]: Command end result
[0m11:57:37.567833 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m11:57:37.569232 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m11:57:37.581567 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\snowflake-incremental-pipeline\dbt_project\target\run_results.json
[0m11:57:37.582897 [info ] [MainThread]: 
[0m11:57:37.582897 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:57:37.584697 [info ] [MainThread]: 
[0m11:57:37.586792 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=6
[0m11:57:37.590434 [debug] [MainThread]: Command `dbt run` succeeded at 11:57:37.590434 after 24.21 seconds
[0m11:57:37.591627 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DDDDAB76D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DDDED62680>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DDF14521D0>]}
[0m11:57:37.593144 [debug] [MainThread]: Flushing usage events
[0m11:57:39.312449 [debug] [MainThread]: An error was encountered while trying to flush usage events
