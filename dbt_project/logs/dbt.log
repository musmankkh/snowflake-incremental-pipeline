[0m09:51:39.857485 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B28E437670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B28F6B7700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B28F6B7910>]}


============================== 09:51:39.858337 | ca4dcbb1-d285-4e82-851d-c8cd40cd5144 ==============================
[0m09:51:39.858337 [info ] [MainThread]: Running with dbt=1.11.6
[0m09:51:39.858337 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'None', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'profiles_dir': 'D:\\snowflake-incremental-pipeline\\dbt_project', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'indirect_selection': 'eager', 'invocation_command': 'dbt debug', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'D:\\snowflake-incremental-pipeline\\dbt_project\\logs'}
[0m09:51:39.889326 [info ] [MainThread]: dbt version: 1.11.6
[0m09:51:39.891422 [info ] [MainThread]: python version: 3.10.11
[0m09:51:39.891422 [info ] [MainThread]: python path: C:\Users\Alien\AppData\Local\Programs\Python\Python310\python.exe
[0m09:51:39.891422 [info ] [MainThread]: os info: Windows-10-10.0.26200-SP0
[0m09:51:39.897062 [info ] [MainThread]: Using profiles dir at D:\snowflake-incremental-pipeline\dbt_project
[0m09:51:39.897062 [info ] [MainThread]: Using profiles.yml file at D:\snowflake-incremental-pipeline\dbt_project\profiles.yml
[0m09:51:39.897062 [info ] [MainThread]: Using dbt_project.yml file at D:\snowflake-incremental-pipeline\dbt_project\dbt_project.yml
[0m09:51:40.009748 [info ] [MainThread]: Configuration:
[0m09:51:40.009748 [info ] [MainThread]:   profiles.yml file [[31mERROR invalid[0m]
[0m09:51:40.009748 [info ] [MainThread]:   dbt_project.yml file [[31mERROR invalid[0m]
[0m09:51:40.009748 [info ] [MainThread]: Required dependencies:
[0m09:51:40.024109 [debug] [MainThread]: Executing "git --help"
[0m09:51:40.085107 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--no-lazy-fetch]\n           [--no-optional-locks] [--no-advice] [--bare] [--git-dir=<path>]\n           [--work-tree=<path>] [--namespace=<name>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone      Clone a repository into a new directory\n   init       Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add        Add file contents to the index\n   mv         Move or rename a file, a directory, or a symlink\n   restore    Restore working tree files\n   rm         Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect     Use binary search to find the commit that introduced a bug\n   diff       Show changes between commits, commit and working tree, etc\n   grep       Print lines matching a pattern\n   log        Show commit logs\n   show       Show various types of objects\n   status     Show the working tree status\n\ngrow, mark and tweak your common history\n   backfill   Download missing objects in a partial clone\n   branch     List, create, or delete branches\n   commit     Record changes to the repository\n   merge      Join two or more development histories together\n   rebase     Reapply commits on top of another base tip\n   reset      Reset current HEAD to the specified state\n   switch     Switch branches\n   tag        Create, list, delete or verify tags\n\ncollaborate (see also: git help workflows)\n   fetch      Download objects and refs from another repository\n   pull       Fetch from and integrate with another repository or a local branch\n   push       Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m09:51:40.086857 [debug] [MainThread]: STDERR: "b''"
[0m09:51:40.086857 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m09:51:40.088676 [info ] [MainThread]: Connection test skipped since no profile was found
[0m09:51:40.088676 [info ] [MainThread]: [31m2 checks failed:[0m
[0m09:51:40.088676 [info ] [MainThread]: Profile loading failed for the following reason:
Runtime Error
  Could not find profile named 'sales_pipelines'


[0m09:51:40.088676 [info ] [MainThread]: Project loading failed for the following reason:
Runtime Error
  at path []: Additional properties are not allowed ('macro-path', 'model-path', 'test-path' were unexpected)

Error encountered in D:\snowflake-incremental-pipeline\dbt_project\dbt_project.yml


[0m09:51:40.093567 [debug] [MainThread]: Command `dbt debug` failed at 09:51:40.093567 after 0.46 seconds
[0m09:51:40.095361 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B28E437670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B28F6B7A00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B28F740AF0>]}
[0m09:51:40.095361 [debug] [MainThread]: Flushing usage events
[0m09:51:41.195547 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m09:52:57.752114 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002124D4C7640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002124E7866B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002124E784A90>]}


============================== 09:52:57.752114 | 931b5c7d-649f-49c7-adf0-22f49484d699 ==============================
[0m09:52:57.752114 [info ] [MainThread]: Running with dbt=1.11.6
[0m09:52:57.752114 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'None', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'profiles_dir': 'D:\\snowflake-incremental-pipeline\\dbt_project', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'indirect_selection': 'eager', 'invocation_command': 'dbt debug', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'D:\\snowflake-incremental-pipeline\\dbt_project\\logs'}
[0m09:52:57.768212 [info ] [MainThread]: dbt version: 1.11.6
[0m09:52:57.768212 [info ] [MainThread]: python version: 3.10.11
[0m09:52:57.768212 [info ] [MainThread]: python path: C:\Users\Alien\AppData\Local\Programs\Python\Python310\python.exe
[0m09:52:57.784321 [info ] [MainThread]: os info: Windows-10-10.0.26200-SP0
[0m09:52:57.784321 [error] [MainThread]: Encountered an error:
Parsing Error
  Env var required but not provided: 'SNOWFLAKE_ACCOUNT'
[0m09:52:57.800140 [debug] [MainThread]: Command `dbt debug` failed at 09:52:57.800140 after 0.23 seconds
[0m09:52:57.800140 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002124D4C7640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002124E5C9810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002124DF37700>]}
[0m09:52:57.800140 [debug] [MainThread]: Flushing usage events
[0m09:53:00.144878 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m09:54:10.051976 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AA9A1376D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AA9B3F4370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AA9B3F5B70>]}


============================== 09:54:10.055155 | 8c7b6de8-9ec2-4769-9c8a-4840f1d4f7d4 ==============================
[0m09:54:10.055155 [info ] [MainThread]: Running with dbt=1.11.6
[0m09:54:10.055155 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'empty': 'None', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'profiles_dir': 'D:\\snowflake-incremental-pipeline\\dbt_project', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'use_colors': 'True', 'indirect_selection': 'eager', 'invocation_command': 'dbt debug', 'use_experimental_parser': 'False', 'log_path': 'D:\\snowflake-incremental-pipeline\\dbt_project\\logs'}
[0m09:54:10.071985 [info ] [MainThread]: dbt version: 1.11.6
[0m09:54:10.071985 [info ] [MainThread]: python version: 3.10.11
[0m09:54:10.071985 [info ] [MainThread]: python path: C:\Users\Alien\AppData\Local\Programs\Python\Python310\python.exe
[0m09:54:10.071985 [info ] [MainThread]: os info: Windows-10-10.0.26200-SP0
[0m09:54:11.076726 [debug] [MainThread]: Snowflake adapter: Setting snowflake.connector to ERROR (file logging only)
[0m09:54:11.085061 [debug] [MainThread]: Snowflake adapter: Setting botocore to ERROR (file logging only)
[0m09:54:11.085061 [debug] [MainThread]: Snowflake adapter: Setting boto3 to ERROR (file logging only)
[0m09:54:11.174054 [info ] [MainThread]: Using profiles dir at D:\snowflake-incremental-pipeline\dbt_project
[0m09:54:11.174054 [info ] [MainThread]: Using profiles.yml file at D:\snowflake-incremental-pipeline\dbt_project\profiles.yml
[0m09:54:11.174054 [info ] [MainThread]: Using dbt_project.yml file at D:\snowflake-incremental-pipeline\dbt_project\dbt_project.yml
[0m09:54:11.174054 [info ] [MainThread]: adapter type: snowflake
[0m09:54:11.180460 [info ] [MainThread]: adapter version: 1.11.2
[0m09:54:11.280476 [info ] [MainThread]: Configuration:
[0m09:54:11.280476 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m09:54:11.280476 [info ] [MainThread]:   dbt_project.yml file [[31mERROR invalid[0m]
[0m09:54:11.280476 [info ] [MainThread]: Required dependencies:
[0m09:54:11.280476 [debug] [MainThread]: Executing "git --help"
[0m09:54:11.348021 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--no-lazy-fetch]\n           [--no-optional-locks] [--no-advice] [--bare] [--git-dir=<path>]\n           [--work-tree=<path>] [--namespace=<name>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone      Clone a repository into a new directory\n   init       Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add        Add file contents to the index\n   mv         Move or rename a file, a directory, or a symlink\n   restore    Restore working tree files\n   rm         Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect     Use binary search to find the commit that introduced a bug\n   diff       Show changes between commits, commit and working tree, etc\n   grep       Print lines matching a pattern\n   log        Show commit logs\n   show       Show various types of objects\n   status     Show the working tree status\n\ngrow, mark and tweak your common history\n   backfill   Download missing objects in a partial clone\n   branch     List, create, or delete branches\n   commit     Record changes to the repository\n   merge      Join two or more development histories together\n   rebase     Reapply commits on top of another base tip\n   reset      Reset current HEAD to the specified state\n   switch     Switch branches\n   tag        Create, list, delete or verify tags\n\ncollaborate (see also: git help workflows)\n   fetch      Download objects and refs from another repository\n   pull       Fetch from and integrate with another repository or a local branch\n   push       Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m09:54:11.348021 [debug] [MainThread]: STDERR: "b''"
[0m09:54:11.348021 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m09:54:11.348021 [info ] [MainThread]: Connection:
[0m09:54:11.348021 [info ] [MainThread]:   account: qsnwhfd-ts32426
[0m09:54:11.353903 [info ] [MainThread]:   user: musmankkh456
[0m09:54:11.355330 [info ] [MainThread]:   database: INCREMENTALETL
[0m09:54:11.355330 [info ] [MainThread]:   warehouse: MY_WH
[0m09:54:11.355330 [info ] [MainThread]:   role: ACCOUNTADMIN
[0m09:54:11.358305 [info ] [MainThread]:   schema: LANDINGZONE
[0m09:54:11.358305 [info ] [MainThread]:   authenticator: None
[0m09:54:11.358305 [info ] [MainThread]:   oauth_client_id: None
[0m09:54:11.360557 [info ] [MainThread]:   query_tag: None
[0m09:54:11.360557 [info ] [MainThread]:   client_session_keep_alive: False
[0m09:54:11.360557 [info ] [MainThread]:   host: None
[0m09:54:11.360557 [info ] [MainThread]:   port: None
[0m09:54:11.360557 [info ] [MainThread]:   proxy_host: None
[0m09:54:11.366215 [info ] [MainThread]:   proxy_port: None
[0m09:54:11.367091 [info ] [MainThread]:   protocol: None
[0m09:54:11.367091 [info ] [MainThread]:   connect_retries: 1
[0m09:54:11.368900 [info ] [MainThread]:   connect_timeout: None
[0m09:54:11.369913 [info ] [MainThread]:   retry_on_database_errors: False
[0m09:54:11.371944 [info ] [MainThread]:   retry_all: False
[0m09:54:11.372371 [info ] [MainThread]:   insecure_mode: False
[0m09:54:11.372371 [info ] [MainThread]:   reuse_connections: True
[0m09:54:11.372371 [info ] [MainThread]:   s3_stage_vpce_dns_name: None
[0m09:54:11.372371 [info ] [MainThread]:   platform_detection_timeout_seconds: 0.0
[0m09:54:11.378145 [info ] [MainThread]: Registered adapter: snowflake=1.11.2
[0m09:54:11.916797 [debug] [MainThread]: Acquiring new snowflake connection 'debug'
[0m09:54:12.020624 [debug] [MainThread]: Using snowflake connection "debug"
[0m09:54:12.022678 [debug] [MainThread]: On debug: select 1 as id
[0m09:54:12.024492 [debug] [MainThread]: Opening a new connection, currently in state init
[0m09:54:13.653644 [debug] [MainThread]: SQL status: SUCCESS 1 in 1.629 seconds
[0m09:54:13.656183 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m09:54:13.656183 [info ] [MainThread]: [31m1 check failed:[0m
[0m09:54:13.664890 [info ] [MainThread]: Project loading failed for the following reason:
Runtime Error
  at path []: Additional properties are not allowed ('macro-path', 'model-path', 'test-path' were unexpected)

Error encountered in D:\snowflake-incremental-pipeline\dbt_project\dbt_project.yml


[0m09:54:13.671895 [debug] [MainThread]: Command `dbt debug` failed at 09:54:13.664890 after 3.78 seconds
[0m09:54:13.672307 [debug] [MainThread]: Connection 'debug' was left open.
[0m09:54:13.672307 [debug] [MainThread]: On debug: Close
[0m09:54:13.963254 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AA9A1376D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AAAC9EBF70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AAAC9EBB20>]}
[0m09:54:13.963254 [debug] [MainThread]: Flushing usage events
[0m09:54:15.086977 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m09:54:55.451800 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028CC4167670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028CC5424220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028CC5425B40>]}


============================== 09:54:55.456238 | deaccf6e-1741-4eb3-bf3f-cc0f91e94b85 ==============================
[0m09:54:55.456238 [info ] [MainThread]: Running with dbt=1.11.6
[0m09:54:55.463650 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'None', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'profiles_dir': 'D:\\snowflake-incremental-pipeline\\dbt_project', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'indirect_selection': 'eager', 'invocation_command': 'dbt debug', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'D:\\snowflake-incremental-pipeline\\dbt_project\\logs'}
[0m09:54:55.489029 [info ] [MainThread]: dbt version: 1.11.6
[0m09:54:55.489029 [info ] [MainThread]: python version: 3.10.11
[0m09:54:55.496222 [info ] [MainThread]: python path: C:\Users\Alien\AppData\Local\Programs\Python\Python310\python.exe
[0m09:54:55.497199 [info ] [MainThread]: os info: Windows-10-10.0.26200-SP0
[0m09:54:56.468483 [debug] [MainThread]: Snowflake adapter: Setting snowflake.connector to ERROR (file logging only)
[0m09:54:56.470495 [debug] [MainThread]: Snowflake adapter: Setting botocore to ERROR (file logging only)
[0m09:54:56.470495 [debug] [MainThread]: Snowflake adapter: Setting boto3 to ERROR (file logging only)
[0m09:54:56.560284 [info ] [MainThread]: Using profiles dir at D:\snowflake-incremental-pipeline\dbt_project
[0m09:54:56.560284 [info ] [MainThread]: Using profiles.yml file at D:\snowflake-incremental-pipeline\dbt_project\profiles.yml
[0m09:54:56.560284 [info ] [MainThread]: Using dbt_project.yml file at D:\snowflake-incremental-pipeline\dbt_project\dbt_project.yml
[0m09:54:56.560284 [info ] [MainThread]: adapter type: snowflake
[0m09:54:56.560284 [info ] [MainThread]: adapter version: 1.11.2
[0m09:54:56.713728 [info ] [MainThread]: Configuration:
[0m09:54:56.713728 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m09:54:56.713728 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m09:54:56.729528 [info ] [MainThread]: Required dependencies:
[0m09:54:56.730588 [debug] [MainThread]: Executing "git --help"
[0m09:54:56.784273 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--no-lazy-fetch]\n           [--no-optional-locks] [--no-advice] [--bare] [--git-dir=<path>]\n           [--work-tree=<path>] [--namespace=<name>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone      Clone a repository into a new directory\n   init       Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add        Add file contents to the index\n   mv         Move or rename a file, a directory, or a symlink\n   restore    Restore working tree files\n   rm         Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect     Use binary search to find the commit that introduced a bug\n   diff       Show changes between commits, commit and working tree, etc\n   grep       Print lines matching a pattern\n   log        Show commit logs\n   show       Show various types of objects\n   status     Show the working tree status\n\ngrow, mark and tweak your common history\n   backfill   Download missing objects in a partial clone\n   branch     List, create, or delete branches\n   commit     Record changes to the repository\n   merge      Join two or more development histories together\n   rebase     Reapply commits on top of another base tip\n   reset      Reset current HEAD to the specified state\n   switch     Switch branches\n   tag        Create, list, delete or verify tags\n\ncollaborate (see also: git help workflows)\n   fetch      Download objects and refs from another repository\n   pull       Fetch from and integrate with another repository or a local branch\n   push       Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m09:54:56.784273 [debug] [MainThread]: STDERR: "b''"
[0m09:54:56.784273 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m09:54:56.784273 [info ] [MainThread]: Connection:
[0m09:54:56.784273 [info ] [MainThread]:   account: qsnwhfd-ts32426
[0m09:54:56.784273 [info ] [MainThread]:   user: musmankkh456
[0m09:54:56.784273 [info ] [MainThread]:   database: INCREMENTALETL
[0m09:54:56.784273 [info ] [MainThread]:   warehouse: MY_WH
[0m09:54:56.784273 [info ] [MainThread]:   role: ACCOUNTADMIN
[0m09:54:56.793134 [info ] [MainThread]:   schema: LANDINGZONE
[0m09:54:56.794457 [info ] [MainThread]:   authenticator: None
[0m09:54:56.794457 [info ] [MainThread]:   oauth_client_id: None
[0m09:54:56.794457 [info ] [MainThread]:   query_tag: None
[0m09:54:56.794457 [info ] [MainThread]:   client_session_keep_alive: False
[0m09:54:56.794457 [info ] [MainThread]:   host: None
[0m09:54:56.794457 [info ] [MainThread]:   port: None
[0m09:54:56.794457 [info ] [MainThread]:   proxy_host: None
[0m09:54:56.794457 [info ] [MainThread]:   proxy_port: None
[0m09:54:56.794457 [info ] [MainThread]:   protocol: None
[0m09:54:56.794457 [info ] [MainThread]:   connect_retries: 1
[0m09:54:56.794457 [info ] [MainThread]:   connect_timeout: None
[0m09:54:56.794457 [info ] [MainThread]:   retry_on_database_errors: False
[0m09:54:56.794457 [info ] [MainThread]:   retry_all: False
[0m09:54:56.794457 [info ] [MainThread]:   insecure_mode: False
[0m09:54:56.794457 [info ] [MainThread]:   reuse_connections: True
[0m09:54:56.809238 [info ] [MainThread]:   s3_stage_vpce_dns_name: None
[0m09:54:56.810404 [info ] [MainThread]:   platform_detection_timeout_seconds: 0.0
[0m09:54:56.810404 [info ] [MainThread]: Registered adapter: snowflake=1.11.2
[0m09:54:57.349717 [debug] [MainThread]: Acquiring new snowflake connection 'debug'
[0m09:54:57.405190 [debug] [MainThread]: Using snowflake connection "debug"
[0m09:54:57.421024 [debug] [MainThread]: On debug: select 1 as id
[0m09:54:57.421024 [debug] [MainThread]: Opening a new connection, currently in state init
[0m09:54:58.816175 [debug] [MainThread]: SQL status: SUCCESS 1 in 1.396 seconds
[0m09:54:58.816175 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m09:54:58.816175 [info ] [MainThread]: [32mAll checks passed![0m
[0m09:54:58.816175 [debug] [MainThread]: Command `dbt debug` succeeded at 09:54:58.816175 after 3.56 seconds
[0m09:54:58.832191 [debug] [MainThread]: Connection 'debug' was left open.
[0m09:54:58.832191 [debug] [MainThread]: On debug: Close
[0m09:54:59.226940 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028CC4167670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028CC44F9600>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028CD7DE7FD0>]}
[0m09:54:59.226940 [debug] [MainThread]: Flushing usage events
[0m09:55:00.455480 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m09:55:08.298675 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AB3E1176A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AB3F3C1000>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AB3F3C23B0>]}


============================== 09:55:08.301795 | cf264312-ac92-4197-ba55-e15dd02858b2 ==============================
[0m09:55:08.301795 [info ] [MainThread]: Running with dbt=1.11.6
[0m09:55:08.301795 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'D:\\snowflake-incremental-pipeline\\dbt_project', 'invocation_command': 'dbt run', 'use_colors': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'D:\\snowflake-incremental-pipeline\\dbt_project\\logs'}
[0m09:55:09.310172 [debug] [MainThread]: Snowflake adapter: Setting snowflake.connector to ERROR (file logging only)
[0m09:55:09.318233 [debug] [MainThread]: Snowflake adapter: Setting botocore to ERROR (file logging only)
[0m09:55:09.318233 [debug] [MainThread]: Snowflake adapter: Setting boto3 to ERROR (file logging only)
[0m09:55:09.655169 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'cf264312-ac92-4197-ba55-e15dd02858b2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AB3EBB52A0>]}
[0m09:55:09.742693 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'cf264312-ac92-4197-ba55-e15dd02858b2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AB3F458DF0>]}
[0m09:55:09.742693 [info ] [MainThread]: Registered adapter: snowflake=1.11.2
[0m09:55:10.275622 [debug] [MainThread]: checksum: ec27062d18352e9cfca161c166fcc73d2201661a23a3cf3c89196a27f32cfcb8, vars: {}, profile: , target: , version: 1.11.6
[0m09:55:10.275622 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m09:55:10.275622 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'cf264312-ac92-4197-ba55-e15dd02858b2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AB3EC17160>]}
[0m09:55:10.312391 [error] [MainThread]: Encountered an error:
Parsing Error
  Error reading sales_pipelines: processed\schema.yml - Runtime Error
    Syntax error near line 22
    ------------------------------
    19 |     description: "final model for processed sales data
    20 |     columns:
    21 |     - name: invoice_no
    22 |       description: "Invoice number"
    23 |       tests:
    24 |         - not_null
    25 |         - unique
    
    Raw Error:
    ------------------------------
    while parsing a block mapping
      in "<unicode string>", line 18, column 5
    did not find expected key
      in "<unicode string>", line 22, column 21
[0m09:55:10.312391 [debug] [MainThread]: Command `dbt run` failed at 09:55:10.312391 after 2.16 seconds
[0m09:55:10.312391 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AB3E1176A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AB505805E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AB50582890>]}
[0m09:55:10.312391 [debug] [MainThread]: Flushing usage events
[0m09:55:11.221632 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m09:56:00.649365 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002123FF37640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000212411B6EF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000212411B77F0>]}


============================== 09:56:00.650993 | 41b000eb-c3cf-4d28-8da7-630da4e7bf83 ==============================
[0m09:56:00.650993 [info ] [MainThread]: Running with dbt=1.11.6
[0m09:56:00.650993 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'empty': 'None', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'profiles_dir': 'D:\\snowflake-incremental-pipeline\\dbt_project', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'use_colors': 'True', 'invocation_command': 'dbt clean', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'D:\\snowflake-incremental-pipeline\\dbt_project\\logs'}
[0m09:56:00.869159 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '41b000eb-c3cf-4d28-8da7-630da4e7bf83', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000212411B7A00>]}
[0m09:56:00.900998 [debug] [MainThread]: Command `dbt clean` succeeded at 09:56:00.900998 after 0.38 seconds
[0m09:56:00.904978 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002123FF37640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000212411B6EF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002124124B370>]}
[0m09:56:00.904978 [debug] [MainThread]: Flushing usage events
[0m09:56:02.524654 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m09:56:09.647119 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002185FD57700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021861002230>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021861000AF0>]}


============================== 09:56:09.659804 | 91db5f1a-a708-4591-9a00-3bb6982ed371 ==============================
[0m09:56:09.659804 [info ] [MainThread]: Running with dbt=1.11.6
[0m09:56:09.659804 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'D:\\snowflake-incremental-pipeline\\dbt_project', 'indirect_selection': 'eager', 'use_colors': 'True', 'invocation_command': 'dbt run', 'use_experimental_parser': 'False', 'log_path': 'D:\\snowflake-incremental-pipeline\\dbt_project\\logs'}
[0m09:56:10.611816 [debug] [MainThread]: Snowflake adapter: Setting snowflake.connector to ERROR (file logging only)
[0m09:56:10.611816 [debug] [MainThread]: Snowflake adapter: Setting botocore to ERROR (file logging only)
[0m09:56:10.616851 [debug] [MainThread]: Snowflake adapter: Setting boto3 to ERROR (file logging only)
[0m09:56:10.932286 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '91db5f1a-a708-4591-9a00-3bb6982ed371', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021860FD3370>]}
[0m09:56:11.004028 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '91db5f1a-a708-4591-9a00-3bb6982ed371', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000218607CB9D0>]}
[0m09:56:11.004028 [info ] [MainThread]: Registered adapter: snowflake=1.11.2
[0m09:56:11.571967 [debug] [MainThread]: checksum: ec27062d18352e9cfca161c166fcc73d2201661a23a3cf3c89196a27f32cfcb8, vars: {}, profile: , target: , version: 1.11.6
[0m09:56:11.571967 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m09:56:11.571967 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '91db5f1a-a708-4591-9a00-3bb6982ed371', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000218720B57B0>]}
[0m09:56:13.869438 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '91db5f1a-a708-4591-9a00-3bb6982ed371', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002187267E260>]}
[0m09:56:14.023557 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m09:56:14.023557 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m09:56:14.052880 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '91db5f1a-a708-4591-9a00-3bb6982ed371', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000218726378B0>]}
[0m09:56:14.052880 [info ] [MainThread]: Found 2 models, 6 data tests, 1 source, 522 macros
[0m09:56:14.054579 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '91db5f1a-a708-4591-9a00-3bb6982ed371', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000218721F5300>]}
[0m09:56:14.054579 [info ] [MainThread]: 
[0m09:56:14.054579 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m09:56:14.054579 [info ] [MainThread]: 
[0m09:56:14.054579 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m09:56:14.063893 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m09:56:14.079916 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m09:56:14.168637 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m09:56:14.168637 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m09:56:14.168637 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m09:56:14.168637 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m09:56:14.168637 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:56:14.168637 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:56:15.456651 [debug] [ThreadPool]: SQL status: SUCCESS 3 in 1.285 seconds
[0m09:56:15.609031 [debug] [ThreadPool]: SQL status: SUCCESS 3 in 1.453 seconds
[0m09:56:15.625385 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_INCREMENTALETL, now create_INCREMENTALETL_LANDINGZONE_processed)
[0m09:56:15.625385 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_INCREMENTALETL, now create_INCREMENTALETL_LANDINGZONE_staging)
[0m09:56:15.625385 [debug] [ThreadPool]: Creating schema "database: "INCREMENTALETL"
schema: "LANDINGZONE_processed"
"
[0m09:56:15.633343 [debug] [ThreadPool]: Creating schema "database: "INCREMENTALETL"
schema: "LANDINGZONE_staging"
"
[0m09:56:15.649456 [debug] [ThreadPool]: Using snowflake connection "create_INCREMENTALETL_LANDINGZONE_processed"
[0m09:56:15.649456 [debug] [ThreadPool]: Using snowflake connection "create_INCREMENTALETL_LANDINGZONE_staging"
[0m09:56:15.657938 [debug] [ThreadPool]: On create_INCREMENTALETL_LANDINGZONE_processed: create schema if not exists INCREMENTALETL.LANDINGZONE_processed
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "create_INCREMENTALETL_LANDINGZONE_processed"} */
[0m09:56:15.657938 [debug] [ThreadPool]: On create_INCREMENTALETL_LANDINGZONE_staging: create schema if not exists INCREMENTALETL.LANDINGZONE_staging
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "create_INCREMENTALETL_LANDINGZONE_staging"} */
[0m09:56:15.805599 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.153 seconds
[0m09:56:15.821420 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.171 seconds
[0m09:56:15.843908 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL_LANDINGZONE_processed'
[0m09:56:15.845677 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL_LANDINGZONE_staging'
[0m09:56:15.877669 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL_LANDINGZONE_processed"
[0m09:56:15.895911 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL_LANDINGZONE_staging"
[0m09:56:15.895911 [debug] [ThreadPool]: On list_INCREMENTALETL_LANDINGZONE_processed: show objects in INCREMENTALETL.LANDINGZONE_processed
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL_LANDINGZONE_processed"} */;
[0m09:56:15.895911 [debug] [ThreadPool]: On list_INCREMENTALETL_LANDINGZONE_staging: show objects in INCREMENTALETL.LANDINGZONE_staging
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL_LANDINGZONE_staging"} */;
[0m09:56:15.895911 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:56:15.895911 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:56:17.567718 [debug] [ThreadPool]: SQL status: SUCCESS 0 in 1.665 seconds
[0m09:56:17.584117 [debug] [ThreadPool]: SQL status: SUCCESS 0 in 1.690 seconds
[0m09:56:17.600516 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '91db5f1a-a708-4591-9a00-3bb6982ed371', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021871FFD690>]}
[0m09:56:17.616492 [debug] [Thread-2 (]: Began running node model.sales_pipelines.stg_sales
[0m09:56:17.616492 [info ] [Thread-2 (]: 1 of 2 START sql incremental model LANDINGZONE_staging.stg_sales ............... [RUN]
[0m09:56:17.634786 [debug] [Thread-2 (]: Acquiring new snowflake connection 'model.sales_pipelines.stg_sales'
[0m09:56:17.637159 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.stg_sales
[0m09:56:17.665322 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.stg_sales"
[0m09:56:17.665322 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.stg_sales
[0m09:56:17.763241 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.stg_sales"
[0m09:56:17.779935 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m09:56:17.781949 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: create or replace transient table INCREMENTALETL.LANDINGZONE_staging.stg_sales
    
    
    
    as (

WITH raw_data AS (
    SELECT * FROM INCREMENTALETL.LANDINGZONE.RAW_SALES
),

staged AS (
    SELECT
        -- â”€â”€ IDs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        CAST(INVOICENO       AS VARCHAR)    AS invoice_no,
        CAST(STOCKCODE       AS VARCHAR)    AS stock_code,
        CAST(DESCRIPTION     AS VARCHAR)    AS description,

        -- â”€â”€ Quantities & Prices â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        TRY_CAST(QUANTITY    AS INT)        AS quantity,
        TRY_CAST(UNITPRICE   AS FLOAT)      AS unit_price,
        TRY_CAST(DISCOUNT    AS FLOAT)      AS discount,
        TRY_CAST(SHIPPINGCOST AS FLOAT)     AS shipping_cost,
        TRY_CAST(CUSTOMERID  AS FLOAT)      AS customer_id,

        -- â”€â”€ Date Fix: VARCHAR â†’ proper TIMESTAMP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        -- Handles format: '2020-01-01 00:00:00'
        TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS') AS invoice_date,
        YEAR(TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS'))  AS invoice_year,
        MONTH(TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS')) AS invoice_month,

        -- â”€â”€ Categoricals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        TRIM(UPPER(COUNTRY))         AS country,
        TRIM(UPPER(PAYMENTMETHOD))   AS payment_method,
        TRIM(UPPER(CATEGORY))        AS category,
        TRIM(UPPER(SALESCHANNEL))    AS sales_channel,
        TRIM(UPPER(RETURNSTATUS))    AS return_status,
        TRIM(UPPER(SHIPMENTPROVIDER)) AS shipment_provider,
        TRIM(UPPER(WAREHOUSELOCATION)) AS warehouse_location,
        TRIM(UPPER(ORDERPRIORITY))   AS order_priority,

        -- â”€â”€ Audit columns â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        CURRENT_TIMESTAMP()          AS stg_loaded_at

    FROM raw_data
),

-- â”€â”€ Data Quality Flags â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
validated AS (
    SELECT
        *,
        CASE
            WHEN invoice_date IS NULL           THEN 'INVALID_DATE'
            WHEN quantity IS NULL               THEN 'INVALID_QUANTITY'
            WHEN unit_price IS NULL             THEN 'INVALID_PRICE'
            WHEN shipping_cost < 0              THEN 'NEGATIVE_SHIPPING'
            WHEN discount < 0 OR discount > 1   THEN 'INVALID_DISCOUNT'
            WHEN invoice_no IS NULL             THEN 'NULL_INVOICE'
            ELSE 'VALID'
        END AS data_quality_flag

    FROM staged
)

SELECT * FROM validated


    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m09:56:17.783963 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m09:56:20.212170 [debug] [Thread-2 (]: SQL status: SUCCESS 744 in 2.435 seconds
[0m09:56:20.244034 [debug] [Thread-2 (]: Applying DROP to: INCREMENTALETL.LANDINGZONE_staging.stg_sales__dbt_tmp
[0m09:56:20.259787 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m09:56:20.259787 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: drop view if exists INCREMENTALETL.LANDINGZONE_staging.stg_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m09:56:20.424531 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.157 seconds
[0m09:56:20.444895 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '91db5f1a-a708-4591-9a00-3bb6982ed371', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000218741C60B0>]}
[0m09:56:20.444895 [info ] [Thread-2 (]: 1 of 2 OK created sql incremental model LANDINGZONE_staging.stg_sales .......... [[32mSUCCESS 744[0m in 2.81s]
[0m09:56:20.461340 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.stg_sales
[0m09:56:20.462508 [debug] [Thread-4 (]: Began running node model.sales_pipelines.processed_sales
[0m09:56:20.465075 [info ] [Thread-4 (]: 2 of 2 START sql incremental model LANDINGZONE_processed.processed_sales ....... [RUN]
[0m09:56:20.466655 [debug] [Thread-4 (]: Acquiring new snowflake connection 'model.sales_pipelines.processed_sales'
[0m09:56:20.466655 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.processed_sales
[0m09:56:20.469269 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.processed_sales"
[0m09:56:20.478262 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.processed_sales
[0m09:56:20.481860 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.processed_sales"
[0m09:56:20.493181 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m09:56:20.493181 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: create or replace transient table INCREMENTALETL.LANDINGZONE_processed.processed_sales
    
    
    
    as (

WITH staging AS (
    SELECT * FROM INCREMENTALETL.LANDINGZONE_staging.stg_sales
),

-- â”€â”€ Step 1: Remove duplicates (keep latest loaded record) â”€â”€â”€â”€â”€â”€â”€â”€
deduped AS (
    SELECT *,
        ROW_NUMBER() OVER (
            PARTITION BY invoice_no
            ORDER BY stg_loaded_at DESC
        ) AS row_num
    FROM staging
),

-- â”€â”€ Step 2: Keep only VALID records, remove bad data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cleaned AS (
    SELECT
        invoice_no,
        stock_code,
        description,
        quantity,
        unit_price,
        discount,
        shipping_cost,
        customer_id,
        invoice_date,
        invoice_year,
        invoice_month,
        country,
        payment_method,
        category,
        sales_channel,
        return_status,
        shipment_provider,
        warehouse_location,
        order_priority,

        -- â”€â”€ Derived / Enriched columns â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        ROUND(quantity * unit_price, 2)                        AS gross_amount,
        ROUND(quantity * unit_price * (1 - discount), 2)       AS net_amount,
        ROUND(quantity * unit_price * (1 - discount)
              + COALESCE(shipping_cost, 0), 2)                 AS total_amount,

        CASE
            WHEN return_status = 'RETURNED' THEN TRUE
            ELSE FALSE
        END AS is_returned,

        CASE
            WHEN customer_id IS NULL THEN TRUE
            ELSE FALSE
        END AS is_guest_customer,

        -- â”€â”€ Audit â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        stg_loaded_at,
        CURRENT_TIMESTAMP() AS processed_at

    FROM deduped
    WHERE
        row_num = 1                     -- remove duplicates
        AND data_quality_flag = 'VALID' -- only clean records
        AND quantity > 0                -- remove returns/negatives
        AND unit_price > 0              -- remove invalid prices
        AND invoice_date IS NOT NULL    -- must have valid date
),

-- â”€â”€ Step 3: Final output â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
final AS (
    SELECT * FROM cleaned
)

SELECT * FROM final


    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m09:56:20.493181 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m09:56:21.845557 [debug] [Thread-4 (]: SQL status: SUCCESS 714 in 1.358 seconds
[0m09:56:21.878624 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL.LANDINGZONE_processed.processed_sales__dbt_tmp
[0m09:56:21.878624 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m09:56:21.886457 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: drop view if exists INCREMENTALETL.LANDINGZONE_processed.processed_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m09:56:22.031917 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.152 seconds
[0m09:56:22.042292 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '91db5f1a-a708-4591-9a00-3bb6982ed371', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002185EBD9810>]}
[0m09:56:22.042292 [info ] [Thread-4 (]: 2 of 2 OK created sql incremental model LANDINGZONE_processed.processed_sales .. [[32mSUCCESS 714[0m in 1.58s]
[0m09:56:22.050561 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.processed_sales
[0m09:56:22.050561 [debug] [MainThread]: Connection 'master' was properly closed.
[0m09:56:22.058796 [debug] [MainThread]: Connection 'create_INCREMENTALETL_LANDINGZONE_staging' was left open.
[0m09:56:22.058796 [debug] [MainThread]: On create_INCREMENTALETL_LANDINGZONE_staging: Close
[0m09:56:22.303621 [debug] [MainThread]: Connection 'create_INCREMENTALETL_LANDINGZONE_processed' was left open.
[0m09:56:22.320779 [debug] [MainThread]: On create_INCREMENTALETL_LANDINGZONE_processed: Close
[0m09:56:22.557203 [debug] [MainThread]: Connection 'list_INCREMENTALETL_LANDINGZONE_processed' was left open.
[0m09:56:22.557203 [debug] [MainThread]: On list_INCREMENTALETL_LANDINGZONE_processed: Close
[0m09:56:22.871240 [debug] [MainThread]: Connection 'list_INCREMENTALETL_LANDINGZONE_staging' was left open.
[0m09:56:22.871240 [debug] [MainThread]: On list_INCREMENTALETL_LANDINGZONE_staging: Close
[0m09:56:23.130280 [debug] [MainThread]: Connection 'model.sales_pipelines.stg_sales' was left open.
[0m09:56:23.138044 [debug] [MainThread]: On model.sales_pipelines.stg_sales: Close
[0m09:56:23.399746 [debug] [MainThread]: Connection 'model.sales_pipelines.processed_sales' was left open.
[0m09:56:23.399746 [debug] [MainThread]: On model.sales_pipelines.processed_sales: Close
[0m09:56:23.689823 [info ] [MainThread]: 
[0m09:56:23.705212 [info ] [MainThread]: Finished running 2 incremental models in 0 hours 0 minutes and 9.64 seconds (9.64s).
[0m09:56:23.705212 [debug] [MainThread]: Command end result
[0m09:56:23.776652 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m09:56:23.776652 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m09:56:23.793311 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\snowflake-incremental-pipeline\dbt_project\target\run_results.json
[0m09:56:23.795325 [info ] [MainThread]: 
[0m09:56:23.796233 [info ] [MainThread]: [32mCompleted successfully[0m
[0m09:56:23.796233 [info ] [MainThread]: 
[0m09:56:23.796233 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=2
[0m09:56:23.801728 [debug] [MainThread]: Command `dbt run` succeeded at 09:56:23.801728 after 14.28 seconds
[0m09:56:23.801728 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002185FD57700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000218726C2080>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002185E77B730>]}
[0m09:56:23.801728 [debug] [MainThread]: Flushing usage events
[0m09:56:24.848948 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:03:40.906666 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DC90A776D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DC91D20190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DC91D21B70>]}


============================== 10:03:40.914667 | 3077fbe5-720c-4849-82b0-e23f685deb33 ==============================
[0m10:03:40.914667 [info ] [MainThread]: Running with dbt=1.11.6
[0m10:03:40.916682 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'None', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'profiles_dir': 'D:\\snowflake-incremental-pipeline\\dbt_project', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'invocation_command': 'dbt clean', 'indirect_selection': 'eager', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'D:\\snowflake-incremental-pipeline\\dbt_project\\logs'}
[0m10:03:41.155181 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3077fbe5-720c-4849-82b0-e23f685deb33', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DC90E3E290>]}
[0m10:03:41.180592 [debug] [MainThread]: Command `dbt clean` succeeded at 10:03:41.180592 after 0.46 seconds
[0m10:03:41.180592 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DC90A776D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DC8E01EB00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DC91D8E200>]}
[0m10:03:41.186443 [debug] [MainThread]: Flushing usage events
[0m10:03:42.301857 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:03:48.668464 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AC549476D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AC55BF1000>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AC55BF3520>]}


============================== 10:03:48.669937 | bcf06440-7bec-4498-84c8-f7b6f7226223 ==============================
[0m10:03:48.669937 [info ] [MainThread]: Running with dbt=1.11.6
[0m10:03:48.675452 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'profiles_dir': 'D:\\snowflake-incremental-pipeline\\dbt_project', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'use_colors': 'True', 'invocation_command': 'dbt run', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'D:\\snowflake-incremental-pipeline\\dbt_project\\logs'}
[0m10:03:49.628360 [debug] [MainThread]: Snowflake adapter: Setting snowflake.connector to ERROR (file logging only)
[0m10:03:49.628360 [debug] [MainThread]: Snowflake adapter: Setting botocore to ERROR (file logging only)
[0m10:03:49.628360 [debug] [MainThread]: Snowflake adapter: Setting boto3 to ERROR (file logging only)
[0m10:03:49.957204 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'bcf06440-7bec-4498-84c8-f7b6f7226223', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AC55C312A0>]}
[0m10:03:50.061109 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'bcf06440-7bec-4498-84c8-f7b6f7226223', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AC55C8A860>]}
[0m10:03:50.061109 [info ] [MainThread]: Registered adapter: snowflake=1.11.2
[0m10:03:50.704449 [debug] [MainThread]: checksum: ec27062d18352e9cfca161c166fcc73d2201661a23a3cf3c89196a27f32cfcb8, vars: {}, profile: , target: , version: 1.11.6
[0m10:03:50.704449 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m10:03:50.714838 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'bcf06440-7bec-4498-84c8-f7b6f7226223', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AC66E2DC30>]}
[0m10:03:53.141784 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'bcf06440-7bec-4498-84c8-f7b6f7226223', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AC66D51BD0>]}
[0m10:03:53.318595 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m10:03:53.318595 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m10:03:53.343063 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'bcf06440-7bec-4498-84c8-f7b6f7226223', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AC672113F0>]}
[0m10:03:53.343063 [info ] [MainThread]: Found 2 models, 6 data tests, 1 source, 522 macros
[0m10:03:53.343063 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bcf06440-7bec-4498-84c8-f7b6f7226223', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AC672103A0>]}
[0m10:03:53.348397 [info ] [MainThread]: 
[0m10:03:53.348397 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m10:03:53.351171 [info ] [MainThread]: 
[0m10:03:53.351171 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m10:03:53.361126 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m10:03:53.370829 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m10:03:53.448609 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m10:03:53.448609 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m10:03:53.448609 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m10:03:53.448609 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m10:03:53.448609 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:03:53.448609 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:03:54.920152 [debug] [ThreadPool]: SQL status: SUCCESS 5 in 1.460 seconds
[0m10:03:54.940183 [debug] [ThreadPool]: SQL status: SUCCESS 5 in 1.480 seconds
[0m10:03:54.948461 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_INCREMENTALETL, now create_INCREMENTALETL__processed)
[0m10:03:54.950429 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_INCREMENTALETL, now create_INCREMENTALETL__staging)
[0m10:03:54.953595 [debug] [ThreadPool]: Creating schema "database: "INCREMENTALETL"
schema: "_processed"
"
[0m10:03:54.958871 [debug] [ThreadPool]: Creating schema "database: "INCREMENTALETL"
schema: "_staging"
"
[0m10:03:54.984349 [debug] [ThreadPool]: Using snowflake connection "create_INCREMENTALETL__processed"
[0m10:03:54.997325 [debug] [ThreadPool]: Using snowflake connection "create_INCREMENTALETL__staging"
[0m10:03:55.000679 [debug] [ThreadPool]: On create_INCREMENTALETL__processed: create schema if not exists INCREMENTALETL._processed
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "create_INCREMENTALETL__processed"} */
[0m10:03:55.000679 [debug] [ThreadPool]: On create_INCREMENTALETL__staging: create schema if not exists INCREMENTALETL._staging
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "create_INCREMENTALETL__staging"} */
[0m10:03:55.161832 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.156 seconds
[0m10:03:55.185383 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.175 seconds
[0m10:03:55.187718 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__processed'
[0m10:03:55.208355 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__staging'
[0m10:03:55.208355 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__processed"
[0m10:03:55.208355 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__staging"
[0m10:03:55.208355 [debug] [ThreadPool]: On list_INCREMENTALETL__processed: show objects in INCREMENTALETL._processed
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__processed"} */;
[0m10:03:55.216787 [debug] [ThreadPool]: On list_INCREMENTALETL__staging: show objects in INCREMENTALETL._staging
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__staging"} */;
[0m10:03:55.216787 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:03:55.216787 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:03:56.069196 [debug] [ThreadPool]: SQL status: SUCCESS 0 in 0.851 seconds
[0m10:03:56.593800 [debug] [ThreadPool]: SQL status: SUCCESS 0 in 1.376 seconds
[0m10:03:56.597331 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bcf06440-7bec-4498-84c8-f7b6f7226223', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AC68BF2950>]}
[0m10:03:56.606319 [debug] [Thread-2 (]: Began running node model.sales_pipelines.stg_sales
[0m10:03:56.606319 [info ] [Thread-2 (]: 1 of 2 START sql incremental model _staging.stg_sales .......................... [RUN]
[0m10:03:56.610423 [debug] [Thread-2 (]: Acquiring new snowflake connection 'model.sales_pipelines.stg_sales'
[0m10:03:56.611944 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.stg_sales
[0m10:03:56.636609 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.stg_sales"
[0m10:03:56.639914 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.stg_sales
[0m10:03:56.739705 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.stg_sales"
[0m10:03:56.747840 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:03:56.747840 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: create or replace transient table INCREMENTALETL._staging.stg_sales
    
    
    
    as (

WITH raw_data AS (
    SELECT * FROM INCREMENTALETL.LANDINGZONE.RAW_SALES
),

staged AS (
    SELECT
        -- â”€â”€ IDs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        CAST(INVOICENO       AS VARCHAR)    AS invoice_no,
        CAST(STOCKCODE       AS VARCHAR)    AS stock_code,
        CAST(DESCRIPTION     AS VARCHAR)    AS description,

        -- â”€â”€ Quantities & Prices â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        TRY_CAST(QUANTITY    AS INT)        AS quantity,
        TRY_CAST(UNITPRICE   AS FLOAT)      AS unit_price,
        TRY_CAST(DISCOUNT    AS FLOAT)      AS discount,
        TRY_CAST(SHIPPINGCOST AS FLOAT)     AS shipping_cost,
        TRY_CAST(CUSTOMERID  AS FLOAT)      AS customer_id,

        -- â”€â”€ Date Fix: VARCHAR â†’ proper TIMESTAMP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        -- Handles format: '2020-01-01 00:00:00'
        TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS') AS invoice_date,
        YEAR(TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS'))  AS invoice_year,
        MONTH(TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS')) AS invoice_month,

        -- â”€â”€ Categoricals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        TRIM(UPPER(COUNTRY))         AS country,
        TRIM(UPPER(PAYMENTMETHOD))   AS payment_method,
        TRIM(UPPER(CATEGORY))        AS category,
        TRIM(UPPER(SALESCHANNEL))    AS sales_channel,
        TRIM(UPPER(RETURNSTATUS))    AS return_status,
        TRIM(UPPER(SHIPMENTPROVIDER)) AS shipment_provider,
        TRIM(UPPER(WAREHOUSELOCATION)) AS warehouse_location,
        TRIM(UPPER(ORDERPRIORITY))   AS order_priority,

        -- â”€â”€ Audit columns â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        CURRENT_TIMESTAMP()          AS stg_loaded_at

    FROM raw_data
),

-- â”€â”€ Data Quality Flags â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
validated AS (
    SELECT
        *,
        CASE
            WHEN invoice_date IS NULL           THEN 'INVALID_DATE'
            WHEN quantity IS NULL               THEN 'INVALID_QUANTITY'
            WHEN unit_price IS NULL             THEN 'INVALID_PRICE'
            WHEN shipping_cost < 0              THEN 'NEGATIVE_SHIPPING'
            WHEN discount < 0 OR discount > 1   THEN 'INVALID_DISCOUNT'
            WHEN invoice_no IS NULL             THEN 'NULL_INVOICE'
            ELSE 'VALID'
        END AS data_quality_flag

    FROM staged
)

SELECT * FROM validated


    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:03:56.747840 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m10:03:58.128694 [debug] [Thread-2 (]: SQL status: SUCCESS 744 in 1.378 seconds
[0m10:03:58.144900 [debug] [Thread-2 (]: Applying DROP to: INCREMENTALETL._staging.stg_sales__dbt_tmp
[0m10:03:58.153332 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:03:58.153332 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: drop view if exists INCREMENTALETL._staging.stg_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:03:58.343759 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.184 seconds
[0m10:03:58.392271 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bcf06440-7bec-4498-84c8-f7b6f7226223', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AC68DC3820>]}
[0m10:03:58.394287 [info ] [Thread-2 (]: 1 of 2 OK created sql incremental model _staging.stg_sales ..................... [[32mSUCCESS 744[0m in 1.78s]
[0m10:03:58.394287 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.stg_sales
[0m10:03:58.394287 [debug] [Thread-4 (]: Began running node model.sales_pipelines.processed_sales
[0m10:03:58.401589 [info ] [Thread-4 (]: 2 of 2 START sql incremental model _processed.processed_sales .................. [RUN]
[0m10:03:58.407645 [debug] [Thread-4 (]: Acquiring new snowflake connection 'model.sales_pipelines.processed_sales'
[0m10:03:58.410268 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.processed_sales
[0m10:03:58.422339 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.processed_sales"
[0m10:03:58.426103 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.processed_sales
[0m10:03:58.437228 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.processed_sales"
[0m10:03:58.445610 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:03:58.447631 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: create or replace transient table INCREMENTALETL._processed.processed_sales
    
    
    
    as (

WITH staging AS (
    SELECT * FROM INCREMENTALETL._staging.stg_sales
),

-- â”€â”€ Step 1: Remove duplicates (keep latest loaded record) â”€â”€â”€â”€â”€â”€â”€â”€
deduped AS (
    SELECT *,
        ROW_NUMBER() OVER (
            PARTITION BY invoice_no
            ORDER BY stg_loaded_at DESC
        ) AS row_num
    FROM staging
),

-- â”€â”€ Step 2: Keep only VALID records, remove bad data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cleaned AS (
    SELECT
        invoice_no,
        stock_code,
        description,
        quantity,
        unit_price,
        discount,
        shipping_cost,
        customer_id,
        invoice_date,
        invoice_year,
        invoice_month,
        country,
        payment_method,
        category,
        sales_channel,
        return_status,
        shipment_provider,
        warehouse_location,
        order_priority,

        -- â”€â”€ Derived / Enriched columns â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        ROUND(quantity * unit_price, 2)                        AS gross_amount,
        ROUND(quantity * unit_price * (1 - discount), 2)       AS net_amount,
        ROUND(quantity * unit_price * (1 - discount)
              + COALESCE(shipping_cost, 0), 2)                 AS total_amount,

        CASE
            WHEN return_status = 'RETURNED' THEN TRUE
            ELSE FALSE
        END AS is_returned,

        CASE
            WHEN customer_id IS NULL THEN TRUE
            ELSE FALSE
        END AS is_guest_customer,

        -- â”€â”€ Audit â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        stg_loaded_at,
        CURRENT_TIMESTAMP() AS processed_at

    FROM deduped
    WHERE
        row_num = 1                     -- remove duplicates
        AND data_quality_flag = 'VALID' -- only clean records
        AND quantity > 0                -- remove returns/negatives
        AND unit_price > 0              -- remove invalid prices
        AND invoice_date IS NOT NULL    -- must have valid date
),

-- â”€â”€ Step 3: Final output â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
final AS (
    SELECT * FROM cleaned
)

SELECT * FROM final


    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:03:58.447631 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m10:03:59.976942 [debug] [Thread-4 (]: SQL status: SUCCESS 714 in 1.526 seconds
[0m10:03:59.989050 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._processed.processed_sales__dbt_tmp
[0m10:03:59.993079 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:03:59.993079 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: drop view if exists INCREMENTALETL._processed.processed_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:04:00.163424 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.166 seconds
[0m10:04:00.169192 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bcf06440-7bec-4498-84c8-f7b6f7226223', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AC68D7D960>]}
[0m10:04:00.171199 [info ] [Thread-4 (]: 2 of 2 OK created sql incremental model _processed.processed_sales ............. [[32mSUCCESS 714[0m in 1.76s]
[0m10:04:00.173207 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.processed_sales
[0m10:04:00.178518 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:04:00.180531 [debug] [MainThread]: Connection 'create_INCREMENTALETL__staging' was left open.
[0m10:04:00.180531 [debug] [MainThread]: On create_INCREMENTALETL__staging: Close
[0m10:04:00.488581 [debug] [MainThread]: Connection 'create_INCREMENTALETL__processed' was left open.
[0m10:04:00.490591 [debug] [MainThread]: On create_INCREMENTALETL__processed: Close
[0m10:04:00.730013 [debug] [MainThread]: Connection 'list_INCREMENTALETL__processed' was left open.
[0m10:04:00.732024 [debug] [MainThread]: On list_INCREMENTALETL__processed: Close
[0m10:04:01.101549 [debug] [MainThread]: Connection 'list_INCREMENTALETL__staging' was left open.
[0m10:04:01.103563 [debug] [MainThread]: On list_INCREMENTALETL__staging: Close
[0m10:04:01.383362 [debug] [MainThread]: Connection 'model.sales_pipelines.stg_sales' was left open.
[0m10:04:01.385377 [debug] [MainThread]: On model.sales_pipelines.stg_sales: Close
[0m10:04:01.643452 [debug] [MainThread]: Connection 'model.sales_pipelines.processed_sales' was left open.
[0m10:04:01.645853 [debug] [MainThread]: On model.sales_pipelines.processed_sales: Close
[0m10:04:02.021858 [info ] [MainThread]: 
[0m10:04:02.023878 [info ] [MainThread]: Finished running 2 incremental models in 0 hours 0 minutes and 8.67 seconds (8.67s).
[0m10:04:02.027909 [debug] [MainThread]: Command end result
[0m10:04:02.071805 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m10:04:02.085588 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m10:04:02.101378 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\snowflake-incremental-pipeline\dbt_project\target\run_results.json
[0m10:04:02.103392 [info ] [MainThread]: 
[0m10:04:02.105406 [info ] [MainThread]: [32mCompleted successfully[0m
[0m10:04:02.107421 [info ] [MainThread]: 
[0m10:04:02.109439 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=2
[0m10:04:02.111449 [debug] [MainThread]: Command `dbt run` succeeded at 10:04:02.111449 after 13.57 seconds
[0m10:04:02.113460 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AC549476D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AC672113F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AC51D4FA60>]}
[0m10:04:02.113460 [debug] [MainThread]: Flushing usage events
[0m10:04:03.012411 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:17:07.891131 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001737A0376D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001737B2E2620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001737B2E30D0>]}


============================== 10:17:07.902134 | 08552267-c37e-463b-85c5-ce4da7140f22 ==============================
[0m10:17:07.902134 [info ] [MainThread]: Running with dbt=1.11.6
[0m10:17:07.902134 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'profiles_dir': 'D:\\snowflake-incremental-pipeline\\dbt_project', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'use_colors': 'True', 'invocation_command': 'dbt run', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'D:\\snowflake-incremental-pipeline\\dbt_project\\logs'}
[0m10:17:08.853547 [debug] [MainThread]: Snowflake adapter: Setting snowflake.connector to ERROR (file logging only)
[0m10:17:08.853547 [debug] [MainThread]: Snowflake adapter: Setting botocore to ERROR (file logging only)
[0m10:17:08.853547 [debug] [MainThread]: Snowflake adapter: Setting boto3 to ERROR (file logging only)
[0m10:17:09.157834 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '08552267-c37e-463b-85c5-ce4da7140f22', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001730CA31AE0>]}
[0m10:17:09.235630 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '08552267-c37e-463b-85c5-ce4da7140f22', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001737B37BD60>]}
[0m10:17:09.235630 [info ] [MainThread]: Registered adapter: snowflake=1.11.2
[0m10:17:09.745253 [debug] [MainThread]: checksum: ec27062d18352e9cfca161c166fcc73d2201661a23a3cf3c89196a27f32cfcb8, vars: {}, profile: , target: , version: 1.11.6
[0m10:17:10.126420 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m10:17:10.126420 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m10:17:10.126420 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m10:17:10.205790 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '08552267-c37e-463b-85c5-ce4da7140f22', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001730E11C130>]}
[0m10:17:10.349166 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m10:17:10.349166 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m10:17:10.365031 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '08552267-c37e-463b-85c5-ce4da7140f22', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001730E0BC700>]}
[0m10:17:10.365031 [info ] [MainThread]: Found 2 models, 6 data tests, 1 source, 522 macros
[0m10:17:10.365031 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '08552267-c37e-463b-85c5-ce4da7140f22', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001730E0BC730>]}
[0m10:17:10.381760 [info ] [MainThread]: 
[0m10:17:10.381760 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m10:17:10.381760 [info ] [MainThread]: 
[0m10:17:10.381760 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m10:17:10.381760 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m10:17:10.396421 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m10:17:10.492117 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m10:17:10.492117 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m10:17:10.492117 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m10:17:10.492117 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m10:17:10.492117 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:17:10.492117 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:17:12.227193 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 1.726 seconds
[0m10:17:12.212712 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 1.721 seconds
[0m10:17:12.245233 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__processed'
[0m10:17:12.245233 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__staging'
[0m10:17:12.316313 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__processed"
[0m10:17:12.323346 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__staging"
[0m10:17:12.323346 [debug] [ThreadPool]: On list_INCREMENTALETL__processed: show objects in INCREMENTALETL._processed
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__processed"} */;
[0m10:17:12.323346 [debug] [ThreadPool]: On list_INCREMENTALETL__staging: show objects in INCREMENTALETL._staging
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__staging"} */;
[0m10:17:12.323346 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:17:12.323346 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:17:13.100457 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.769 seconds
[0m10:17:13.157559 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.825 seconds
[0m10:17:13.157559 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '08552267-c37e-463b-85c5-ce4da7140f22', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001730CA33670>]}
[0m10:17:13.157559 [debug] [Thread-2 (]: Began running node model.sales_pipelines.stg_sales
[0m10:17:13.173400 [info ] [Thread-2 (]: 1 of 2 START sql incremental model _staging.stg_sales .......................... [RUN]
[0m10:17:13.173400 [debug] [Thread-2 (]: Acquiring new snowflake connection 'model.sales_pipelines.stg_sales'
[0m10:17:13.173400 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.stg_sales
[0m10:17:13.189398 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.stg_sales"
[0m10:17:13.205458 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.stg_sales
[0m10:17:13.286035 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:17:13.286035 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: create or replace  temporary view INCREMENTALETL._staging.stg_sales__dbt_tmp
  
  
  
  
  as (
    

WITH raw_data AS (
    SELECT * FROM INCREMENTALETL.LANDINGZONE.RAW_SALES
),

staged AS (
    SELECT
        -- â”€â”€ IDs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        CAST(INVOICENO       AS VARCHAR)    AS invoice_no,
        CAST(STOCKCODE       AS VARCHAR)    AS stock_code,
        CAST(DESCRIPTION     AS VARCHAR)    AS description,

        -- â”€â”€ Quantities & Prices â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        TRY_CAST(QUANTITY    AS INT)        AS quantity,
        TRY_CAST(UNITPRICE   AS FLOAT)      AS unit_price,
        TRY_CAST(DISCOUNT    AS FLOAT)      AS discount,
        TRY_CAST(SHIPPINGCOST AS FLOAT)     AS shipping_cost,
        TRY_CAST(CUSTOMERID  AS FLOAT)      AS customer_id,

        -- â”€â”€ Date Fix: VARCHAR â†’ proper TIMESTAMP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        -- Handles format: '2020-01-01 00:00:00'
        TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS') AS invoice_date,
        YEAR(TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS'))  AS invoice_year,
        MONTH(TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS')) AS invoice_month,

        -- â”€â”€ Categoricals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        TRIM(UPPER(COUNTRY))         AS country,
        TRIM(UPPER(PAYMENTMETHOD))   AS payment_method,
        TRIM(UPPER(CATEGORY))        AS category,
        TRIM(UPPER(SALESCHANNEL))    AS sales_channel,
        TRIM(UPPER(RETURNSTATUS))    AS return_status,
        TRIM(UPPER(SHIPMENTPROVIDER)) AS shipment_provider,
        TRIM(UPPER(WAREHOUSELOCATION)) AS warehouse_location,
        TRIM(UPPER(ORDERPRIORITY))   AS order_priority,

        -- â”€â”€ Audit columns â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        CURRENT_TIMESTAMP()          AS stg_loaded_at

    FROM raw_data
),

-- â”€â”€ Data Quality Flags â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
validated AS (
    SELECT
        *,
        CASE
            WHEN invoice_date IS NULL           THEN 'INVALID_DATE'
            WHEN quantity IS NULL               THEN 'INVALID_QUANTITY'
            WHEN unit_price IS NULL             THEN 'INVALID_PRICE'
            WHEN shipping_cost < 0              THEN 'NEGATIVE_SHIPPING'
            WHEN discount < 0 OR discount > 1   THEN 'INVALID_DISCOUNT'
            WHEN invoice_no IS NULL             THEN 'NULL_INVOICE'
            ELSE 'VALID'
        END AS data_quality_flag

    FROM staged
)

SELECT * FROM validated


    -- â”€â”€ Incremental: only process new months not already in staging â”€â”€
    WHERE (invoice_year, invoice_month) NOT IN (
        SELECT DISTINCT invoice_year, invoice_month
        FROM INCREMENTALETL._staging.stg_sales
    )

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:17:13.286035 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m10:17:14.557558 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 1.269 seconds
[0m10:17:14.573678 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:17:14.573678 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:17:14.717784 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.139 seconds
[0m10:17:14.733104 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:17:14.735116 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:17:14.871085 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.136 seconds
[0m10:17:14.922780 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:17:14.924796 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:17:15.062185 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.134 seconds
[0m10:17:15.179606 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:17:15.179606 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table "INCREMENTALETL"."_STAGING"."STG_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:17:15.312086 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.136 seconds
[0m10:17:15.358521 [debug] [Thread-2 (]: 
    In "INCREMENTALETL"."_STAGING"."STG_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m10:17:15.426055 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.stg_sales"
[0m10:17:15.426055 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:17:15.426055 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:17:15.579533 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.151 seconds
[0m10:17:15.587558 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:17:15.591261 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: merge into INCREMENTALETL._staging.stg_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._staging.stg_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on ((DBT_INTERNAL_SOURCE.INVOICENO = DBT_INTERNAL_DEST.INVOICENO))

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","DATA_QUALITY_FLAG" = DBT_INTERNAL_SOURCE."DATA_QUALITY_FLAG"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "DATA_QUALITY_FLAG")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "DATA_QUALITY_FLAG")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:17:15.775314 [debug] [Thread-2 (]: Snowflake adapter: Snowflake query id: 01c2829d-3202-5b3f-0013-d452000962be
[0m10:17:15.775314 [debug] [Thread-2 (]: Snowflake adapter: Snowflake error: 000904 (42000): SQL compilation error: error line 3 at position 13
invalid identifier 'DBT_INTERNAL_SOURCE.INVOICENO'
[0m10:17:15.782805 [debug] [Thread-2 (]: Database Error in model stg_sales (models\staging\stg_sales.sql)
  000904 (42000): SQL compilation error: error line 3 at position 13
  invalid identifier 'DBT_INTERNAL_SOURCE.INVOICENO'
  compiled code at target\run\sales_pipelines\models\staging\stg_sales.sql
[0m10:17:15.791247 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '08552267-c37e-463b-85c5-ce4da7140f22', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001730E77FD00>]}
[0m10:17:15.791247 [error] [Thread-2 (]: 1 of 2 ERROR creating sql incremental model _staging.stg_sales ................. [[31mERROR[0m in 2.62s]
[0m10:17:15.791247 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.stg_sales
[0m10:17:15.791247 [debug] [Thread-8 (]: Marking all children of 'model.sales_pipelines.stg_sales' to be skipped because of status 'error'.  Reason: Database Error in model stg_sales (models\staging\stg_sales.sql)
  000904 (42000): SQL compilation error: error line 3 at position 13
  invalid identifier 'DBT_INTERNAL_SOURCE.INVOICENO'
  compiled code at target\run\sales_pipelines\models\staging\stg_sales.sql.
[0m10:17:15.791247 [debug] [Thread-4 (]: Began running node model.sales_pipelines.processed_sales
[0m10:17:15.806798 [info ] [Thread-4 (]: 2 of 2 SKIP relation _processed.processed_sales ................................ [[33mSKIP[0m]
[0m10:17:15.808522 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.processed_sales
[0m10:17:15.808522 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:17:15.814862 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m10:17:15.814862 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m10:17:16.108477 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m10:17:16.108477 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m10:17:16.358467 [debug] [MainThread]: Connection 'list_INCREMENTALETL__processed' was left open.
[0m10:17:16.360093 [debug] [MainThread]: On list_INCREMENTALETL__processed: Close
[0m10:17:16.635513 [debug] [MainThread]: Connection 'list_INCREMENTALETL__staging' was left open.
[0m10:17:16.637893 [debug] [MainThread]: On list_INCREMENTALETL__staging: Close
[0m10:17:16.921236 [debug] [MainThread]: Connection 'model.sales_pipelines.stg_sales' was left open.
[0m10:17:16.921236 [debug] [MainThread]: On model.sales_pipelines.stg_sales: Close
[0m10:17:17.181771 [info ] [MainThread]: 
[0m10:17:17.181771 [info ] [MainThread]: Finished running 2 incremental models in 0 hours 0 minutes and 6.80 seconds (6.80s).
[0m10:17:17.188638 [debug] [MainThread]: Command end result
[0m10:17:17.265020 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m10:17:17.267809 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m10:17:17.282650 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\snowflake-incremental-pipeline\dbt_project\target\run_results.json
[0m10:17:17.282650 [info ] [MainThread]: 
[0m10:17:17.284660 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m10:17:17.286678 [info ] [MainThread]: 
[0m10:17:17.288963 [error] [MainThread]: [31mFailure in model stg_sales (models\staging\stg_sales.sql)[0m
[0m10:17:17.290706 [error] [MainThread]:   Database Error in model stg_sales (models\staging\stg_sales.sql)
  000904 (42000): SQL compilation error: error line 3 at position 13
  invalid identifier 'DBT_INTERNAL_SOURCE.INVOICENO'
  compiled code at target\run\sales_pipelines\models\staging\stg_sales.sql
[0m10:17:17.293018 [info ] [MainThread]: 
[0m10:17:17.295865 [info ] [MainThread]:   compiled code at target\compiled\sales_pipelines\models\staging\stg_sales.sql
[0m10:17:17.295865 [info ] [MainThread]: 
[0m10:17:17.298077 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=1 NO-OP=0 TOTAL=2
[0m10:17:17.299619 [debug] [MainThread]: Command `dbt run` failed at 10:17:17.299619 after 9.56 seconds
[0m10:17:17.301629 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001737A0376D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017378A2C400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001730CEFB130>]}
[0m10:17:17.301629 [debug] [MainThread]: Flushing usage events
[0m10:17:18.958681 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:18:44.808764 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E9FD947640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E9FEBF33A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E9FEBF35B0>]}


============================== 10:18:44.828448 | 7211c3a9-729a-44e2-9508-e4f443bc1949 ==============================
[0m10:18:44.828448 [info ] [MainThread]: Running with dbt=1.11.6
[0m10:18:44.830479 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'profiles_dir': 'D:\\snowflake-incremental-pipeline\\dbt_project', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'invocation_command': 'dbt run', 'indirect_selection': 'eager', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'D:\\snowflake-incremental-pipeline\\dbt_project\\logs'}
[0m10:18:45.785023 [debug] [MainThread]: Snowflake adapter: Setting snowflake.connector to ERROR (file logging only)
[0m10:18:45.785023 [debug] [MainThread]: Snowflake adapter: Setting botocore to ERROR (file logging only)
[0m10:18:45.787032 [debug] [MainThread]: Snowflake adapter: Setting boto3 to ERROR (file logging only)
[0m10:18:46.093226 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7211c3a9-729a-44e2-9508-e4f443bc1949', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E9FEBF2A40>]}
[0m10:18:46.172758 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7211c3a9-729a-44e2-9508-e4f443bc1949', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E9FEA4A110>]}
[0m10:18:46.172758 [info ] [MainThread]: Registered adapter: snowflake=1.11.2
[0m10:18:46.759931 [debug] [MainThread]: checksum: ec27062d18352e9cfca161c166fcc73d2201661a23a3cf3c89196a27f32cfcb8, vars: {}, profile: , target: , version: 1.11.6
[0m10:18:47.037530 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m10:18:47.037530 [debug] [MainThread]: Partial parsing: updated file: sales_pipelines://models\staging\stg_sales.sql
[0m10:18:47.617298 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7211c3a9-729a-44e2-9508-e4f443bc1949', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E991781AB0>]}
[0m10:18:47.769726 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m10:18:47.771733 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m10:18:47.791283 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7211c3a9-729a-44e2-9508-e4f443bc1949', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E99130F670>]}
[0m10:18:47.794313 [info ] [MainThread]: Found 2 models, 6 data tests, 1 source, 522 macros
[0m10:18:47.794313 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7211c3a9-729a-44e2-9508-e4f443bc1949', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E9912DDC90>]}
[0m10:18:47.799270 [info ] [MainThread]: 
[0m10:18:47.799270 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m10:18:47.799270 [info ] [MainThread]: 
[0m10:18:47.799270 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m10:18:47.809249 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m10:18:47.822927 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m10:18:48.021611 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m10:18:48.021611 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m10:18:48.021611 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m10:18:48.021611 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m10:18:48.027119 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:18:48.027119 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:18:50.145169 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 2.132 seconds
[0m10:18:50.326864 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 2.299 seconds
[0m10:18:50.338166 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__staging'
[0m10:18:50.338166 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__processed'
[0m10:18:50.369945 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__staging"
[0m10:18:50.385838 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__processed"
[0m10:18:50.385838 [debug] [ThreadPool]: On list_INCREMENTALETL__staging: show objects in INCREMENTALETL._staging
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__staging"} */;
[0m10:18:50.385838 [debug] [ThreadPool]: On list_INCREMENTALETL__processed: show objects in INCREMENTALETL._processed
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__processed"} */;
[0m10:18:50.385838 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:18:50.398039 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:18:51.210860 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.816 seconds
[0m10:18:51.210860 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.823 seconds
[0m10:18:51.236610 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7211c3a9-729a-44e2-9508-e4f443bc1949', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E98FE75360>]}
[0m10:18:51.252775 [debug] [Thread-2 (]: Began running node model.sales_pipelines.stg_sales
[0m10:18:51.252775 [info ] [Thread-2 (]: 1 of 2 START sql incremental model _staging.stg_sales .......................... [RUN]
[0m10:18:51.252775 [debug] [Thread-2 (]: Acquiring new snowflake connection 'model.sales_pipelines.stg_sales'
[0m10:18:51.252775 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.stg_sales
[0m10:18:51.300357 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.stg_sales"
[0m10:18:51.300357 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.stg_sales
[0m10:18:51.382188 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:18:51.382188 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: create or replace  temporary view INCREMENTALETL._staging.stg_sales__dbt_tmp
  
  
  
  
  as (
    

WITH raw_data AS (
    SELECT * FROM INCREMENTALETL.LANDINGZONE.RAW_SALES
),

staged AS (
    SELECT
        -- â”€â”€ IDs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        CAST(INVOICENO       AS VARCHAR)    AS invoice_no,
        CAST(STOCKCODE       AS VARCHAR)    AS stock_code,
        CAST(DESCRIPTION     AS VARCHAR)    AS description,

        -- â”€â”€ Quantities & Prices â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        TRY_CAST(QUANTITY    AS INT)        AS quantity,
        TRY_CAST(UNITPRICE   AS FLOAT)      AS unit_price,
        TRY_CAST(DISCOUNT    AS FLOAT)      AS discount,
        TRY_CAST(SHIPPINGCOST AS FLOAT)     AS shipping_cost,
        TRY_CAST(CUSTOMERID  AS FLOAT)      AS customer_id,

        -- â”€â”€ Date Fix: VARCHAR â†’ proper TIMESTAMP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        -- Handles format: '2020-01-01 00:00:00'
        TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS') AS invoice_date,
        YEAR(TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS'))  AS invoice_year,
        MONTH(TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS')) AS invoice_month,

        -- â”€â”€ Categoricals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        TRIM(UPPER(COUNTRY))         AS country,
        TRIM(UPPER(PAYMENTMETHOD))   AS payment_method,
        TRIM(UPPER(CATEGORY))        AS category,
        TRIM(UPPER(SALESCHANNEL))    AS sales_channel,
        TRIM(UPPER(RETURNSTATUS))    AS return_status,
        TRIM(UPPER(SHIPMENTPROVIDER)) AS shipment_provider,
        TRIM(UPPER(WAREHOUSELOCATION)) AS warehouse_location,
        TRIM(UPPER(ORDERPRIORITY))   AS order_priority,

        -- â”€â”€ Audit columns â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        CURRENT_TIMESTAMP()          AS stg_loaded_at

    FROM raw_data
),

-- â”€â”€ Data Quality Flags â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
validated AS (
    SELECT
        *,
        CASE
            WHEN invoice_date IS NULL           THEN 'INVALID_DATE'
            WHEN quantity IS NULL               THEN 'INVALID_QUANTITY'
            WHEN unit_price IS NULL             THEN 'INVALID_PRICE'
            WHEN shipping_cost < 0              THEN 'NEGATIVE_SHIPPING'
            WHEN discount < 0 OR discount > 1   THEN 'INVALID_DISCOUNT'
            WHEN invoice_no IS NULL             THEN 'NULL_INVOICE'
            ELSE 'VALID'
        END AS data_quality_flag

    FROM staged
)

SELECT * FROM validated


    -- â”€â”€ Incremental: only process new months not already in staging â”€â”€
    WHERE (invoice_year, invoice_month) NOT IN (
        SELECT DISTINCT invoice_year, invoice_month
        FROM INCREMENTALETL._staging.stg_sales
    )

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:18:51.382188 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m10:18:52.661523 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 1.273 seconds
[0m10:18:52.668130 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:18:52.668130 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:18:52.818262 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.142 seconds
[0m10:18:52.845875 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:18:52.845875 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:18:52.982028 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.136 seconds
[0m10:18:53.034139 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:18:53.034139 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:18:53.172375 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.132 seconds
[0m10:18:53.194097 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:18:53.198138 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table "INCREMENTALETL"."_STAGING"."STG_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:18:53.318790 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.131 seconds
[0m10:18:53.366427 [debug] [Thread-2 (]: 
    In "INCREMENTALETL"."_STAGING"."STG_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m10:18:53.413467 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.stg_sales"
[0m10:18:53.413467 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:18:53.413467 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:18:53.560876 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.156 seconds
[0m10:18:53.577021 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:18:53.577021 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: merge into INCREMENTALETL._staging.stg_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._staging.stg_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on ((DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no))

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","DATA_QUALITY_FLAG" = DBT_INTERNAL_SOURCE."DATA_QUALITY_FLAG"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "DATA_QUALITY_FLAG")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "DATA_QUALITY_FLAG")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:18:54.408451 [debug] [Thread-2 (]: SQL status: SUCCESS 696 in 0.831 seconds
[0m10:18:54.408451 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:18:54.408451 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:18:54.666283 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.253 seconds
[0m10:18:54.698197 [debug] [Thread-2 (]: Applying DROP to: INCREMENTALETL._staging.stg_sales__dbt_tmp
[0m10:18:54.698197 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:18:54.698197 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: drop view if exists INCREMENTALETL._staging.stg_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:18:54.859094 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.159 seconds
[0m10:18:54.922767 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7211c3a9-729a-44e2-9508-e4f443bc1949', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E9FE40ADD0>]}
[0m10:18:54.922767 [info ] [Thread-2 (]: 1 of 2 OK created sql incremental model _staging.stg_sales ..................... [[32mSUCCESS 696[0m in 3.67s]
[0m10:18:54.922767 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.stg_sales
[0m10:18:54.922767 [debug] [Thread-4 (]: Began running node model.sales_pipelines.processed_sales
[0m10:18:54.922767 [info ] [Thread-4 (]: 2 of 2 START sql incremental model _processed.processed_sales .................. [RUN]
[0m10:18:54.922767 [debug] [Thread-4 (]: Acquiring new snowflake connection 'model.sales_pipelines.processed_sales'
[0m10:18:54.922767 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.processed_sales
[0m10:18:54.938674 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.processed_sales"
[0m10:18:54.941829 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.processed_sales
[0m10:18:54.941829 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:18:54.941829 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: create or replace  temporary view INCREMENTALETL._processed.processed_sales__dbt_tmp
  
  
  
  
  as (
    

WITH staging AS (
    SELECT * FROM INCREMENTALETL._staging.stg_sales
),

-- â”€â”€ Step 1: Remove duplicates (keep latest loaded record) â”€â”€â”€â”€â”€â”€â”€â”€
deduped AS (
    SELECT *,
        ROW_NUMBER() OVER (
            PARTITION BY invoice_no
            ORDER BY stg_loaded_at DESC
        ) AS row_num
    FROM staging
),

-- â”€â”€ Step 2: Keep only VALID records, remove bad data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cleaned AS (
    SELECT
        invoice_no,
        stock_code,
        description,
        quantity,
        unit_price,
        discount,
        shipping_cost,
        customer_id,
        invoice_date,
        invoice_year,
        invoice_month,
        country,
        payment_method,
        category,
        sales_channel,
        return_status,
        shipment_provider,
        warehouse_location,
        order_priority,

        -- â”€â”€ Derived / Enriched columns â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        ROUND(quantity * unit_price, 2)                        AS gross_amount,
        ROUND(quantity * unit_price * (1 - discount), 2)       AS net_amount,
        ROUND(quantity * unit_price * (1 - discount)
              + COALESCE(shipping_cost, 0), 2)                 AS total_amount,

        CASE
            WHEN return_status = 'RETURNED' THEN TRUE
            ELSE FALSE
        END AS is_returned,

        CASE
            WHEN customer_id IS NULL THEN TRUE
            ELSE FALSE
        END AS is_guest_customer,

        -- â”€â”€ Audit â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        stg_loaded_at,
        CURRENT_TIMESTAMP() AS processed_at

    FROM deduped
    WHERE
        row_num = 1                     -- remove duplicates
        AND data_quality_flag = 'VALID' -- only clean records
        AND quantity > 0                -- remove returns/negatives
        AND unit_price > 0              -- remove invalid prices
        AND invoice_date IS NOT NULL    -- must have valid date
),

-- â”€â”€ Step 3: Final output â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
final AS (
    SELECT * FROM cleaned
)

SELECT * FROM final


    -- â”€â”€ Incremental: only process months not already in processed â”€â”€
    WHERE (invoice_year, invoice_month) NOT IN (
        SELECT DISTINCT invoice_year, invoice_month
        FROM INCREMENTALETL._processed.processed_sales
    )

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:18:54.954630 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m10:18:55.982815 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 1.028 seconds
[0m10:18:55.986578 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:18:55.986578 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:18:56.250710 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.257 seconds
[0m10:18:56.266556 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:18:56.268572 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:18:56.448432 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.189 seconds
[0m10:18:56.480405 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:18:56.480405 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:18:57.157818 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.686 seconds
[0m10:18:57.173654 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:18:57.173654 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:18:57.481154 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.300 seconds
[0m10:18:57.504038 [debug] [Thread-4 (]: 
    In "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m10:18:57.511918 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.processed_sales"
[0m10:18:57.515002 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:18:57.515002 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:18:57.687611 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.171 seconds
[0m10:18:57.687611 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:18:57.687611 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: merge into INCREMENTALETL._processed.processed_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._processed.processed_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on ((DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no))

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","GROSS_AMOUNT" = DBT_INTERNAL_SOURCE."GROSS_AMOUNT","NET_AMOUNT" = DBT_INTERNAL_SOURCE."NET_AMOUNT","TOTAL_AMOUNT" = DBT_INTERNAL_SOURCE."TOTAL_AMOUNT","IS_RETURNED" = DBT_INTERNAL_SOURCE."IS_RETURNED","IS_GUEST_CUSTOMER" = DBT_INTERNAL_SOURCE."IS_GUEST_CUSTOMER","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","PROCESSED_AT" = DBT_INTERNAL_SOURCE."PROCESSED_AT"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:18:58.732367 [debug] [Thread-4 (]: SQL status: SUCCESS 664 in 1.047 seconds
[0m10:18:58.732367 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:18:58.732367 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:18:59.119354 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.370 seconds
[0m10:18:59.123383 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._processed.processed_sales__dbt_tmp
[0m10:18:59.123383 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:18:59.123383 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: drop view if exists INCREMENTALETL._processed.processed_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:18:59.321969 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.195 seconds
[0m10:18:59.332420 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7211c3a9-729a-44e2-9508-e4f443bc1949', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E9FC7CA860>]}
[0m10:18:59.332420 [info ] [Thread-4 (]: 2 of 2 OK created sql incremental model _processed.processed_sales ............. [[32mSUCCESS 664[0m in 4.41s]
[0m10:18:59.348271 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.processed_sales
[0m10:18:59.356628 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:18:59.356628 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m10:18:59.356628 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m10:18:59.654221 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m10:18:59.656246 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m10:18:59.930762 [debug] [MainThread]: Connection 'list_INCREMENTALETL__staging' was left open.
[0m10:18:59.930762 [debug] [MainThread]: On list_INCREMENTALETL__staging: Close
[0m10:19:00.216856 [debug] [MainThread]: Connection 'list_INCREMENTALETL__processed' was left open.
[0m10:19:00.218866 [debug] [MainThread]: On list_INCREMENTALETL__processed: Close
[0m10:19:00.653598 [debug] [MainThread]: Connection 'model.sales_pipelines.stg_sales' was left open.
[0m10:19:00.655621 [debug] [MainThread]: On model.sales_pipelines.stg_sales: Close
[0m10:19:00.935240 [debug] [MainThread]: Connection 'model.sales_pipelines.processed_sales' was left open.
[0m10:19:00.935240 [debug] [MainThread]: On model.sales_pipelines.processed_sales: Close
[0m10:19:01.981693 [info ] [MainThread]: 
[0m10:19:01.981693 [info ] [MainThread]: Finished running 2 incremental models in 0 hours 0 minutes and 14.18 seconds (14.18s).
[0m10:19:01.999308 [debug] [MainThread]: Command end result
[0m10:19:02.045782 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m10:19:02.045782 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m10:19:02.045782 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\snowflake-incremental-pipeline\dbt_project\target\run_results.json
[0m10:19:02.045782 [info ] [MainThread]: 
[0m10:19:02.045782 [info ] [MainThread]: [32mCompleted successfully[0m
[0m10:19:02.061472 [info ] [MainThread]: 
[0m10:19:02.062951 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=2
[0m10:19:02.062951 [debug] [MainThread]: Command `dbt run` succeeded at 10:19:02.062951 after 17.37 seconds
[0m10:19:02.062951 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E9FD947640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E991780970>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E9FDD0B670>]}
[0m10:19:02.062951 [debug] [MainThread]: Flushing usage events
[0m10:19:04.790487 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:22:33.625023 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021FFF897640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021F81C633A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021F81C635B0>]}


============================== 10:22:33.625023 | 85ef9ac8-6b2c-4f3a-83eb-fadc531490d9 ==============================
[0m10:22:33.625023 [info ] [MainThread]: Running with dbt=1.11.6
[0m10:22:33.632891 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'D:\\snowflake-incremental-pipeline\\dbt_project', 'indirect_selection': 'eager', 'use_colors': 'True', 'invocation_command': 'dbt run', 'use_experimental_parser': 'False', 'log_path': 'D:\\snowflake-incremental-pipeline\\dbt_project\\logs'}
[0m10:22:34.552762 [debug] [MainThread]: Snowflake adapter: Setting snowflake.connector to ERROR (file logging only)
[0m10:22:34.552762 [debug] [MainThread]: Snowflake adapter: Setting botocore to ERROR (file logging only)
[0m10:22:34.568788 [debug] [MainThread]: Snowflake adapter: Setting boto3 to ERROR (file logging only)
[0m10:22:34.853669 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '85ef9ac8-6b2c-4f3a-83eb-fadc531490d9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021F81C62A40>]}
[0m10:22:34.948363 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '85ef9ac8-6b2c-4f3a-83eb-fadc531490d9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021F81ABA110>]}
[0m10:22:34.948363 [info ] [MainThread]: Registered adapter: snowflake=1.11.2
[0m10:22:35.454595 [debug] [MainThread]: checksum: ec27062d18352e9cfca161c166fcc73d2201661a23a3cf3c89196a27f32cfcb8, vars: {}, profile: , target: , version: 1.11.6
[0m10:22:35.739202 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m10:22:35.739202 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m10:22:35.739202 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m10:22:35.818902 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '85ef9ac8-6b2c-4f3a-83eb-fadc531490d9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021F9436C130>]}
[0m10:22:35.961573 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m10:22:35.961573 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m10:22:35.977679 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '85ef9ac8-6b2c-4f3a-83eb-fadc531490d9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021F9430FB80>]}
[0m10:22:35.993486 [info ] [MainThread]: Found 2 models, 6 data tests, 1 source, 522 macros
[0m10:22:35.993486 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '85ef9ac8-6b2c-4f3a-83eb-fadc531490d9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021F9430FC10>]}
[0m10:22:35.999612 [info ] [MainThread]: 
[0m10:22:35.999612 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m10:22:35.999612 [info ] [MainThread]: 
[0m10:22:35.999612 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m10:22:36.009539 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m10:22:36.025399 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m10:22:36.104016 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m10:22:36.104016 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m10:22:36.104016 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m10:22:36.104016 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m10:22:36.104016 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:22:36.104016 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:22:37.693480 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 1.585 seconds
[0m10:22:37.710705 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 1.598 seconds
[0m10:22:37.718988 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__staging'
[0m10:22:37.725105 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__processed'
[0m10:22:37.756763 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__staging"
[0m10:22:37.772704 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__processed"
[0m10:22:37.772704 [debug] [ThreadPool]: On list_INCREMENTALETL__staging: show objects in INCREMENTALETL._staging
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__staging"} */;
[0m10:22:37.772704 [debug] [ThreadPool]: On list_INCREMENTALETL__processed: show objects in INCREMENTALETL._processed
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__processed"} */;
[0m10:22:37.772704 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:22:37.772704 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:22:38.654925 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.883 seconds
[0m10:22:38.670578 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.906 seconds
[0m10:22:38.686492 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '85ef9ac8-6b2c-4f3a-83eb-fadc531490d9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021F943CE860>]}
[0m10:22:38.703229 [debug] [Thread-2 (]: Began running node model.sales_pipelines.stg_sales
[0m10:22:38.703229 [info ] [Thread-2 (]: 1 of 2 START sql incremental model _staging.stg_sales .......................... [RUN]
[0m10:22:38.714409 [debug] [Thread-2 (]: Acquiring new snowflake connection 'model.sales_pipelines.stg_sales'
[0m10:22:38.718214 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.stg_sales
[0m10:22:38.757998 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.stg_sales"
[0m10:22:38.760005 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.stg_sales
[0m10:22:38.836959 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:22:38.838964 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: create or replace  temporary view INCREMENTALETL._staging.stg_sales__dbt_tmp
  
  
  
  
  as (
    

WITH raw_data AS (
    SELECT * FROM INCREMENTALETL.LANDINGZONE.RAW_SALES
),

staged AS (
    SELECT
        -- â”€â”€ IDs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        CAST(INVOICENO       AS VARCHAR)    AS invoice_no,
        CAST(STOCKCODE       AS VARCHAR)    AS stock_code,
        CAST(DESCRIPTION     AS VARCHAR)    AS description,

        -- â”€â”€ Quantities & Prices â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        TRY_CAST(QUANTITY    AS INT)        AS quantity,
        TRY_CAST(UNITPRICE   AS FLOAT)      AS unit_price,
        TRY_CAST(DISCOUNT    AS FLOAT)      AS discount,
        TRY_CAST(SHIPPINGCOST AS FLOAT)     AS shipping_cost,
        TRY_CAST(CUSTOMERID  AS FLOAT)      AS customer_id,

        -- â”€â”€ Date Fix: VARCHAR â†’ proper TIMESTAMP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        -- Handles format: '2020-01-01 00:00:00'
        TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS') AS invoice_date,
        YEAR(TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS'))  AS invoice_year,
        MONTH(TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS')) AS invoice_month,

        -- â”€â”€ Categoricals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        TRIM(UPPER(COUNTRY))         AS country,
        TRIM(UPPER(PAYMENTMETHOD))   AS payment_method,
        TRIM(UPPER(CATEGORY))        AS category,
        TRIM(UPPER(SALESCHANNEL))    AS sales_channel,
        TRIM(UPPER(RETURNSTATUS))    AS return_status,
        TRIM(UPPER(SHIPMENTPROVIDER)) AS shipment_provider,
        TRIM(UPPER(WAREHOUSELOCATION)) AS warehouse_location,
        TRIM(UPPER(ORDERPRIORITY))   AS order_priority,

        -- â”€â”€ Audit columns â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        CURRENT_TIMESTAMP()          AS stg_loaded_at

    FROM raw_data
),

-- â”€â”€ Data Quality Flags â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
validated AS (
    SELECT
        *,
        CASE
            WHEN invoice_date IS NULL           THEN 'INVALID_DATE'
            WHEN quantity IS NULL               THEN 'INVALID_QUANTITY'
            WHEN unit_price IS NULL             THEN 'INVALID_PRICE'
            WHEN shipping_cost < 0              THEN 'NEGATIVE_SHIPPING'
            WHEN discount < 0 OR discount > 1   THEN 'INVALID_DISCOUNT'
            WHEN invoice_no IS NULL             THEN 'NULL_INVOICE'
            ELSE 'VALID'
        END AS data_quality_flag

    FROM staged
)

SELECT * FROM validated


    -- â”€â”€ Incremental: only process new months not already in staging â”€â”€
    WHERE (invoice_year, invoice_month) NOT IN (
        SELECT DISTINCT invoice_year, invoice_month
        FROM INCREMENTALETL._staging.stg_sales
    )

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:22:38.838964 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m10:22:39.670361 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.840 seconds
[0m10:22:39.715930 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:22:39.715930 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:22:39.890542 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.170 seconds
[0m10:22:39.906362 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:22:39.906362 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:22:40.035699 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.133 seconds
[0m10:22:40.099357 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:22:40.099357 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:22:40.227532 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.138 seconds
[0m10:22:40.354565 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:22:40.354565 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table "INCREMENTALETL"."_STAGING"."STG_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:22:40.487114 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.131 seconds
[0m10:22:40.529536 [debug] [Thread-2 (]: 
    In "INCREMENTALETL"."_STAGING"."STG_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m10:22:40.561382 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.stg_sales"
[0m10:22:40.577166 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:22:40.577166 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:22:40.724040 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.147 seconds
[0m10:22:40.724040 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:22:40.724040 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: merge into INCREMENTALETL._staging.stg_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._staging.stg_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on ((DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no))

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","DATA_QUALITY_FLAG" = DBT_INTERNAL_SOURCE."DATA_QUALITY_FLAG"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "DATA_QUALITY_FLAG")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "DATA_QUALITY_FLAG")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:22:41.517460 [debug] [Thread-2 (]: SQL status: SUCCESS 744 in 0.793 seconds
[0m10:22:41.517460 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:22:41.517460 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:22:41.755668 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.242 seconds
[0m10:22:41.804404 [debug] [Thread-2 (]: Applying DROP to: INCREMENTALETL._staging.stg_sales__dbt_tmp
[0m10:22:41.812474 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:22:41.812474 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: drop view if exists INCREMENTALETL._staging.stg_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:22:41.978043 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.159 seconds
[0m10:22:42.036458 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '85ef9ac8-6b2c-4f3a-83eb-fadc531490d9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021F81CF8EE0>]}
[0m10:22:42.036458 [info ] [Thread-2 (]: 1 of 2 OK created sql incremental model _staging.stg_sales ..................... [[32mSUCCESS 744[0m in 3.32s]
[0m10:22:42.036458 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.stg_sales
[0m10:22:42.036458 [debug] [Thread-4 (]: Began running node model.sales_pipelines.processed_sales
[0m10:22:42.036458 [info ] [Thread-4 (]: 2 of 2 START sql incremental model _processed.processed_sales .................. [RUN]
[0m10:22:42.043006 [debug] [Thread-4 (]: Acquiring new snowflake connection 'model.sales_pipelines.processed_sales'
[0m10:22:42.043006 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.processed_sales
[0m10:22:42.043006 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.processed_sales"
[0m10:22:42.052202 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.processed_sales
[0m10:22:42.052202 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:22:42.052202 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: create or replace  temporary view INCREMENTALETL._processed.processed_sales__dbt_tmp
  
  
  
  
  as (
    

WITH staging AS (
    SELECT * FROM INCREMENTALETL._staging.stg_sales
),

-- â”€â”€ Step 1: Remove duplicates (keep latest loaded record) â”€â”€â”€â”€â”€â”€â”€â”€
deduped AS (
    SELECT *,
        ROW_NUMBER() OVER (
            PARTITION BY invoice_no
            ORDER BY stg_loaded_at DESC
        ) AS row_num
    FROM staging
),

-- â”€â”€ Step 2: Keep only VALID records, remove bad data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cleaned AS (
    SELECT
        invoice_no,
        stock_code,
        description,
        quantity,
        unit_price,
        discount,
        shipping_cost,
        customer_id,
        invoice_date,
        invoice_year,
        invoice_month,
        country,
        payment_method,
        category,
        sales_channel,
        return_status,
        shipment_provider,
        warehouse_location,
        order_priority,

        -- â”€â”€ Derived / Enriched columns â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        ROUND(quantity * unit_price, 2)                        AS gross_amount,
        ROUND(quantity * unit_price * (1 - discount), 2)       AS net_amount,
        ROUND(quantity * unit_price * (1 - discount)
              + COALESCE(shipping_cost, 0), 2)                 AS total_amount,

        CASE
            WHEN return_status = 'RETURNED' THEN TRUE
            ELSE FALSE
        END AS is_returned,

        CASE
            WHEN customer_id IS NULL THEN TRUE
            ELSE FALSE
        END AS is_guest_customer,

        -- â”€â”€ Audit â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        stg_loaded_at,
        CURRENT_TIMESTAMP() AS processed_at

    FROM deduped
    WHERE
        row_num = 1                     -- remove duplicates
        AND data_quality_flag = 'VALID' -- only clean records
        AND quantity > 0                -- remove returns/negatives
        AND unit_price > 0              -- remove invalid prices
        AND invoice_date IS NOT NULL    -- must have valid date
),

-- â”€â”€ Step 3: Final output â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
final AS (
    SELECT * FROM cleaned
)

SELECT * FROM final


    -- â”€â”€ Incremental: only process months not already in processed â”€â”€
    WHERE (invoice_year, invoice_month) NOT IN (
        SELECT DISTINCT invoice_year, invoice_month
        FROM INCREMENTALETL._processed.processed_sales
    )

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:22:42.052202 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m10:22:42.846989 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.790 seconds
[0m10:22:42.857755 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:22:42.857755 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:22:43.035169 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.164 seconds
[0m10:22:43.057273 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:22:43.057273 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:22:43.193245 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.128 seconds
[0m10:22:43.196325 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:22:43.196325 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:22:43.357645 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.154 seconds
[0m10:22:43.373550 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:22:43.389354 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:22:43.534207 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.141 seconds
[0m10:22:43.550154 [debug] [Thread-4 (]: 
    In "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m10:22:43.566025 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.processed_sales"
[0m10:22:43.574865 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:22:43.574865 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:22:43.727346 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.161 seconds
[0m10:22:43.727346 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:22:43.743069 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: merge into INCREMENTALETL._processed.processed_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._processed.processed_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on ((DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no))

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","GROSS_AMOUNT" = DBT_INTERNAL_SOURCE."GROSS_AMOUNT","NET_AMOUNT" = DBT_INTERNAL_SOURCE."NET_AMOUNT","TOTAL_AMOUNT" = DBT_INTERNAL_SOURCE."TOTAL_AMOUNT","IS_RETURNED" = DBT_INTERNAL_SOURCE."IS_RETURNED","IS_GUEST_CUSTOMER" = DBT_INTERNAL_SOURCE."IS_GUEST_CUSTOMER","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","PROCESSED_AT" = DBT_INTERNAL_SOURCE."PROCESSED_AT"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:22:44.599649 [debug] [Thread-4 (]: SQL status: SUCCESS 708 in 0.852 seconds
[0m10:22:44.600673 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:22:44.602683 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:22:44.893823 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.299 seconds
[0m10:22:44.922475 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._processed.processed_sales__dbt_tmp
[0m10:22:44.922475 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:22:44.922475 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: drop view if exists INCREMENTALETL._processed.processed_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:22:45.084280 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.164 seconds
[0m10:22:45.100267 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '85ef9ac8-6b2c-4f3a-83eb-fadc531490d9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021F9438A080>]}
[0m10:22:45.100267 [info ] [Thread-4 (]: 2 of 2 OK created sql incremental model _processed.processed_sales ............. [[32mSUCCESS 708[0m in 3.06s]
[0m10:22:45.114576 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.processed_sales
[0m10:22:45.121388 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:22:45.121388 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m10:22:45.121388 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m10:22:45.357852 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m10:22:45.357852 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m10:22:45.953783 [debug] [MainThread]: Connection 'list_INCREMENTALETL__staging' was left open.
[0m10:22:45.953783 [debug] [MainThread]: On list_INCREMENTALETL__staging: Close
[0m10:22:46.339730 [debug] [MainThread]: Connection 'list_INCREMENTALETL__processed' was left open.
[0m10:22:46.342779 [debug] [MainThread]: On list_INCREMENTALETL__processed: Close
[0m10:22:46.644660 [debug] [MainThread]: Connection 'model.sales_pipelines.stg_sales' was left open.
[0m10:22:46.646427 [debug] [MainThread]: On model.sales_pipelines.stg_sales: Close
[0m10:22:46.885187 [debug] [MainThread]: Connection 'model.sales_pipelines.processed_sales' was left open.
[0m10:22:46.885187 [debug] [MainThread]: On model.sales_pipelines.processed_sales: Close
[0m10:22:47.125938 [info ] [MainThread]: 
[0m10:22:47.127952 [info ] [MainThread]: Finished running 2 incremental models in 0 hours 0 minutes and 11.13 seconds (11.13s).
[0m10:22:47.127952 [debug] [MainThread]: Command end result
[0m10:22:47.197207 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m10:22:47.199215 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m10:22:47.213028 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\snowflake-incremental-pipeline\dbt_project\target\run_results.json
[0m10:22:47.213028 [info ] [MainThread]: 
[0m10:22:47.213028 [info ] [MainThread]: [32mCompleted successfully[0m
[0m10:22:47.213028 [info ] [MainThread]: 
[0m10:22:47.218932 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=2
[0m10:22:47.218932 [debug] [MainThread]: Command `dbt run` succeeded at 10:22:47.218932 after 13.72 seconds
[0m10:22:47.222145 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021FFF897640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021F943AD1E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021F9431B5B0>]}
[0m10:22:47.224845 [debug] [MainThread]: Flushing usage events
[0m10:22:48.503615 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:23:47.469714 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022D247876D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022D26A02620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022D26A030D0>]}


============================== 10:23:47.469714 | 55eb9576-2ff6-4634-af24-bd8e35a4dc4a ==============================
[0m10:23:47.469714 [info ] [MainThread]: Running with dbt=1.11.6
[0m10:23:47.469714 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'profiles_dir': 'D:\\snowflake-incremental-pipeline\\dbt_project', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'indirect_selection': 'eager', 'use_colors': 'True', 'invocation_command': 'dbt run', 'use_experimental_parser': 'False', 'log_path': 'D:\\snowflake-incremental-pipeline\\dbt_project\\logs'}
[0m10:23:48.490074 [debug] [MainThread]: Snowflake adapter: Setting snowflake.connector to ERROR (file logging only)
[0m10:23:48.490074 [debug] [MainThread]: Snowflake adapter: Setting botocore to ERROR (file logging only)
[0m10:23:48.492080 [debug] [MainThread]: Snowflake adapter: Setting boto3 to ERROR (file logging only)
[0m10:23:48.871044 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '55eb9576-2ff6-4634-af24-bd8e35a4dc4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022D37A0E8C0>]}
[0m10:23:48.961825 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '55eb9576-2ff6-4634-af24-bd8e35a4dc4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022D26A97D60>]}
[0m10:23:48.963833 [info ] [MainThread]: Registered adapter: snowflake=1.11.2
[0m10:23:49.494167 [debug] [MainThread]: checksum: ec27062d18352e9cfca161c166fcc73d2201661a23a3cf3c89196a27f32cfcb8, vars: {}, profile: , target: , version: 1.11.6
[0m10:23:49.735371 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m10:23:49.735371 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m10:23:49.737377 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m10:23:49.814326 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '55eb9576-2ff6-4634-af24-bd8e35a4dc4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022D390FC130>]}
[0m10:23:49.962449 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m10:23:49.966462 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m10:23:49.988032 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '55eb9576-2ff6-4634-af24-bd8e35a4dc4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022D3909CA60>]}
[0m10:23:49.990041 [info ] [MainThread]: Found 2 models, 6 data tests, 1 source, 522 macros
[0m10:23:49.990041 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '55eb9576-2ff6-4634-af24-bd8e35a4dc4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022D3909CB20>]}
[0m10:23:49.990041 [info ] [MainThread]: 
[0m10:23:49.990041 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m10:23:49.990041 [info ] [MainThread]: 
[0m10:23:49.990041 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m10:23:49.999719 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m10:23:49.999719 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m10:23:50.111393 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m10:23:50.111393 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m10:23:50.111393 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m10:23:50.111393 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m10:23:50.111393 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:23:50.111393 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:23:52.039636 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 1.924 seconds
[0m10:23:52.045323 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 1.931 seconds
[0m10:23:52.049501 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__staging'
[0m10:23:52.057498 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__processed'
[0m10:23:52.079925 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__processed"
[0m10:23:52.084413 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__staging"
[0m10:23:52.084413 [debug] [ThreadPool]: On list_INCREMENTALETL__processed: show objects in INCREMENTALETL._processed
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__processed"} */;
[0m10:23:52.085643 [debug] [ThreadPool]: On list_INCREMENTALETL__staging: show objects in INCREMENTALETL._staging
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__staging"} */;
[0m10:23:52.087471 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:23:52.087471 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:23:52.788191 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.714 seconds
[0m10:23:52.899039 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.818 seconds
[0m10:23:52.911021 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '55eb9576-2ff6-4634-af24-bd8e35a4dc4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022D37A0D4B0>]}
[0m10:23:52.917774 [debug] [Thread-2 (]: Began running node model.sales_pipelines.stg_sales
[0m10:23:52.928419 [info ] [Thread-2 (]: 1 of 2 START sql incremental model _staging.stg_sales .......................... [RUN]
[0m10:23:52.932059 [debug] [Thread-2 (]: Acquiring new snowflake connection 'model.sales_pipelines.stg_sales'
[0m10:23:52.933551 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.stg_sales
[0m10:23:52.962781 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.stg_sales"
[0m10:23:52.962781 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.stg_sales
[0m10:23:53.057764 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:23:53.057764 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: create or replace  temporary view INCREMENTALETL._staging.stg_sales__dbt_tmp
  
  
  
  
  as (
    

WITH raw_data AS (
    SELECT * FROM INCREMENTALETL.LANDINGZONE.RAW_SALES
),

staged AS (
    SELECT
        -- â”€â”€ IDs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        CAST(INVOICENO       AS VARCHAR)    AS invoice_no,
        CAST(STOCKCODE       AS VARCHAR)    AS stock_code,
        CAST(DESCRIPTION     AS VARCHAR)    AS description,

        -- â”€â”€ Quantities & Prices â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        TRY_CAST(QUANTITY    AS INT)        AS quantity,
        TRY_CAST(UNITPRICE   AS FLOAT)      AS unit_price,
        TRY_CAST(DISCOUNT    AS FLOAT)      AS discount,
        TRY_CAST(SHIPPINGCOST AS FLOAT)     AS shipping_cost,
        TRY_CAST(CUSTOMERID  AS FLOAT)      AS customer_id,

        -- â”€â”€ Date Fix: VARCHAR â†’ proper TIMESTAMP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        -- Handles format: '2020-01-01 00:00:00'
        TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS') AS invoice_date,
        YEAR(TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS'))  AS invoice_year,
        MONTH(TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS')) AS invoice_month,

        -- â”€â”€ Categoricals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        TRIM(UPPER(COUNTRY))         AS country,
        TRIM(UPPER(PAYMENTMETHOD))   AS payment_method,
        TRIM(UPPER(CATEGORY))        AS category,
        TRIM(UPPER(SALESCHANNEL))    AS sales_channel,
        TRIM(UPPER(RETURNSTATUS))    AS return_status,
        TRIM(UPPER(SHIPMENTPROVIDER)) AS shipment_provider,
        TRIM(UPPER(WAREHOUSELOCATION)) AS warehouse_location,
        TRIM(UPPER(ORDERPRIORITY))   AS order_priority,

        -- â”€â”€ Audit columns â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        CURRENT_TIMESTAMP()          AS stg_loaded_at

    FROM raw_data
),

-- â”€â”€ Data Quality Flags â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
validated AS (
    SELECT
        *,
        CASE
            WHEN invoice_date IS NULL           THEN 'INVALID_DATE'
            WHEN quantity IS NULL               THEN 'INVALID_QUANTITY'
            WHEN unit_price IS NULL             THEN 'INVALID_PRICE'
            WHEN shipping_cost < 0              THEN 'NEGATIVE_SHIPPING'
            WHEN discount < 0 OR discount > 1   THEN 'INVALID_DISCOUNT'
            WHEN invoice_no IS NULL             THEN 'NULL_INVOICE'
            ELSE 'VALID'
        END AS data_quality_flag

    FROM staged
)

SELECT * FROM validated


    -- â”€â”€ Incremental: only process new months not already in staging â”€â”€
    WHERE (invoice_year, invoice_month) NOT IN (
        SELECT DISTINCT invoice_year, invoice_month
        FROM INCREMENTALETL._staging.stg_sales
    )

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:23:53.057764 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m10:23:57.706041 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 4.647 seconds
[0m10:23:57.721941 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:23:57.721941 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:23:57.916282 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.179 seconds
[0m10:23:57.942368 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:23:57.944386 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:23:58.137731 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.205 seconds
[0m10:23:58.185718 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:23:58.185718 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:23:58.378352 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.179 seconds
[0m10:23:58.489662 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:23:58.489662 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table "INCREMENTALETL"."_STAGING"."STG_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:23:58.731588 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.237 seconds
[0m10:23:58.763485 [debug] [Thread-2 (]: 
    In "INCREMENTALETL"."_STAGING"."STG_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m10:23:58.810787 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.stg_sales"
[0m10:23:58.821638 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:23:58.821638 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:23:59.134581 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.325 seconds
[0m10:23:59.150427 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:23:59.150427 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: merge into INCREMENTALETL._staging.stg_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._staging.stg_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on ((DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no))

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","DATA_QUALITY_FLAG" = DBT_INTERNAL_SOURCE."DATA_QUALITY_FLAG"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "DATA_QUALITY_FLAG")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "DATA_QUALITY_FLAG")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:23:59.960058 [debug] [Thread-2 (]: SQL status: SUCCESS 1460 in 0.808 seconds
[0m10:23:59.960058 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:23:59.960058 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:24:00.269318 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.302 seconds
[0m10:24:00.303168 [debug] [Thread-2 (]: Applying DROP to: INCREMENTALETL._staging.stg_sales__dbt_tmp
[0m10:24:00.314974 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:24:00.316983 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: drop view if exists INCREMENTALETL._staging.stg_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:24:00.490669 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.184 seconds
[0m10:24:00.553861 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '55eb9576-2ff6-4634-af24-bd8e35a4dc4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022D37C06950>]}
[0m10:24:00.553861 [info ] [Thread-2 (]: 1 of 2 OK created sql incremental model _staging.stg_sales ..................... [[32mSUCCESS 1460[0m in 7.62s]
[0m10:24:00.553861 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.stg_sales
[0m10:24:00.553861 [debug] [Thread-4 (]: Began running node model.sales_pipelines.processed_sales
[0m10:24:00.553861 [info ] [Thread-4 (]: 2 of 2 START sql incremental model _processed.processed_sales .................. [RUN]
[0m10:24:00.553861 [debug] [Thread-4 (]: Acquiring new snowflake connection 'model.sales_pipelines.processed_sales'
[0m10:24:00.553861 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.processed_sales
[0m10:24:00.570872 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.processed_sales"
[0m10:24:00.570872 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.processed_sales
[0m10:24:00.570872 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:24:00.585449 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: create or replace  temporary view INCREMENTALETL._processed.processed_sales__dbt_tmp
  
  
  
  
  as (
    

WITH staging AS (
    SELECT * FROM INCREMENTALETL._staging.stg_sales
),

-- â”€â”€ Step 1: Remove duplicates (keep latest loaded record) â”€â”€â”€â”€â”€â”€â”€â”€
deduped AS (
    SELECT *,
        ROW_NUMBER() OVER (
            PARTITION BY invoice_no
            ORDER BY stg_loaded_at DESC
        ) AS row_num
    FROM staging
),

-- â”€â”€ Step 2: Keep only VALID records, remove bad data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cleaned AS (
    SELECT
        invoice_no,
        stock_code,
        description,
        quantity,
        unit_price,
        discount,
        shipping_cost,
        customer_id,
        invoice_date,
        invoice_year,
        invoice_month,
        country,
        payment_method,
        category,
        sales_channel,
        return_status,
        shipment_provider,
        warehouse_location,
        order_priority,

        -- â”€â”€ Derived / Enriched columns â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        ROUND(quantity * unit_price, 2)                        AS gross_amount,
        ROUND(quantity * unit_price * (1 - discount), 2)       AS net_amount,
        ROUND(quantity * unit_price * (1 - discount)
              + COALESCE(shipping_cost, 0), 2)                 AS total_amount,

        CASE
            WHEN return_status = 'RETURNED' THEN TRUE
            ELSE FALSE
        END AS is_returned,

        CASE
            WHEN customer_id IS NULL THEN TRUE
            ELSE FALSE
        END AS is_guest_customer,

        -- â”€â”€ Audit â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        stg_loaded_at,
        CURRENT_TIMESTAMP() AS processed_at

    FROM deduped
    WHERE
        row_num = 1                     -- remove duplicates
        AND data_quality_flag = 'VALID' -- only clean records
        AND quantity > 0                -- remove returns/negatives
        AND unit_price > 0              -- remove invalid prices
        AND invoice_date IS NOT NULL    -- must have valid date
),

-- â”€â”€ Step 3: Final output â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
final AS (
    SELECT * FROM cleaned
)

SELECT * FROM final


    -- â”€â”€ Incremental: only process months not already in processed â”€â”€
    WHERE (invoice_year, invoice_month) NOT IN (
        SELECT DISTINCT invoice_year, invoice_month
        FROM INCREMENTALETL._processed.processed_sales
    )

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:24:00.585449 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m10:24:01.584264 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 1.011 seconds
[0m10:24:01.600441 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:24:01.600441 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:24:01.974176 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.372 seconds
[0m10:24:01.984978 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:24:01.984978 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:24:02.211217 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.217 seconds
[0m10:24:02.227315 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:24:02.227315 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:24:02.425164 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.189 seconds
[0m10:24:02.453092 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:24:02.453092 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:24:02.613054 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.163 seconds
[0m10:24:02.628895 [debug] [Thread-4 (]: 
    In "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m10:24:02.644751 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.processed_sales"
[0m10:24:02.644751 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:24:02.644751 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:24:02.830455 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.173 seconds
[0m10:24:02.832478 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:24:02.836273 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: merge into INCREMENTALETL._processed.processed_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._processed.processed_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on ((DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no))

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","GROSS_AMOUNT" = DBT_INTERNAL_SOURCE."GROSS_AMOUNT","NET_AMOUNT" = DBT_INTERNAL_SOURCE."NET_AMOUNT","TOTAL_AMOUNT" = DBT_INTERNAL_SOURCE."TOTAL_AMOUNT","IS_RETURNED" = DBT_INTERNAL_SOURCE."IS_RETURNED","IS_GUEST_CUSTOMER" = DBT_INTERNAL_SOURCE."IS_GUEST_CUSTOMER","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","PROCESSED_AT" = DBT_INTERNAL_SOURCE."PROCESSED_AT"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:24:03.697707 [debug] [Thread-4 (]: SQL status: SUCCESS 1387 in 0.863 seconds
[0m10:24:03.697707 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:24:03.697707 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:24:04.007575 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.311 seconds
[0m10:24:04.023382 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._processed.processed_sales__dbt_tmp
[0m10:24:04.039565 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:24:04.039565 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: drop view if exists INCREMENTALETL._processed.processed_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:24:04.258063 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.220 seconds
[0m10:24:04.271738 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '55eb9576-2ff6-4634-af24-bd8e35a4dc4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022D26858F70>]}
[0m10:24:04.271738 [info ] [Thread-4 (]: 2 of 2 OK created sql incremental model _processed.processed_sales ............. [[32mSUCCESS 1387[0m in 3.72s]
[0m10:24:04.271738 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.processed_sales
[0m10:24:04.277489 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:24:04.278777 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m10:24:04.278777 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m10:24:04.566926 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m10:24:04.579289 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m10:24:04.868698 [debug] [MainThread]: Connection 'list_INCREMENTALETL__staging' was left open.
[0m10:24:04.884417 [debug] [MainThread]: On list_INCREMENTALETL__staging: Close
[0m10:24:05.127208 [debug] [MainThread]: Connection 'list_INCREMENTALETL__processed' was left open.
[0m10:24:05.127208 [debug] [MainThread]: On list_INCREMENTALETL__processed: Close
[0m10:24:05.406073 [debug] [MainThread]: Connection 'model.sales_pipelines.stg_sales' was left open.
[0m10:24:05.408097 [debug] [MainThread]: On model.sales_pipelines.stg_sales: Close
[0m10:24:05.727208 [debug] [MainThread]: Connection 'model.sales_pipelines.processed_sales' was left open.
[0m10:24:05.729233 [debug] [MainThread]: On model.sales_pipelines.processed_sales: Close
[0m10:24:06.016642 [info ] [MainThread]: 
[0m10:24:06.016642 [info ] [MainThread]: Finished running 2 incremental models in 0 hours 0 minutes and 16.03 seconds (16.03s).
[0m10:24:06.027791 [debug] [MainThread]: Command end result
[0m10:24:06.095773 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m10:24:06.095773 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m10:24:06.111423 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\snowflake-incremental-pipeline\dbt_project\target\run_results.json
[0m10:24:06.111423 [info ] [MainThread]: 
[0m10:24:06.111423 [info ] [MainThread]: [32mCompleted successfully[0m
[0m10:24:06.111423 [info ] [MainThread]: 
[0m10:24:06.111423 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=2
[0m10:24:06.111423 [debug] [MainThread]: Command `dbt run` succeeded at 10:24:06.111423 after 18.79 seconds
[0m10:24:06.111423 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022D247876D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022D39149360>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022D391496F0>]}
[0m10:24:06.111423 [debug] [MainThread]: Flushing usage events
[0m10:24:07.627483 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:25:15.577907 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E819FD75E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E81B266EF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E81B2677F0>]}


============================== 10:25:15.577907 | 7507418e-2fab-4670-b7c9-98b3cb95f352 ==============================
[0m10:25:15.577907 [info ] [MainThread]: Running with dbt=1.11.6
[0m10:25:15.577907 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'profiles_dir': 'D:\\snowflake-incremental-pipeline\\dbt_project', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'invocation_command': 'dbt run', 'indirect_selection': 'eager', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'D:\\snowflake-incremental-pipeline\\dbt_project\\logs'}
[0m10:25:17.059099 [debug] [MainThread]: Snowflake adapter: Setting snowflake.connector to ERROR (file logging only)
[0m10:25:17.059099 [debug] [MainThread]: Snowflake adapter: Setting botocore to ERROR (file logging only)
[0m10:25:17.059099 [debug] [MainThread]: Snowflake adapter: Setting boto3 to ERROR (file logging only)
[0m10:25:17.797757 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7507418e-2fab-4670-b7c9-98b3cb95f352', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E81B0EA6E0>]}
[0m10:25:18.080370 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7507418e-2fab-4670-b7c9-98b3cb95f352', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E82C26BCA0>]}
[0m10:25:18.080370 [info ] [MainThread]: Registered adapter: snowflake=1.11.2
[0m10:25:19.346625 [debug] [MainThread]: checksum: ec27062d18352e9cfca161c166fcc73d2201661a23a3cf3c89196a27f32cfcb8, vars: {}, profile: , target: , version: 1.11.6
[0m10:25:19.873894 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m10:25:19.873894 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m10:25:19.873894 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m10:25:20.047270 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7507418e-2fab-4670-b7c9-98b3cb95f352', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E82D990130>]}
[0m10:25:20.406669 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m10:25:20.421864 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m10:25:20.473927 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7507418e-2fab-4670-b7c9-98b3cb95f352', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E82D92E440>]}
[0m10:25:20.473927 [info ] [MainThread]: Found 2 models, 6 data tests, 1 source, 522 macros
[0m10:25:20.478987 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7507418e-2fab-4670-b7c9-98b3cb95f352', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E82D92FF10>]}
[0m10:25:20.480624 [info ] [MainThread]: 
[0m10:25:20.485233 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m10:25:20.489501 [info ] [MainThread]: 
[0m10:25:20.489501 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m10:25:20.514275 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m10:25:20.523285 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m10:25:20.723100 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m10:25:20.723100 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m10:25:20.737165 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m10:25:20.737165 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m10:25:20.737165 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:25:20.739758 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:25:21.896692 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 1.164 seconds
[0m10:25:21.907664 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 1.169 seconds
[0m10:25:21.924581 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__processed'
[0m10:25:21.924581 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__staging'
[0m10:25:21.970518 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__staging"
[0m10:25:21.970518 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__processed"
[0m10:25:21.970518 [debug] [ThreadPool]: On list_INCREMENTALETL__staging: show objects in INCREMENTALETL._staging
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__staging"} */;
[0m10:25:21.986423 [debug] [ThreadPool]: On list_INCREMENTALETL__processed: show objects in INCREMENTALETL._processed
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__processed"} */;
[0m10:25:21.987538 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:25:21.987538 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:25:22.739139 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.754 seconds
[0m10:25:22.747454 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.759 seconds
[0m10:25:22.747454 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7507418e-2fab-4670-b7c9-98b3cb95f352', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E817E56E60>]}
[0m10:25:22.763574 [debug] [Thread-2 (]: Began running node model.sales_pipelines.stg_sales
[0m10:25:22.779323 [info ] [Thread-2 (]: 1 of 2 START sql incremental model _staging.stg_sales .......................... [RUN]
[0m10:25:22.779323 [debug] [Thread-2 (]: Acquiring new snowflake connection 'model.sales_pipelines.stg_sales'
[0m10:25:22.779323 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.stg_sales
[0m10:25:22.826731 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.stg_sales"
[0m10:25:22.826731 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.stg_sales
[0m10:25:22.906157 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:25:22.907484 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: create or replace  temporary view INCREMENTALETL._staging.stg_sales__dbt_tmp
  
  
  
  
  as (
    

WITH raw_data AS (
    SELECT * FROM INCREMENTALETL.LANDINGZONE.RAW_SALES
),

staged AS (
    SELECT
        -- â”€â”€ IDs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        CAST(INVOICENO       AS VARCHAR)    AS invoice_no,
        CAST(STOCKCODE       AS VARCHAR)    AS stock_code,
        CAST(DESCRIPTION     AS VARCHAR)    AS description,

        -- â”€â”€ Quantities & Prices â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        TRY_CAST(QUANTITY    AS INT)        AS quantity,
        TRY_CAST(UNITPRICE   AS FLOAT)      AS unit_price,
        TRY_CAST(DISCOUNT    AS FLOAT)      AS discount,
        TRY_CAST(SHIPPINGCOST AS FLOAT)     AS shipping_cost,
        TRY_CAST(CUSTOMERID  AS FLOAT)      AS customer_id,

        -- â”€â”€ Date Fix: VARCHAR â†’ proper TIMESTAMP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        -- Handles format: '2020-01-01 00:00:00'
        TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS') AS invoice_date,
        YEAR(TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS'))  AS invoice_year,
        MONTH(TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS')) AS invoice_month,

        -- â”€â”€ Categoricals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        TRIM(UPPER(COUNTRY))         AS country,
        TRIM(UPPER(PAYMENTMETHOD))   AS payment_method,
        TRIM(UPPER(CATEGORY))        AS category,
        TRIM(UPPER(SALESCHANNEL))    AS sales_channel,
        TRIM(UPPER(RETURNSTATUS))    AS return_status,
        TRIM(UPPER(SHIPMENTPROVIDER)) AS shipment_provider,
        TRIM(UPPER(WAREHOUSELOCATION)) AS warehouse_location,
        TRIM(UPPER(ORDERPRIORITY))   AS order_priority,

        -- â”€â”€ Audit columns â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        CURRENT_TIMESTAMP()          AS stg_loaded_at

    FROM raw_data
),

-- â”€â”€ Data Quality Flags â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
validated AS (
    SELECT
        *,
        CASE
            WHEN invoice_date IS NULL           THEN 'INVALID_DATE'
            WHEN quantity IS NULL               THEN 'INVALID_QUANTITY'
            WHEN unit_price IS NULL             THEN 'INVALID_PRICE'
            WHEN shipping_cost < 0              THEN 'NEGATIVE_SHIPPING'
            WHEN discount < 0 OR discount > 1   THEN 'INVALID_DISCOUNT'
            WHEN invoice_no IS NULL             THEN 'NULL_INVOICE'
            ELSE 'VALID'
        END AS data_quality_flag

    FROM staged
)

SELECT * FROM validated


    -- â”€â”€ Incremental: only process new months not already in staging â”€â”€
    WHERE (invoice_year, invoice_month) NOT IN (
        SELECT DISTINCT invoice_year, invoice_month
        FROM INCREMENTALETL._staging.stg_sales
    )

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:25:22.909025 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m10:25:23.713243 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.811 seconds
[0m10:25:23.745067 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:25:23.745067 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:25:23.904805 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.149 seconds
[0m10:25:23.926033 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:25:23.926033 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:25:24.067036 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.135 seconds
[0m10:25:24.116411 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:25:24.116411 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:25:24.247624 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.131 seconds
[0m10:25:24.362847 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:25:24.362847 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table "INCREMENTALETL"."_STAGING"."STG_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:25:24.492510 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.131 seconds
[0m10:25:24.508282 [debug] [Thread-2 (]: 
    In "INCREMENTALETL"."_STAGING"."STG_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m10:25:24.556162 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.stg_sales"
[0m10:25:24.556162 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:25:24.556162 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:25:24.716888 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.155 seconds
[0m10:25:24.716888 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:25:24.716888 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: merge into INCREMENTALETL._staging.stg_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._staging.stg_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on ((DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no))

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","DATA_QUALITY_FLAG" = DBT_INTERNAL_SOURCE."DATA_QUALITY_FLAG"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "DATA_QUALITY_FLAG")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "DATA_QUALITY_FLAG")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:25:25.660450 [debug] [Thread-2 (]: SQL status: SUCCESS 2914 in 0.931 seconds
[0m10:25:25.660450 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:25:25.660450 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:25:25.922648 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.256 seconds
[0m10:25:25.950499 [debug] [Thread-2 (]: Applying DROP to: INCREMENTALETL._staging.stg_sales__dbt_tmp
[0m10:25:25.966252 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:25:25.966252 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: drop view if exists INCREMENTALETL._staging.stg_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:25:26.129305 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.156 seconds
[0m10:25:26.184959 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7507418e-2fab-4670-b7c9-98b3cb95f352', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E818E57970>]}
[0m10:25:26.186968 [info ] [Thread-2 (]: 1 of 2 OK created sql incremental model _staging.stg_sales ..................... [[32mSUCCESS 2914[0m in 3.40s]
[0m10:25:26.186968 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.stg_sales
[0m10:25:26.186968 [debug] [Thread-4 (]: Began running node model.sales_pipelines.processed_sales
[0m10:25:26.191349 [info ] [Thread-4 (]: 2 of 2 START sql incremental model _processed.processed_sales .................. [RUN]
[0m10:25:26.191349 [debug] [Thread-4 (]: Acquiring new snowflake connection 'model.sales_pipelines.processed_sales'
[0m10:25:26.191349 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.processed_sales
[0m10:25:26.191349 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.processed_sales"
[0m10:25:26.191349 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.processed_sales
[0m10:25:26.206558 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:25:26.206558 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: create or replace  temporary view INCREMENTALETL._processed.processed_sales__dbt_tmp
  
  
  
  
  as (
    

WITH staging AS (
    SELECT * FROM INCREMENTALETL._staging.stg_sales
),

-- â”€â”€ Step 1: Remove duplicates (keep latest loaded record) â”€â”€â”€â”€â”€â”€â”€â”€
deduped AS (
    SELECT *,
        ROW_NUMBER() OVER (
            PARTITION BY invoice_no
            ORDER BY stg_loaded_at DESC
        ) AS row_num
    FROM staging
),

-- â”€â”€ Step 2: Keep only VALID records, remove bad data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cleaned AS (
    SELECT
        invoice_no,
        stock_code,
        description,
        quantity,
        unit_price,
        discount,
        shipping_cost,
        customer_id,
        invoice_date,
        invoice_year,
        invoice_month,
        country,
        payment_method,
        category,
        sales_channel,
        return_status,
        shipment_provider,
        warehouse_location,
        order_priority,

        -- â”€â”€ Derived / Enriched columns â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        ROUND(quantity * unit_price, 2)                        AS gross_amount,
        ROUND(quantity * unit_price * (1 - discount), 2)       AS net_amount,
        ROUND(quantity * unit_price * (1 - discount)
              + COALESCE(shipping_cost, 0), 2)                 AS total_amount,

        CASE
            WHEN return_status = 'RETURNED' THEN TRUE
            ELSE FALSE
        END AS is_returned,

        CASE
            WHEN customer_id IS NULL THEN TRUE
            ELSE FALSE
        END AS is_guest_customer,

        -- â”€â”€ Audit â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        stg_loaded_at,
        CURRENT_TIMESTAMP() AS processed_at

    FROM deduped
    WHERE
        row_num = 1                     -- remove duplicates
        AND data_quality_flag = 'VALID' -- only clean records
        AND quantity > 0                -- remove returns/negatives
        AND unit_price > 0              -- remove invalid prices
        AND invoice_date IS NOT NULL    -- must have valid date
),

-- â”€â”€ Step 3: Final output â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
final AS (
    SELECT * FROM cleaned
)

SELECT * FROM final


    -- â”€â”€ Incremental: only process months not already in processed â”€â”€
    WHERE (invoice_year, invoice_month) NOT IN (
        SELECT DISTINCT invoice_year, invoice_month
        FROM INCREMENTALETL._processed.processed_sales
    )

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:25:26.206558 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m10:25:27.409594 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 1.194 seconds
[0m10:25:27.427107 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:25:27.427107 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:25:27.587473 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.156 seconds
[0m10:25:27.603230 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:25:27.603230 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:25:27.746954 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.130 seconds
[0m10:25:27.762835 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:25:27.762835 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:25:27.923490 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.155 seconds
[0m10:25:27.923490 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:25:27.939220 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:25:28.080141 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.141 seconds
[0m10:25:28.116887 [debug] [Thread-4 (]: 
    In "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m10:25:28.125773 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.processed_sales"
[0m10:25:28.136873 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:25:28.136873 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:25:28.295201 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.161 seconds
[0m10:25:28.295201 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:25:28.295201 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: merge into INCREMENTALETL._processed.processed_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._processed.processed_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on ((DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no))

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","GROSS_AMOUNT" = DBT_INTERNAL_SOURCE."GROSS_AMOUNT","NET_AMOUNT" = DBT_INTERNAL_SOURCE."NET_AMOUNT","TOTAL_AMOUNT" = DBT_INTERNAL_SOURCE."TOTAL_AMOUNT","IS_RETURNED" = DBT_INTERNAL_SOURCE."IS_RETURNED","IS_GUEST_CUSTOMER" = DBT_INTERNAL_SOURCE."IS_GUEST_CUSTOMER","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","PROCESSED_AT" = DBT_INTERNAL_SOURCE."PROCESSED_AT"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:25:29.754062 [debug] [Thread-4 (]: SQL status: SUCCESS 2762 in 1.452 seconds
[0m10:25:29.754062 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:25:29.766185 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:25:30.043018 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.283 seconds
[0m10:25:30.058959 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._processed.processed_sales__dbt_tmp
[0m10:25:30.074680 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:25:30.074680 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: drop view if exists INCREMENTALETL._processed.processed_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:25:30.286539 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.210 seconds
[0m10:25:30.299159 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7507418e-2fab-4670-b7c9-98b3cb95f352', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E82C315C90>]}
[0m10:25:30.299159 [info ] [Thread-4 (]: 2 of 2 OK created sql incremental model _processed.processed_sales ............. [[32mSUCCESS 2762[0m in 4.11s]
[0m10:25:30.299159 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.processed_sales
[0m10:25:30.311941 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:25:30.315054 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m10:25:30.315054 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m10:25:30.556701 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m10:25:30.556701 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m10:25:30.878963 [debug] [MainThread]: Connection 'list_INCREMENTALETL__processed' was left open.
[0m10:25:30.878963 [debug] [MainThread]: On list_INCREMENTALETL__processed: Close
[0m10:25:31.139466 [debug] [MainThread]: Connection 'list_INCREMENTALETL__staging' was left open.
[0m10:25:31.139466 [debug] [MainThread]: On list_INCREMENTALETL__staging: Close
[0m10:25:31.378893 [debug] [MainThread]: Connection 'model.sales_pipelines.stg_sales' was left open.
[0m10:25:31.395027 [debug] [MainThread]: On model.sales_pipelines.stg_sales: Close
[0m10:25:31.635482 [debug] [MainThread]: Connection 'model.sales_pipelines.processed_sales' was left open.
[0m10:25:31.635482 [debug] [MainThread]: On model.sales_pipelines.processed_sales: Close
[0m10:25:31.875501 [info ] [MainThread]: 
[0m10:25:31.891231 [info ] [MainThread]: Finished running 2 incremental models in 0 hours 0 minutes and 11.39 seconds (11.39s).
[0m10:25:31.891231 [debug] [MainThread]: Command end result
[0m10:25:31.954696 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m10:25:31.958714 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m10:25:31.968758 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\snowflake-incremental-pipeline\dbt_project\target\run_results.json
[0m10:25:31.970508 [info ] [MainThread]: 
[0m10:25:31.970508 [info ] [MainThread]: [32mCompleted successfully[0m
[0m10:25:31.970508 [info ] [MainThread]: 
[0m10:25:31.970508 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=2
[0m10:25:31.970508 [debug] [MainThread]: Command `dbt run` succeeded at 10:25:31.970508 after 16.52 seconds
[0m10:25:31.976926 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E819FD75E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E81A348DC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E81A36BDF0>]}
[0m10:25:31.976926 [debug] [MainThread]: Flushing usage events
[0m10:25:33.337511 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:26:49.847845 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FBF04E76D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FBF17A2620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FBF17A30D0>]}


============================== 10:26:49.861224 | c28fb5f3-01ee-4769-bec7-f0697c9edffe ==============================
[0m10:26:49.861224 [info ] [MainThread]: Running with dbt=1.11.6
[0m10:26:49.861224 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'D:\\snowflake-incremental-pipeline\\dbt_project', 'invocation_command': 'dbt run', 'use_colors': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'D:\\snowflake-incremental-pipeline\\dbt_project\\logs'}
[0m10:26:50.834904 [debug] [MainThread]: Snowflake adapter: Setting snowflake.connector to ERROR (file logging only)
[0m10:26:50.834904 [debug] [MainThread]: Snowflake adapter: Setting botocore to ERROR (file logging only)
[0m10:26:50.834904 [debug] [MainThread]: Snowflake adapter: Setting boto3 to ERROR (file logging only)
[0m10:26:51.134982 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c28fb5f3-01ee-4769-bec7-f0697c9edffe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FB82A1E8C0>]}
[0m10:26:51.214529 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c28fb5f3-01ee-4769-bec7-f0697c9edffe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FBF1837D60>]}
[0m10:26:51.214529 [info ] [MainThread]: Registered adapter: snowflake=1.11.2
[0m10:26:51.753077 [debug] [MainThread]: checksum: ec27062d18352e9cfca161c166fcc73d2201661a23a3cf3c89196a27f32cfcb8, vars: {}, profile: , target: , version: 1.11.6
[0m10:26:51.973462 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m10:26:51.973462 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m10:26:51.973462 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m10:26:52.052547 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c28fb5f3-01ee-4769-bec7-f0697c9edffe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FB8410C130>]}
[0m10:26:52.195256 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m10:26:52.195256 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m10:26:52.210947 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c28fb5f3-01ee-4769-bec7-f0697c9edffe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FB840A4A60>]}
[0m10:26:52.210947 [info ] [MainThread]: Found 2 models, 6 data tests, 1 source, 522 macros
[0m10:26:52.210947 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c28fb5f3-01ee-4769-bec7-f0697c9edffe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FB840A4B20>]}
[0m10:26:52.226714 [info ] [MainThread]: 
[0m10:26:52.226714 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m10:26:52.226714 [info ] [MainThread]: 
[0m10:26:52.226714 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m10:26:52.226714 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m10:26:52.242614 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m10:26:52.336866 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m10:26:52.336866 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m10:26:52.336866 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m10:26:52.336866 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m10:26:52.336866 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:26:52.347579 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:26:53.573589 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 1.227 seconds
[0m10:26:53.579207 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 1.239 seconds
[0m10:26:53.595038 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__staging'
[0m10:26:53.598310 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__processed'
[0m10:26:53.642702 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__staging"
[0m10:26:53.654424 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__processed"
[0m10:26:53.656437 [debug] [ThreadPool]: On list_INCREMENTALETL__staging: show objects in INCREMENTALETL._staging
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__staging"} */;
[0m10:26:53.658466 [debug] [ThreadPool]: On list_INCREMENTALETL__processed: show objects in INCREMENTALETL._processed
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__processed"} */;
[0m10:26:53.660484 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:26:53.662263 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:26:54.522823 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.862 seconds
[0m10:26:54.547409 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.887 seconds
[0m10:26:54.548851 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c28fb5f3-01ee-4769-bec7-f0697c9edffe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FB841594B0>]}
[0m10:26:54.563569 [debug] [Thread-2 (]: Began running node model.sales_pipelines.stg_sales
[0m10:26:54.579354 [info ] [Thread-2 (]: 1 of 2 START sql incremental model _staging.stg_sales .......................... [RUN]
[0m10:26:54.583391 [debug] [Thread-2 (]: Acquiring new snowflake connection 'model.sales_pipelines.stg_sales'
[0m10:26:54.585408 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.stg_sales
[0m10:26:54.626813 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.stg_sales"
[0m10:26:54.626813 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.stg_sales
[0m10:26:54.691772 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:26:54.706325 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: create or replace  temporary view INCREMENTALETL._staging.stg_sales__dbt_tmp
  
  
  
  
  as (
    

WITH raw_data AS (
    SELECT * FROM INCREMENTALETL.LANDINGZONE.RAW_SALES
),

staged AS (
    SELECT
        -- â”€â”€ IDs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        CAST(INVOICENO       AS VARCHAR)    AS invoice_no,
        CAST(STOCKCODE       AS VARCHAR)    AS stock_code,
        CAST(DESCRIPTION     AS VARCHAR)    AS description,

        -- â”€â”€ Quantities & Prices â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        TRY_CAST(QUANTITY    AS INT)        AS quantity,
        TRY_CAST(UNITPRICE   AS FLOAT)      AS unit_price,
        TRY_CAST(DISCOUNT    AS FLOAT)      AS discount,
        TRY_CAST(SHIPPINGCOST AS FLOAT)     AS shipping_cost,
        TRY_CAST(CUSTOMERID  AS FLOAT)      AS customer_id,

        -- â”€â”€ Date Fix: VARCHAR â†’ proper TIMESTAMP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        -- Handles format: '2020-01-01 00:00:00'
        TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS') AS invoice_date,
        YEAR(TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS'))  AS invoice_year,
        MONTH(TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS')) AS invoice_month,

        -- â”€â”€ Categoricals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        TRIM(UPPER(COUNTRY))         AS country,
        TRIM(UPPER(PAYMENTMETHOD))   AS payment_method,
        TRIM(UPPER(CATEGORY))        AS category,
        TRIM(UPPER(SALESCHANNEL))    AS sales_channel,
        TRIM(UPPER(RETURNSTATUS))    AS return_status,
        TRIM(UPPER(SHIPMENTPROVIDER)) AS shipment_provider,
        TRIM(UPPER(WAREHOUSELOCATION)) AS warehouse_location,
        TRIM(UPPER(ORDERPRIORITY))   AS order_priority,

        -- â”€â”€ Audit columns â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        CURRENT_TIMESTAMP()          AS stg_loaded_at

    FROM raw_data
),

-- â”€â”€ Data Quality Flags â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
validated AS (
    SELECT
        *,
        CASE
            WHEN invoice_date IS NULL           THEN 'INVALID_DATE'
            WHEN quantity IS NULL               THEN 'INVALID_QUANTITY'
            WHEN unit_price IS NULL             THEN 'INVALID_PRICE'
            WHEN shipping_cost < 0              THEN 'NEGATIVE_SHIPPING'
            WHEN discount < 0 OR discount > 1   THEN 'INVALID_DISCOUNT'
            WHEN invoice_no IS NULL             THEN 'NULL_INVOICE'
            ELSE 'VALID'
        END AS data_quality_flag

    FROM staged
)

SELECT * FROM validated


    -- â”€â”€ Incremental: only process new months not already in staging â”€â”€
    WHERE (invoice_year, invoice_month) NOT IN (
        SELECT DISTINCT invoice_year, invoice_month
        FROM INCREMENTALETL._staging.stg_sales
    )

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:26:54.706325 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m10:26:55.668438 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.962 seconds
[0m10:26:55.702508 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:26:55.704519 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:26:56.061282 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.365 seconds
[0m10:26:56.077308 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:26:56.077308 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:26:56.222308 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.141 seconds
[0m10:26:56.240470 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:26:56.240470 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:26:56.398996 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.147 seconds
[0m10:26:56.510019 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:26:56.510019 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table "INCREMENTALETL"."_STAGING"."STG_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:26:56.706450 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.191 seconds
[0m10:26:56.738737 [debug] [Thread-2 (]: 
    In "INCREMENTALETL"."_STAGING"."STG_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m10:26:56.788339 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.stg_sales"
[0m10:26:56.788339 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:26:56.788339 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:26:56.931700 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.149 seconds
[0m10:26:56.947893 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:26:56.947893 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: merge into INCREMENTALETL._staging.stg_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._staging.stg_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on ((DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no))

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","DATA_QUALITY_FLAG" = DBT_INTERNAL_SOURCE."DATA_QUALITY_FLAG"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "DATA_QUALITY_FLAG")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "DATA_QUALITY_FLAG")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:26:57.928173 [debug] [Thread-2 (]: SQL status: SUCCESS 2190 in 0.975 seconds
[0m10:26:57.932224 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:26:57.934251 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:26:58.239958 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.304 seconds
[0m10:26:58.275813 [debug] [Thread-2 (]: Applying DROP to: INCREMENTALETL._staging.stg_sales__dbt_tmp
[0m10:26:58.287618 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:26:58.287618 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: drop view if exists INCREMENTALETL._staging.stg_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:26:58.448897 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.171 seconds
[0m10:26:58.512243 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c28fb5f3-01ee-4769-bec7-f0697c9edffe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FB82C16950>]}
[0m10:26:58.512243 [info ] [Thread-2 (]: 1 of 2 OK created sql incremental model _staging.stg_sales ..................... [[32mSUCCESS 2190[0m in 3.93s]
[0m10:26:58.512243 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.stg_sales
[0m10:26:58.519963 [debug] [Thread-4 (]: Began running node model.sales_pipelines.processed_sales
[0m10:26:58.521642 [info ] [Thread-4 (]: 2 of 2 START sql incremental model _processed.processed_sales .................. [RUN]
[0m10:26:58.521642 [debug] [Thread-4 (]: Acquiring new snowflake connection 'model.sales_pipelines.processed_sales'
[0m10:26:58.524828 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.processed_sales
[0m10:26:58.528384 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.processed_sales"
[0m10:26:58.528384 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.processed_sales
[0m10:26:58.540544 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:26:58.544068 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: create or replace  temporary view INCREMENTALETL._processed.processed_sales__dbt_tmp
  
  
  
  
  as (
    

WITH staging AS (
    SELECT * FROM INCREMENTALETL._staging.stg_sales
),

-- â”€â”€ Step 1: Remove duplicates (keep latest loaded record) â”€â”€â”€â”€â”€â”€â”€â”€
deduped AS (
    SELECT *,
        ROW_NUMBER() OVER (
            PARTITION BY invoice_no
            ORDER BY stg_loaded_at DESC
        ) AS row_num
    FROM staging
),

-- â”€â”€ Step 2: Keep only VALID records, remove bad data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cleaned AS (
    SELECT
        invoice_no,
        stock_code,
        description,
        quantity,
        unit_price,
        discount,
        shipping_cost,
        customer_id,
        invoice_date,
        invoice_year,
        invoice_month,
        country,
        payment_method,
        category,
        sales_channel,
        return_status,
        shipment_provider,
        warehouse_location,
        order_priority,

        -- â”€â”€ Derived / Enriched columns â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        ROUND(quantity * unit_price, 2)                        AS gross_amount,
        ROUND(quantity * unit_price * (1 - discount), 2)       AS net_amount,
        ROUND(quantity * unit_price * (1 - discount)
              + COALESCE(shipping_cost, 0), 2)                 AS total_amount,

        CASE
            WHEN return_status = 'RETURNED' THEN TRUE
            ELSE FALSE
        END AS is_returned,

        CASE
            WHEN customer_id IS NULL THEN TRUE
            ELSE FALSE
        END AS is_guest_customer,

        -- â”€â”€ Audit â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        stg_loaded_at,
        CURRENT_TIMESTAMP() AS processed_at

    FROM deduped
    WHERE
        row_num = 1                     -- remove duplicates
        AND data_quality_flag = 'VALID' -- only clean records
        AND quantity > 0                -- remove returns/negatives
        AND unit_price > 0              -- remove invalid prices
        AND invoice_date IS NOT NULL    -- must have valid date
),

-- â”€â”€ Step 3: Final output â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
final AS (
    SELECT * FROM cleaned
)

SELECT * FROM final


    -- â”€â”€ Incremental: only process months not already in processed â”€â”€
    WHERE (invoice_year, invoice_month) NOT IN (
        SELECT DISTINCT invoice_year, invoice_month
        FROM INCREMENTALETL._processed.processed_sales
    )

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:26:58.545255 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m10:26:59.355268 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.814 seconds
[0m10:26:59.363820 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:26:59.363820 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:26:59.528685 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.154 seconds
[0m10:26:59.556617 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:26:59.560599 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:26:59.767015 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.210 seconds
[0m10:26:59.798597 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:26:59.798597 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:26:59.943446 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.148 seconds
[0m10:26:59.974952 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:26:59.974952 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:27:00.110339 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.133 seconds
[0m10:27:00.135064 [debug] [Thread-4 (]: 
    In "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m10:27:00.135064 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.processed_sales"
[0m10:27:00.151137 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:27:00.151137 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:27:00.294276 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.142 seconds
[0m10:27:00.294276 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:27:00.294276 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: merge into INCREMENTALETL._processed.processed_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._processed.processed_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on ((DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no))

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","GROSS_AMOUNT" = DBT_INTERNAL_SOURCE."GROSS_AMOUNT","NET_AMOUNT" = DBT_INTERNAL_SOURCE."NET_AMOUNT","TOTAL_AMOUNT" = DBT_INTERNAL_SOURCE."TOTAL_AMOUNT","IS_RETURNED" = DBT_INTERNAL_SOURCE."IS_RETURNED","IS_GUEST_CUSTOMER" = DBT_INTERNAL_SOURCE."IS_GUEST_CUSTOMER","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","PROCESSED_AT" = DBT_INTERNAL_SOURCE."PROCESSED_AT"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:27:02.532910 [debug] [Thread-4 (]: SQL status: SUCCESS 2090 in 2.232 seconds
[0m10:27:02.532910 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:27:02.532910 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:27:02.791497 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.263 seconds
[0m10:27:02.807445 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._processed.processed_sales__dbt_tmp
[0m10:27:02.823274 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:27:02.823274 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: drop view if exists INCREMENTALETL._processed.processed_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:27:02.986341 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.160 seconds
[0m10:27:03.000403 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c28fb5f3-01ee-4769-bec7-f0697c9edffe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FB848F46D0>]}
[0m10:27:03.000403 [info ] [Thread-4 (]: 2 of 2 OK created sql incremental model _processed.processed_sales ............. [[32mSUCCESS 2090[0m in 4.48s]
[0m10:27:03.000403 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.processed_sales
[0m10:27:03.016195 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:27:03.020247 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m10:27:03.022273 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m10:27:03.352794 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m10:27:03.352794 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m10:27:03.809464 [debug] [MainThread]: Connection 'list_INCREMENTALETL__staging' was left open.
[0m10:27:03.820449 [debug] [MainThread]: On list_INCREMENTALETL__staging: Close
[0m10:27:04.110534 [debug] [MainThread]: Connection 'list_INCREMENTALETL__processed' was left open.
[0m10:27:04.110534 [debug] [MainThread]: On list_INCREMENTALETL__processed: Close
[0m10:27:04.402003 [debug] [MainThread]: Connection 'model.sales_pipelines.stg_sales' was left open.
[0m10:27:04.402003 [debug] [MainThread]: On model.sales_pipelines.stg_sales: Close
[0m10:27:04.656187 [debug] [MainThread]: Connection 'model.sales_pipelines.processed_sales' was left open.
[0m10:27:04.656187 [debug] [MainThread]: On model.sales_pipelines.processed_sales: Close
[0m10:27:04.922454 [info ] [MainThread]: 
[0m10:27:04.922454 [info ] [MainThread]: Finished running 2 incremental models in 0 hours 0 minutes and 12.70 seconds (12.70s).
[0m10:27:04.929482 [debug] [MainThread]: Command end result
[0m10:27:04.987750 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m10:27:04.987750 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m10:27:05.000877 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\snowflake-incremental-pipeline\dbt_project\target\run_results.json
[0m10:27:05.000877 [info ] [MainThread]: 
[0m10:27:05.000877 [info ] [MainThread]: [32mCompleted successfully[0m
[0m10:27:05.000877 [info ] [MainThread]: 
[0m10:27:05.000877 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=2
[0m10:27:05.000877 [debug] [MainThread]: Command `dbt run` succeeded at 10:27:05.000877 after 15.29 seconds
[0m10:27:05.015001 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FBF04E76D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FBEEF03700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FBF1668070>]}
[0m10:27:05.015001 [debug] [MainThread]: Flushing usage events
[0m10:27:06.432057 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:30:14.524452 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020504537700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000205067B2230>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000205067B0AF0>]}


============================== 10:30:14.532186 | 4e682c04-9820-474a-80ff-50fd58965e6a ==============================
[0m10:30:14.532186 [info ] [MainThread]: Running with dbt=1.11.6
[0m10:30:14.532186 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'profiles_dir': 'D:\\snowflake-incremental-pipeline\\dbt_project', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'use_colors': 'True', 'indirect_selection': 'eager', 'invocation_command': 'dbt run', 'use_experimental_parser': 'False', 'log_path': 'D:\\snowflake-incremental-pipeline\\dbt_project\\logs'}
[0m10:30:15.522530 [debug] [MainThread]: Snowflake adapter: Setting snowflake.connector to ERROR (file logging only)
[0m10:30:15.522530 [debug] [MainThread]: Snowflake adapter: Setting botocore to ERROR (file logging only)
[0m10:30:15.522530 [debug] [MainThread]: Snowflake adapter: Setting boto3 to ERROR (file logging only)
[0m10:30:15.838984 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4e682c04-9820-474a-80ff-50fd58965e6a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020506783370>]}
[0m10:30:15.917966 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4e682c04-9820-474a-80ff-50fd58965e6a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020505F7B9D0>]}
[0m10:30:15.917966 [info ] [MainThread]: Registered adapter: snowflake=1.11.2
[0m10:30:16.425478 [debug] [MainThread]: checksum: ec27062d18352e9cfca161c166fcc73d2201661a23a3cf3c89196a27f32cfcb8, vars: {}, profile: , target: , version: 1.11.6
[0m10:30:16.648055 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m10:30:16.664055 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m10:30:16.664055 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m10:30:16.727315 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4e682c04-9820-474a-80ff-50fd58965e6a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020518EAC130>]}
[0m10:30:16.870117 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m10:30:16.870117 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m10:30:16.901792 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4e682c04-9820-474a-80ff-50fd58965e6a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020518E4C880>]}
[0m10:30:16.901792 [info ] [MainThread]: Found 2 models, 6 data tests, 1 source, 522 macros
[0m10:30:16.901792 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4e682c04-9820-474a-80ff-50fd58965e6a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020518E4C8B0>]}
[0m10:30:16.901792 [info ] [MainThread]: 
[0m10:30:16.901792 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m10:30:16.901792 [info ] [MainThread]: 
[0m10:30:16.901792 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m10:30:16.919599 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m10:30:16.921605 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m10:30:17.014309 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m10:30:17.014309 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m10:30:17.014309 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m10:30:17.014309 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m10:30:17.014309 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:30:17.014309 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:30:18.201583 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 1.183 seconds
[0m10:30:18.272261 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 1.247 seconds
[0m10:30:18.283645 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__processed'
[0m10:30:18.283645 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__staging'
[0m10:30:18.345149 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__processed"
[0m10:30:18.345149 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__staging"
[0m10:30:18.345149 [debug] [ThreadPool]: On list_INCREMENTALETL__processed: show objects in INCREMENTALETL._processed
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__processed"} */;
[0m10:30:18.345149 [debug] [ThreadPool]: On list_INCREMENTALETL__staging: show objects in INCREMENTALETL._staging
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__staging"} */;
[0m10:30:18.345149 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:30:18.360935 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:30:19.098997 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.740 seconds
[0m10:30:19.756119 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 1.396 seconds
[0m10:30:19.764213 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4e682c04-9820-474a-80ff-50fd58965e6a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000205177BED10>]}
[0m10:30:19.775693 [debug] [Thread-2 (]: Began running node model.sales_pipelines.stg_sales
[0m10:30:19.788585 [info ] [Thread-2 (]: 1 of 2 START sql incremental model _staging.stg_sales .......................... [RUN]
[0m10:30:19.791755 [debug] [Thread-2 (]: Acquiring new snowflake connection 'model.sales_pipelines.stg_sales'
[0m10:30:19.795814 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.stg_sales
[0m10:30:19.825997 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.stg_sales"
[0m10:30:19.839341 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.stg_sales
[0m10:30:19.919024 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:30:19.919024 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: create or replace  temporary view INCREMENTALETL._staging.stg_sales__dbt_tmp
  
  
  
  
  as (
    

WITH raw_data AS (
    SELECT * FROM INCREMENTALETL.LANDINGZONE.RAW_SALES
),

staged AS (
    SELECT
        -- â”€â”€ IDs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        CAST(INVOICENO       AS VARCHAR)    AS invoice_no,
        CAST(STOCKCODE       AS VARCHAR)    AS stock_code,
        CAST(DESCRIPTION     AS VARCHAR)    AS description,

        -- â”€â”€ Quantities & Prices â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        TRY_CAST(QUANTITY    AS INT)        AS quantity,
        TRY_CAST(UNITPRICE   AS FLOAT)      AS unit_price,
        TRY_CAST(DISCOUNT    AS FLOAT)      AS discount,
        TRY_CAST(SHIPPINGCOST AS FLOAT)     AS shipping_cost,
        TRY_CAST(CUSTOMERID  AS FLOAT)      AS customer_id,

        -- â”€â”€ Date Fix: VARCHAR â†’ proper TIMESTAMP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        -- Handles format: '2020-01-01 00:00:00'
        TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS') AS invoice_date,
        YEAR(TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS'))  AS invoice_year,
        MONTH(TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS')) AS invoice_month,

        -- â”€â”€ Categoricals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        TRIM(UPPER(COUNTRY))         AS country,
        TRIM(UPPER(PAYMENTMETHOD))   AS payment_method,
        TRIM(UPPER(CATEGORY))        AS category,
        TRIM(UPPER(SALESCHANNEL))    AS sales_channel,
        TRIM(UPPER(RETURNSTATUS))    AS return_status,
        TRIM(UPPER(SHIPMENTPROVIDER)) AS shipment_provider,
        TRIM(UPPER(WAREHOUSELOCATION)) AS warehouse_location,
        TRIM(UPPER(ORDERPRIORITY))   AS order_priority,

        -- â”€â”€ Audit columns â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        CURRENT_TIMESTAMP()          AS stg_loaded_at

    FROM raw_data
),

-- â”€â”€ Data Quality Flags â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
validated AS (
    SELECT
        *,
        CASE
            WHEN invoice_date IS NULL           THEN 'INVALID_DATE'
            WHEN quantity IS NULL               THEN 'INVALID_QUANTITY'
            WHEN unit_price IS NULL             THEN 'INVALID_PRICE'
            WHEN shipping_cost < 0              THEN 'NEGATIVE_SHIPPING'
            WHEN discount < 0 OR discount > 1   THEN 'INVALID_DISCOUNT'
            WHEN invoice_no IS NULL             THEN 'NULL_INVOICE'
            ELSE 'VALID'
        END AS data_quality_flag

    FROM staged
)

SELECT * FROM validated


    -- â”€â”€ Incremental: only process new months not already in staging â”€â”€
    WHERE (invoice_year, invoice_month) NOT IN (
        SELECT DISTINCT invoice_year, invoice_month
        FROM INCREMENTALETL._staging.stg_sales
    )

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:30:19.919024 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m10:30:20.769800 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.861 seconds
[0m10:30:20.801651 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:30:20.801651 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:30:20.944083 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.141 seconds
[0m10:30:20.959997 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:30:20.959997 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:30:21.088289 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.131 seconds
[0m10:30:21.136076 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:30:21.136076 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:30:21.267870 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.130 seconds
[0m10:30:21.376400 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:30:21.392110 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table "INCREMENTALETL"."_STAGING"."STG_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:30:21.519823 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.132 seconds
[0m10:30:21.551600 [debug] [Thread-2 (]: 
    In "INCREMENTALETL"."_STAGING"."STG_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m10:30:21.599531 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.stg_sales"
[0m10:30:21.599531 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:30:21.599531 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:30:21.808999 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.203 seconds
[0m10:30:21.808999 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:30:21.808999 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: merge into INCREMENTALETL._staging.stg_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._staging.stg_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on ((DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no))

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","DATA_QUALITY_FLAG" = DBT_INTERNAL_SOURCE."DATA_QUALITY_FLAG"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "DATA_QUALITY_FLAG")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "DATA_QUALITY_FLAG")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:30:22.799691 [debug] [Thread-2 (]: SQL status: SUCCESS 2144 in 0.985 seconds
[0m10:30:22.799691 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:30:22.799691 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:30:23.052016 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.239 seconds
[0m10:30:23.081054 [debug] [Thread-2 (]: Applying DROP to: INCREMENTALETL._staging.stg_sales__dbt_tmp
[0m10:30:23.090838 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:30:23.092846 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: drop view if exists INCREMENTALETL._staging.stg_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:30:23.260254 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.168 seconds
[0m10:30:23.313999 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4e682c04-9820-474a-80ff-50fd58965e6a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020506844F40>]}
[0m10:30:23.313999 [info ] [Thread-2 (]: 1 of 2 OK created sql incremental model _staging.stg_sales ..................... [[32mSUCCESS 2144[0m in 3.52s]
[0m10:30:23.313999 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.stg_sales
[0m10:30:23.323206 [debug] [Thread-4 (]: Began running node model.sales_pipelines.processed_sales
[0m10:30:23.325004 [info ] [Thread-4 (]: 2 of 2 START sql incremental model _processed.processed_sales .................. [RUN]
[0m10:30:23.325004 [debug] [Thread-4 (]: Acquiring new snowflake connection 'model.sales_pipelines.processed_sales'
[0m10:30:23.325004 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.processed_sales
[0m10:30:23.330062 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.processed_sales"
[0m10:30:23.330062 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.processed_sales
[0m10:30:23.330062 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:30:23.345822 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: create or replace  temporary view INCREMENTALETL._processed.processed_sales__dbt_tmp
  
  
  
  
  as (
    

WITH staging AS (
    SELECT * FROM INCREMENTALETL._staging.stg_sales
),

-- â”€â”€ Step 1: Remove duplicates (keep latest loaded record) â”€â”€â”€â”€â”€â”€â”€â”€
deduped AS (
    SELECT *,
        ROW_NUMBER() OVER (
            PARTITION BY invoice_no
            ORDER BY stg_loaded_at DESC
        ) AS row_num
    FROM staging
),

-- â”€â”€ Step 2: Keep only VALID records, remove bad data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cleaned AS (
    SELECT
        invoice_no,
        stock_code,
        description,
        quantity,
        unit_price,
        discount,
        shipping_cost,
        customer_id,
        invoice_date,
        invoice_year,
        invoice_month,
        country,
        payment_method,
        category,
        sales_channel,
        return_status,
        shipment_provider,
        warehouse_location,
        order_priority,

        -- â”€â”€ Derived / Enriched columns â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        ROUND(quantity * unit_price, 2)                        AS gross_amount,
        ROUND(quantity * unit_price * (1 - discount), 2)       AS net_amount,
        ROUND(quantity * unit_price * (1 - discount)
              + COALESCE(shipping_cost, 0), 2)                 AS total_amount,

        CASE
            WHEN return_status = 'RETURNED' THEN TRUE
            ELSE FALSE
        END AS is_returned,

        CASE
            WHEN customer_id IS NULL THEN TRUE
            ELSE FALSE
        END AS is_guest_customer,

        -- â”€â”€ Audit â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        stg_loaded_at,
        CURRENT_TIMESTAMP() AS processed_at

    FROM deduped
    WHERE
        row_num = 1                     -- remove duplicates
        AND data_quality_flag = 'VALID' -- only clean records
        AND quantity > 0                -- remove returns/negatives
        AND unit_price > 0              -- remove invalid prices
        AND invoice_date IS NOT NULL    -- must have valid date
),

-- â”€â”€ Step 3: Final output â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
final AS (
    SELECT * FROM cleaned
)

SELECT * FROM final


    -- â”€â”€ Incremental: only process months not already in processed â”€â”€
    WHERE (invoice_year, invoice_month) NOT IN (
        SELECT DISTINCT invoice_year, invoice_month
        FROM INCREMENTALETL._processed.processed_sales
    )

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:30:23.347061 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m10:30:24.153386 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.811 seconds
[0m10:30:24.167526 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:30:24.167526 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:30:24.327947 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.160 seconds
[0m10:30:24.359565 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:30:24.359565 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:30:24.553159 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.200 seconds
[0m10:30:24.585145 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:30:24.585145 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:30:24.730145 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.137 seconds
[0m10:30:24.746260 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:30:24.746260 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:30:24.894597 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.144 seconds
[0m10:30:24.923627 [debug] [Thread-4 (]: 
    In "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m10:30:24.940358 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.processed_sales"
[0m10:30:24.940358 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:30:24.940358 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:30:25.099848 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.153 seconds
[0m10:30:25.103900 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:30:25.105922 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: merge into INCREMENTALETL._processed.processed_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._processed.processed_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on ((DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no))

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","GROSS_AMOUNT" = DBT_INTERNAL_SOURCE."GROSS_AMOUNT","NET_AMOUNT" = DBT_INTERNAL_SOURCE."NET_AMOUNT","TOTAL_AMOUNT" = DBT_INTERNAL_SOURCE."TOTAL_AMOUNT","IS_RETURNED" = DBT_INTERNAL_SOURCE."IS_RETURNED","IS_GUEST_CUSTOMER" = DBT_INTERNAL_SOURCE."IS_GUEST_CUSTOMER","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","PROCESSED_AT" = DBT_INTERNAL_SOURCE."PROCESSED_AT"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:30:26.203293 [debug] [Thread-4 (]: SQL status: SUCCESS 2034 in 1.095 seconds
[0m10:30:26.203293 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:30:26.203293 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:30:26.504624 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.302 seconds
[0m10:30:26.520595 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._processed.processed_sales__dbt_tmp
[0m10:30:26.520595 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:30:26.520595 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: drop view if exists INCREMENTALETL._processed.processed_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:30:26.686989 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.161 seconds
[0m10:30:26.697105 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4e682c04-9820-474a-80ff-50fd58965e6a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020519674A30>]}
[0m10:30:26.697105 [info ] [Thread-4 (]: 2 of 2 OK created sql incremental model _processed.processed_sales ............. [[32mSUCCESS 2034[0m in 3.37s]
[0m10:30:26.697105 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.processed_sales
[0m10:30:26.714642 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:30:26.714642 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m10:30:26.714642 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m10:30:27.134140 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m10:30:27.147378 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m10:30:27.403236 [debug] [MainThread]: Connection 'list_INCREMENTALETL__processed' was left open.
[0m10:30:27.406277 [debug] [MainThread]: On list_INCREMENTALETL__processed: Close
[0m10:30:27.645815 [debug] [MainThread]: Connection 'list_INCREMENTALETL__staging' was left open.
[0m10:30:27.645815 [debug] [MainThread]: On list_INCREMENTALETL__staging: Close
[0m10:30:27.916175 [debug] [MainThread]: Connection 'model.sales_pipelines.stg_sales' was left open.
[0m10:30:27.916175 [debug] [MainThread]: On model.sales_pipelines.stg_sales: Close
[0m10:30:28.162622 [debug] [MainThread]: Connection 'model.sales_pipelines.processed_sales' was left open.
[0m10:30:28.162622 [debug] [MainThread]: On model.sales_pipelines.processed_sales: Close
[0m10:30:28.418900 [info ] [MainThread]: 
[0m10:30:28.418900 [info ] [MainThread]: Finished running 2 incremental models in 0 hours 0 minutes and 11.50 seconds (11.50s).
[0m10:30:28.418900 [debug] [MainThread]: Command end result
[0m10:30:28.482596 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m10:30:28.498249 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m10:30:28.498249 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\snowflake-incremental-pipeline\dbt_project\target\run_results.json
[0m10:30:28.498249 [info ] [MainThread]: 
[0m10:30:28.498249 [info ] [MainThread]: [32mCompleted successfully[0m
[0m10:30:28.498249 [info ] [MainThread]: 
[0m10:30:28.498249 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=2
[0m10:30:28.513886 [debug] [MainThread]: Command `dbt run` succeeded at 10:30:28.513886 after 14.19 seconds
[0m10:30:28.513886 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020504537700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000205043AD7E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020505878D90>]}
[0m10:30:28.513886 [debug] [MainThread]: Flushing usage events
[0m10:30:29.497918 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:32:16.757079 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CCB3D77700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CCB5FF2230>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CCB5FF0AF0>]}


============================== 10:32:16.763109 | 685366ec-bb99-40b3-97f7-ec3ae3ad4b83 ==============================
[0m10:32:16.763109 [info ] [MainThread]: Running with dbt=1.11.6
[0m10:32:16.763109 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'D:\\snowflake-incremental-pipeline\\dbt_project', 'indirect_selection': 'eager', 'use_colors': 'True', 'invocation_command': 'dbt run', 'use_experimental_parser': 'False', 'log_path': 'D:\\snowflake-incremental-pipeline\\dbt_project\\logs'}
[0m10:32:17.725995 [debug] [MainThread]: Snowflake adapter: Setting snowflake.connector to ERROR (file logging only)
[0m10:32:17.725995 [debug] [MainThread]: Snowflake adapter: Setting botocore to ERROR (file logging only)
[0m10:32:17.725995 [debug] [MainThread]: Snowflake adapter: Setting boto3 to ERROR (file logging only)
[0m10:32:18.033351 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '685366ec-bb99-40b3-97f7-ec3ae3ad4b83', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CCB5FC3370>]}
[0m10:32:18.112215 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '685366ec-bb99-40b3-97f7-ec3ae3ad4b83', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CCB57BB9D0>]}
[0m10:32:18.112215 [info ] [MainThread]: Registered adapter: snowflake=1.11.2
[0m10:32:18.652274 [debug] [MainThread]: checksum: ec27062d18352e9cfca161c166fcc73d2201661a23a3cf3c89196a27f32cfcb8, vars: {}, profile: , target: , version: 1.11.6
[0m10:32:18.887335 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m10:32:18.889027 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m10:32:18.889027 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m10:32:18.971587 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '685366ec-bb99-40b3-97f7-ec3ae3ad4b83', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CCC86EC130>]}
[0m10:32:19.145549 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m10:32:19.145549 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m10:32:19.175311 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '685366ec-bb99-40b3-97f7-ec3ae3ad4b83', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CCC8684880>]}
[0m10:32:19.175311 [info ] [MainThread]: Found 2 models, 6 data tests, 1 source, 522 macros
[0m10:32:19.175311 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '685366ec-bb99-40b3-97f7-ec3ae3ad4b83', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CCC86848B0>]}
[0m10:32:19.175311 [info ] [MainThread]: 
[0m10:32:19.183812 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m10:32:19.183812 [info ] [MainThread]: 
[0m10:32:19.183812 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m10:32:19.192330 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m10:32:19.196830 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m10:32:19.301609 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m10:32:19.301609 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m10:32:19.301609 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m10:32:19.301609 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m10:32:19.301609 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:32:19.301609 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:32:20.519354 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 1.207 seconds
[0m10:32:20.523816 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 1.211 seconds
[0m10:32:20.545784 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__staging'
[0m10:32:20.553628 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__processed'
[0m10:32:20.580192 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__processed"
[0m10:32:20.580192 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__staging"
[0m10:32:20.588181 [debug] [ThreadPool]: On list_INCREMENTALETL__processed: show objects in INCREMENTALETL._processed
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__processed"} */;
[0m10:32:20.588181 [debug] [ThreadPool]: On list_INCREMENTALETL__staging: show objects in INCREMENTALETL._staging
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__staging"} */;
[0m10:32:20.588181 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:32:20.590944 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:32:21.544833 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.955 seconds
[0m10:32:21.546861 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.961 seconds
[0m10:32:21.558732 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '685366ec-bb99-40b3-97f7-ec3ae3ad4b83', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CCC6FFED10>]}
[0m10:32:21.574693 [debug] [Thread-2 (]: Began running node model.sales_pipelines.stg_sales
[0m10:32:21.594282 [info ] [Thread-2 (]: 1 of 2 START sql incremental model _staging.stg_sales .......................... [RUN]
[0m10:32:21.598145 [debug] [Thread-2 (]: Acquiring new snowflake connection 'model.sales_pipelines.stg_sales'
[0m10:32:21.602203 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.stg_sales
[0m10:32:21.647018 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.stg_sales"
[0m10:32:21.655422 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.stg_sales
[0m10:32:21.832994 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:32:21.841810 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: create or replace  temporary view INCREMENTALETL._staging.stg_sales__dbt_tmp
  
  
  
  
  as (
    

WITH raw_data AS (
    SELECT * FROM INCREMENTALETL.LANDINGZONE.RAW_SALES
),

staged AS (
    SELECT
        -- â”€â”€ IDs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        CAST(INVOICENO       AS VARCHAR)    AS invoice_no,
        CAST(STOCKCODE       AS VARCHAR)    AS stock_code,
        CAST(DESCRIPTION     AS VARCHAR)    AS description,

        -- â”€â”€ Quantities & Prices â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        TRY_CAST(QUANTITY    AS INT)        AS quantity,
        TRY_CAST(UNITPRICE   AS FLOAT)      AS unit_price,
        TRY_CAST(DISCOUNT    AS FLOAT)      AS discount,
        TRY_CAST(SHIPPINGCOST AS FLOAT)     AS shipping_cost,
        TRY_CAST(CUSTOMERID  AS FLOAT)      AS customer_id,

        -- â”€â”€ Date Fix: VARCHAR â†’ proper TIMESTAMP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        -- Handles format: '2020-01-01 00:00:00'
        TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS') AS invoice_date,
        YEAR(TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS'))  AS invoice_year,
        MONTH(TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS')) AS invoice_month,

        -- â”€â”€ Categoricals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        TRIM(UPPER(COUNTRY))         AS country,
        TRIM(UPPER(PAYMENTMETHOD))   AS payment_method,
        TRIM(UPPER(CATEGORY))        AS category,
        TRIM(UPPER(SALESCHANNEL))    AS sales_channel,
        TRIM(UPPER(RETURNSTATUS))    AS return_status,
        TRIM(UPPER(SHIPMENTPROVIDER)) AS shipment_provider,
        TRIM(UPPER(WAREHOUSELOCATION)) AS warehouse_location,
        TRIM(UPPER(ORDERPRIORITY))   AS order_priority,

        -- â”€â”€ Audit columns â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        CURRENT_TIMESTAMP()          AS stg_loaded_at

    FROM raw_data
),

-- â”€â”€ Data Quality Flags â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
validated AS (
    SELECT
        *,
        CASE
            WHEN invoice_date IS NULL           THEN 'INVALID_DATE'
            WHEN quantity IS NULL               THEN 'INVALID_QUANTITY'
            WHEN unit_price IS NULL             THEN 'INVALID_PRICE'
            WHEN shipping_cost < 0              THEN 'NEGATIVE_SHIPPING'
            WHEN discount < 0 OR discount > 1   THEN 'INVALID_DISCOUNT'
            WHEN invoice_no IS NULL             THEN 'NULL_INVOICE'
            ELSE 'VALID'
        END AS data_quality_flag

    FROM staged
)

SELECT * FROM validated


    -- â”€â”€ Incremental: only process new months not already in staging â”€â”€
    WHERE (invoice_year, invoice_month) NOT IN (
        SELECT DISTINCT invoice_year, invoice_month
        FROM INCREMENTALETL._staging.stg_sales
    )

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:32:21.845125 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m10:32:22.707176 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.863 seconds
[0m10:32:22.752492 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:32:22.755011 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:32:22.911220 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.156 seconds
[0m10:32:22.935838 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:32:22.938411 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:32:23.077425 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.137 seconds
[0m10:32:23.129495 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:32:23.135832 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:32:23.271541 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.137 seconds
[0m10:32:23.402829 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:32:23.402829 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table "INCREMENTALETL"."_STAGING"."STG_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:32:23.562957 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.153 seconds
[0m10:32:23.611948 [debug] [Thread-2 (]: 
    In "INCREMENTALETL"."_STAGING"."STG_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m10:32:23.703471 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.stg_sales"
[0m10:32:23.703471 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:32:23.703471 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:32:23.866828 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.155 seconds
[0m10:32:23.874281 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:32:23.874281 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: merge into INCREMENTALETL._staging.stg_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._staging.stg_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on ((DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no))

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","DATA_QUALITY_FLAG" = DBT_INTERNAL_SOURCE."DATA_QUALITY_FLAG"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "DATA_QUALITY_FLAG")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "DATA_QUALITY_FLAG")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:32:24.886509 [debug] [Thread-2 (]: SQL status: SUCCESS 2896 in 1.001 seconds
[0m10:32:24.886509 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:32:24.894925 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:32:25.171690 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.274 seconds
[0m10:32:25.188597 [debug] [Thread-2 (]: Applying DROP to: INCREMENTALETL._staging.stg_sales__dbt_tmp
[0m10:32:25.208038 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:32:25.210581 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: drop view if exists INCREMENTALETL._staging.stg_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:32:25.389957 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.180 seconds
[0m10:32:25.444856 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '685366ec-bb99-40b3-97f7-ec3ae3ad4b83', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CCC865E140>]}
[0m10:32:25.446869 [info ] [Thread-2 (]: 1 of 2 OK created sql incremental model _staging.stg_sales ..................... [[32mSUCCESS 2896[0m in 3.85s]
[0m10:32:25.450895 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.stg_sales
[0m10:32:25.450895 [debug] [Thread-4 (]: Began running node model.sales_pipelines.processed_sales
[0m10:32:25.455137 [info ] [Thread-4 (]: 2 of 2 START sql incremental model _processed.processed_sales .................. [RUN]
[0m10:32:25.456280 [debug] [Thread-4 (]: Acquiring new snowflake connection 'model.sales_pipelines.processed_sales'
[0m10:32:25.456280 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.processed_sales
[0m10:32:25.472070 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.processed_sales"
[0m10:32:25.473238 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.processed_sales
[0m10:32:25.489140 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:32:25.495643 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: create or replace  temporary view INCREMENTALETL._processed.processed_sales__dbt_tmp
  
  
  
  
  as (
    

WITH staging AS (
    SELECT * FROM INCREMENTALETL._staging.stg_sales
),

-- â”€â”€ Step 1: Remove duplicates (keep latest loaded record) â”€â”€â”€â”€â”€â”€â”€â”€
deduped AS (
    SELECT *,
        ROW_NUMBER() OVER (
            PARTITION BY invoice_no
            ORDER BY stg_loaded_at DESC
        ) AS row_num
    FROM staging
),

-- â”€â”€ Step 2: Keep only VALID records, remove bad data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cleaned AS (
    SELECT
        invoice_no,
        stock_code,
        description,
        quantity,
        unit_price,
        discount,
        shipping_cost,
        customer_id,
        invoice_date,
        invoice_year,
        invoice_month,
        country,
        payment_method,
        category,
        sales_channel,
        return_status,
        shipment_provider,
        warehouse_location,
        order_priority,

        -- â”€â”€ Derived / Enriched columns â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        ROUND(quantity * unit_price, 2)                        AS gross_amount,
        ROUND(quantity * unit_price * (1 - discount), 2)       AS net_amount,
        ROUND(quantity * unit_price * (1 - discount)
              + COALESCE(shipping_cost, 0), 2)                 AS total_amount,

        CASE
            WHEN return_status = 'RETURNED' THEN TRUE
            ELSE FALSE
        END AS is_returned,

        CASE
            WHEN customer_id IS NULL THEN TRUE
            ELSE FALSE
        END AS is_guest_customer,

        -- â”€â”€ Audit â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        stg_loaded_at,
        CURRENT_TIMESTAMP() AS processed_at

    FROM deduped
    WHERE
        row_num = 1                     -- remove duplicates
        AND data_quality_flag = 'VALID' -- only clean records
        AND quantity > 0                -- remove returns/negatives
        AND unit_price > 0              -- remove invalid prices
        AND invoice_date IS NOT NULL    -- must have valid date
),

-- â”€â”€ Step 3: Final output â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
final AS (
    SELECT * FROM cleaned
)

SELECT * FROM final


    -- â”€â”€ Incremental: only process months not already in processed â”€â”€
    WHERE (invoice_year, invoice_month) NOT IN (
        SELECT DISTINCT invoice_year, invoice_month
        FROM INCREMENTALETL._processed.processed_sales
    )

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:32:25.495643 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m10:32:26.402302 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.914 seconds
[0m10:32:26.421077 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:32:26.434813 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:32:26.618557 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.192 seconds
[0m10:32:26.658484 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:32:26.666863 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:32:26.846452 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.174 seconds
[0m10:32:26.886527 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:32:26.886527 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:32:27.061740 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.173 seconds
[0m10:32:27.076537 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:32:27.076537 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:32:27.256920 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.173 seconds
[0m10:32:27.289773 [debug] [Thread-4 (]: 
    In "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m10:32:27.297920 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.processed_sales"
[0m10:32:27.306652 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:32:27.308279 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:32:27.491945 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.191 seconds
[0m10:32:27.505110 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:32:27.509503 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: merge into INCREMENTALETL._processed.processed_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._processed.processed_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on ((DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no))

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","GROSS_AMOUNT" = DBT_INTERNAL_SOURCE."GROSS_AMOUNT","NET_AMOUNT" = DBT_INTERNAL_SOURCE."NET_AMOUNT","TOTAL_AMOUNT" = DBT_INTERNAL_SOURCE."TOTAL_AMOUNT","IS_RETURNED" = DBT_INTERNAL_SOURCE."IS_RETURNED","IS_GUEST_CUSTOMER" = DBT_INTERNAL_SOURCE."IS_GUEST_CUSTOMER","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","PROCESSED_AT" = DBT_INTERNAL_SOURCE."PROCESSED_AT"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:32:28.572627 [debug] [Thread-4 (]: SQL status: SUCCESS 2755 in 1.061 seconds
[0m10:32:28.572627 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:32:28.572627 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:32:28.873647 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.294 seconds
[0m10:32:28.889422 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._processed.processed_sales__dbt_tmp
[0m10:32:28.903261 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:32:28.907315 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: drop view if exists INCREMENTALETL._processed.processed_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:32:29.109516 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.200 seconds
[0m10:32:29.109516 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '685366ec-bb99-40b3-97f7-ec3ae3ad4b83', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CCC71F6230>]}
[0m10:32:29.117560 [info ] [Thread-4 (]: 2 of 2 OK created sql incremental model _processed.processed_sales ............. [[32mSUCCESS 2755[0m in 3.65s]
[0m10:32:29.120519 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.processed_sales
[0m10:32:29.125947 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:32:29.125947 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m10:32:29.131886 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m10:32:29.379361 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m10:32:29.388792 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m10:32:29.671473 [debug] [MainThread]: Connection 'list_INCREMENTALETL__staging' was left open.
[0m10:32:29.680011 [debug] [MainThread]: On list_INCREMENTALETL__staging: Close
[0m10:32:30.006781 [debug] [MainThread]: Connection 'list_INCREMENTALETL__processed' was left open.
[0m10:32:30.006781 [debug] [MainThread]: On list_INCREMENTALETL__processed: Close
[0m10:32:30.940490 [debug] [MainThread]: Connection 'model.sales_pipelines.stg_sales' was left open.
[0m10:32:30.944491 [debug] [MainThread]: On model.sales_pipelines.stg_sales: Close
[0m10:32:31.201919 [debug] [MainThread]: Connection 'model.sales_pipelines.processed_sales' was left open.
[0m10:32:31.206809 [debug] [MainThread]: On model.sales_pipelines.processed_sales: Close
[0m10:32:31.508701 [info ] [MainThread]: 
[0m10:32:31.511797 [info ] [MainThread]: Finished running 2 incremental models in 0 hours 0 minutes and 12.32 seconds (12.32s).
[0m10:32:31.513200 [debug] [MainThread]: Command end result
[0m10:32:31.608929 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m10:32:31.615866 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m10:32:31.641415 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\snowflake-incremental-pipeline\dbt_project\target\run_results.json
[0m10:32:31.641415 [info ] [MainThread]: 
[0m10:32:31.645608 [info ] [MainThread]: [32mCompleted successfully[0m
[0m10:32:31.645608 [info ] [MainThread]: 
[0m10:32:31.645608 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=2
[0m10:32:31.645608 [debug] [MainThread]: Command `dbt run` succeeded at 10:32:31.645608 after 15.04 seconds
[0m10:32:31.654927 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CCB3D77700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CCC869B730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CCB3763730>]}
[0m10:32:31.657214 [debug] [MainThread]: Flushing usage events
[0m10:32:33.175816 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:46:22.576056 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C2EF3476D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C2F15C1030>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C2F15C0880>]}


============================== 10:46:22.578290 | 27fdac5c-f8e1-456b-a2e9-3ebb870cc9c1 ==============================
[0m10:46:22.578290 [info ] [MainThread]: Running with dbt=1.11.6
[0m10:46:22.578290 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'empty': 'None', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'D:\\snowflake-incremental-pipeline\\dbt_project', 'indirect_selection': 'eager', 'invocation_command': 'dbt clean', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'D:\\snowflake-incremental-pipeline\\dbt_project\\logs'}
[0m10:46:23.107315 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '27fdac5c-f8e1-456b-a2e9-3ebb870cc9c1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C2F15C0880>]}
[0m10:46:23.170692 [debug] [MainThread]: Command `dbt clean` succeeded at 10:46:23.168677 after 0.74 seconds
[0m10:46:23.172708 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C2EF3476D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C2F15935B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C2F165D390>]}
[0m10:46:23.174467 [debug] [MainThread]: Flushing usage events
[0m10:46:24.658422 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:46:35.651107 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000010C330276A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000010C342D1000>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000010C342D23B0>]}


============================== 10:46:35.651107 | a6d40cb8-3051-459b-980c-e6f4b1de68a5 ==============================
[0m10:46:35.651107 [info ] [MainThread]: Running with dbt=1.11.6
[0m10:46:35.651107 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'profiles_dir': 'D:\\snowflake-incremental-pipeline\\dbt_project', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'use_colors': 'True', 'indirect_selection': 'eager', 'invocation_command': 'dbt run --full-refresh', 'use_experimental_parser': 'False', 'log_path': 'D:\\snowflake-incremental-pipeline\\dbt_project\\logs'}
[0m10:46:36.615198 [debug] [MainThread]: Snowflake adapter: Setting snowflake.connector to ERROR (file logging only)
[0m10:46:36.615198 [debug] [MainThread]: Snowflake adapter: Setting botocore to ERROR (file logging only)
[0m10:46:36.615198 [debug] [MainThread]: Snowflake adapter: Setting boto3 to ERROR (file logging only)
[0m10:46:36.916351 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a6d40cb8-3051-459b-980c-e6f4b1de68a5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000010C33AC52A0>]}
[0m10:46:36.996246 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a6d40cb8-3051-459b-980c-e6f4b1de68a5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000010C3436A890>]}
[0m10:46:36.996246 [info ] [MainThread]: Registered adapter: snowflake=1.11.2
[0m10:46:37.503333 [debug] [MainThread]: checksum: ec27062d18352e9cfca161c166fcc73d2201661a23a3cf3c89196a27f32cfcb8, vars: {}, profile: , target: , version: 1.11.6
[0m10:46:37.503333 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m10:46:37.519364 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'a6d40cb8-3051-459b-980c-e6f4b1de68a5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000010C33B27160>]}
[0m10:46:39.566563 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a6d40cb8-3051-459b-980c-e6f4b1de68a5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000010C46A9DF60>]}
[0m10:46:39.710276 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m10:46:39.710276 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m10:46:39.725067 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a6d40cb8-3051-459b-980c-e6f4b1de68a5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000010C459408B0>]}
[0m10:46:39.725067 [info ] [MainThread]: Found 2 models, 6 data tests, 1 source, 522 macros
[0m10:46:39.725067 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a6d40cb8-3051-459b-980c-e6f4b1de68a5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000010C45940790>]}
[0m10:46:39.740959 [info ] [MainThread]: 
[0m10:46:39.740959 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m10:46:39.744308 [info ] [MainThread]: 
[0m10:46:39.744308 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m10:46:39.744308 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m10:46:39.756863 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m10:46:39.851619 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m10:46:39.851619 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m10:46:39.851619 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m10:46:39.851619 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m10:46:39.851619 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:46:39.851619 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:46:41.564049 [debug] [ThreadPool]: SQL status: SUCCESS 2 in 1.705 seconds
[0m10:46:41.567150 [debug] [ThreadPool]: SQL status: SUCCESS 2 in 1.708 seconds
[0m10:46:41.583417 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_INCREMENTALETL, now create_INCREMENTALETL__processed)
[0m10:46:41.583417 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_INCREMENTALETL, now create_INCREMENTALETL__staging)
[0m10:46:41.583417 [debug] [ThreadPool]: Creating schema "database: "INCREMENTALETL"
schema: "_processed"
"
[0m10:46:41.591549 [debug] [ThreadPool]: Creating schema "database: "INCREMENTALETL"
schema: "_staging"
"
[0m10:46:41.613226 [debug] [ThreadPool]: Using snowflake connection "create_INCREMENTALETL__processed"
[0m10:46:41.625636 [debug] [ThreadPool]: Using snowflake connection "create_INCREMENTALETL__staging"
[0m10:46:41.627660 [debug] [ThreadPool]: On create_INCREMENTALETL__processed: create schema if not exists INCREMENTALETL._processed
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "create_INCREMENTALETL__processed"} */
[0m10:46:41.629452 [debug] [ThreadPool]: On create_INCREMENTALETL__staging: create schema if not exists INCREMENTALETL._staging
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "create_INCREMENTALETL__staging"} */
[0m10:46:41.958513 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.322 seconds
[0m10:46:41.961799 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.334 seconds
[0m10:46:41.974762 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__staging'
[0m10:46:41.979103 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__processed'
[0m10:46:42.024953 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__staging"
[0m10:46:42.040802 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__processed"
[0m10:46:42.040802 [debug] [ThreadPool]: On list_INCREMENTALETL__staging: show objects in INCREMENTALETL._staging
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__staging"} */;
[0m10:46:42.040802 [debug] [ThreadPool]: On list_INCREMENTALETL__processed: show objects in INCREMENTALETL._processed
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__processed"} */;
[0m10:46:42.040802 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:46:42.040802 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:46:42.963507 [debug] [ThreadPool]: SQL status: SUCCESS 0 in 0.912 seconds
[0m10:46:43.001942 [debug] [ThreadPool]: SQL status: SUCCESS 0 in 0.960 seconds
[0m10:46:43.017597 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a6d40cb8-3051-459b-980c-e6f4b1de68a5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000010C452DDEA0>]}
[0m10:46:43.035598 [debug] [Thread-2 (]: Began running node model.sales_pipelines.stg_sales
[0m10:46:43.035598 [info ] [Thread-2 (]: 1 of 2 START sql incremental model _staging.stg_sales .......................... [RUN]
[0m10:46:43.035598 [debug] [Thread-2 (]: Acquiring new snowflake connection 'model.sales_pipelines.stg_sales'
[0m10:46:43.035598 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.stg_sales
[0m10:46:43.067131 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.stg_sales"
[0m10:46:43.067131 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.stg_sales
[0m10:46:43.161322 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.stg_sales"
[0m10:46:43.161322 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:46:43.161322 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: create or replace transient table INCREMENTALETL._staging.stg_sales
    
    
    
    as (

WITH raw_data AS (
    SELECT * 
    FROM INCREMENTALETL.LANDINGZONE.RAW_SALES
),

staged AS (
    SELECT
        -- IDs
        CAST(INVOICENO AS VARCHAR)    AS invoice_no,
        CAST(STOCKCODE AS VARCHAR)    AS stock_code,
        CAST(DESCRIPTION AS VARCHAR)  AS description,

        -- Quantities & Prices
        TRY_CAST(QUANTITY AS INT)      AS quantity,
        TRY_CAST(UNITPRICE AS FLOAT)   AS unit_price,
        TRY_CAST(DISCOUNT AS FLOAT)    AS discount,
        TRY_CAST(SHIPPINGCOST AS FLOAT) AS shipping_cost,
        TRY_CAST(CUSTOMERID AS FLOAT)  AS customer_id,

        -- Convert once
        TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS') AS invoice_date,

        TRIM(UPPER(COUNTRY))            AS country,
        TRIM(UPPER(PAYMENTMETHOD))      AS payment_method,
        TRIM(UPPER(CATEGORY))           AS category,
        TRIM(UPPER(SALESCHANNEL))       AS sales_channel,
        TRIM(UPPER(RETURNSTATUS))       AS return_status,
        TRIM(UPPER(SHIPMENTPROVIDER))   AS shipment_provider,
        TRIM(UPPER(WAREHOUSELOCATION))  AS warehouse_location,
        TRIM(UPPER(ORDERPRIORITY))      AS order_priority,

        CURRENT_TIMESTAMP()             AS stg_loaded_at

    FROM raw_data
),

validated AS (
    SELECT
        *,
        YEAR(invoice_date)  AS invoice_year,
        MONTH(invoice_date) AS invoice_month,

        CASE
            WHEN invoice_date IS NULL           THEN 'INVALID_DATE'
            WHEN quantity IS NULL               THEN 'INVALID_QUANTITY'
            WHEN unit_price IS NULL             THEN 'INVALID_PRICE'
            WHEN shipping_cost < 0              THEN 'NEGATIVE_SHIPPING'
            WHEN discount < 0 OR discount > 1   THEN 'INVALID_DISCOUNT'
            WHEN invoice_no IS NULL             THEN 'NULL_INVOICE'
            ELSE 'VALID'
        END AS data_quality_flag
    FROM staged
)

SELECT *
FROM validated


    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:46:43.161322 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m10:46:45.236500 [debug] [Thread-2 (]: SQL status: SUCCESS 13872 in 2.061 seconds
[0m10:46:45.270336 [debug] [Thread-2 (]: Applying DROP to: INCREMENTALETL._staging.stg_sales__dbt_tmp
[0m10:46:45.282392 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:46:45.282392 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: drop view if exists INCREMENTALETL._staging.stg_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:46:45.463950 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.179 seconds
[0m10:46:45.497586 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a6d40cb8-3051-459b-980c-e6f4b1de68a5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000010C4749CCA0>]}
[0m10:46:45.499595 [info ] [Thread-2 (]: 1 of 2 OK created sql incremental model _staging.stg_sales ..................... [[32mSUCCESS 13872[0m in 2.46s]
[0m10:46:45.499595 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.stg_sales
[0m10:46:45.501977 [debug] [Thread-4 (]: Began running node model.sales_pipelines.processed_sales
[0m10:46:45.501977 [info ] [Thread-4 (]: 2 of 2 START sql incremental model _processed.processed_sales .................. [RUN]
[0m10:46:45.501977 [debug] [Thread-4 (]: Acquiring new snowflake connection 'model.sales_pipelines.processed_sales'
[0m10:46:45.501977 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.processed_sales
[0m10:46:45.509584 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.processed_sales"
[0m10:46:45.509584 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.processed_sales
[0m10:46:45.518349 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.processed_sales"
[0m10:46:45.527654 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:46:45.527654 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: create or replace transient table INCREMENTALETL._processed.processed_sales
    
    
    
    as (

WITH staging AS (
    SELECT * 
    FROM INCREMENTALETL._staging.stg_sales
),

deduped AS (
    SELECT *,
        ROW_NUMBER() OVER (
            PARTITION BY invoice_no, stock_code
            ORDER BY stg_loaded_at DESC
        ) AS row_num
    FROM staging
),

cleaned AS (
    SELECT
        invoice_no,
        stock_code,
        description,
        quantity,
        unit_price,
        discount,
        shipping_cost,
        customer_id,
        invoice_date,
        invoice_year,
        invoice_month,
        country,
        payment_method,
        category,
        sales_channel,
        return_status,
        shipment_provider,
        warehouse_location,
        order_priority,

        -- Derived metrics
        ROUND(quantity * unit_price, 2) AS gross_amount,
        ROUND(quantity * unit_price * (1 - discount), 2) AS net_amount,
        ROUND(
            quantity * unit_price * (1 - discount)
            + COALESCE(shipping_cost, 0),
        2) AS total_amount,

        CASE WHEN return_status = 'RETURNED' THEN TRUE ELSE FALSE END AS is_returned,
        CASE WHEN customer_id IS NULL THEN TRUE ELSE FALSE END AS is_guest_customer,

        stg_loaded_at,
        CURRENT_TIMESTAMP() AS processed_at

    FROM deduped
    WHERE
        row_num = 1
        AND data_quality_flag = 'VALID'
        AND invoice_date IS NOT NULL
)

SELECT *
FROM cleaned


    )

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:46:45.530354 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m10:46:50.094727 [debug] [Thread-4 (]: SQL status: SUCCESS 13606 in 4.571 seconds
[0m10:46:50.107639 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._processed.processed_sales__dbt_tmp
[0m10:46:50.107639 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:46:50.123684 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: drop view if exists INCREMENTALETL._processed.processed_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:46:50.270117 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.144 seconds
[0m10:46:50.270117 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a6d40cb8-3051-459b-980c-e6f4b1de68a5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000010C4745FCA0>]}
[0m10:46:50.283446 [info ] [Thread-4 (]: 2 of 2 OK created sql incremental model _processed.processed_sales ............. [[32mSUCCESS 13606[0m in 4.77s]
[0m10:46:50.283446 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.processed_sales
[0m10:46:50.283446 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:46:50.299383 [debug] [MainThread]: Connection 'create_INCREMENTALETL__staging' was left open.
[0m10:46:50.299383 [debug] [MainThread]: On create_INCREMENTALETL__staging: Close
[0m10:46:50.649122 [debug] [MainThread]: Connection 'create_INCREMENTALETL__processed' was left open.
[0m10:46:50.649122 [debug] [MainThread]: On create_INCREMENTALETL__processed: Close
[0m10:46:51.081372 [debug] [MainThread]: Connection 'list_INCREMENTALETL__staging' was left open.
[0m10:46:51.083388 [debug] [MainThread]: On list_INCREMENTALETL__staging: Close
[0m10:46:51.382122 [debug] [MainThread]: Connection 'list_INCREMENTALETL__processed' was left open.
[0m10:46:51.382122 [debug] [MainThread]: On list_INCREMENTALETL__processed: Close
[0m10:46:51.785067 [debug] [MainThread]: Connection 'model.sales_pipelines.stg_sales' was left open.
[0m10:46:51.785067 [debug] [MainThread]: On model.sales_pipelines.stg_sales: Close
[0m10:46:52.087186 [debug] [MainThread]: Connection 'model.sales_pipelines.processed_sales' was left open.
[0m10:46:52.089209 [debug] [MainThread]: On model.sales_pipelines.processed_sales: Close
[0m10:46:52.444829 [info ] [MainThread]: 
[0m10:46:52.460703 [info ] [MainThread]: Finished running 2 incremental models in 0 hours 0 minutes and 12.70 seconds (12.70s).
[0m10:46:52.464345 [debug] [MainThread]: Command end result
[0m10:46:52.508400 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m10:46:52.508400 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m10:46:52.524499 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\snowflake-incremental-pipeline\dbt_project\target\run_results.json
[0m10:46:52.524499 [info ] [MainThread]: 
[0m10:46:52.524499 [info ] [MainThread]: [32mCompleted successfully[0m
[0m10:46:52.524499 [info ] [MainThread]: 
[0m10:46:52.524499 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=2
[0m10:46:52.524499 [debug] [MainThread]: Command `dbt run` succeeded at 10:46:52.524499 after 17.01 seconds
[0m10:46:52.524499 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000010C330276A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000010C33AC52A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000010C31EC5FF0>]}
[0m10:46:52.524499 [debug] [MainThread]: Flushing usage events
[0m10:46:53.521446 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:49:31.837591 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015912A27730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015913CA5C30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015913CA79D0>]}


============================== 10:49:31.852422 | f98f1b2a-86c1-4a27-a79e-a2208cec61a1 ==============================
[0m10:49:31.852422 [info ] [MainThread]: Running with dbt=1.11.6
[0m10:49:31.854266 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'D:\\snowflake-incremental-pipeline\\dbt_project', 'invocation_command': 'dbt run', 'use_colors': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'D:\\snowflake-incremental-pipeline\\dbt_project\\logs'}
[0m10:49:32.776856 [debug] [MainThread]: Snowflake adapter: Setting snowflake.connector to ERROR (file logging only)
[0m10:49:32.776856 [debug] [MainThread]: Snowflake adapter: Setting botocore to ERROR (file logging only)
[0m10:49:32.778867 [debug] [MainThread]: Snowflake adapter: Setting boto3 to ERROR (file logging only)
[0m10:49:33.080891 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f98f1b2a-86c1-4a27-a79e-a2208cec61a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015924CCD4B0>]}
[0m10:49:33.161843 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f98f1b2a-86c1-4a27-a79e-a2208cec61a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015913527100>]}
[0m10:49:33.161843 [info ] [MainThread]: Registered adapter: snowflake=1.11.2
[0m10:49:33.684490 [debug] [MainThread]: checksum: ec27062d18352e9cfca161c166fcc73d2201661a23a3cf3c89196a27f32cfcb8, vars: {}, profile: , target: , version: 1.11.6
[0m10:49:33.968595 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m10:49:33.968595 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m10:49:33.968595 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m10:49:34.043528 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f98f1b2a-86c1-4a27-a79e-a2208cec61a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000159263BC130>]}
[0m10:49:34.177651 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m10:49:34.177651 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m10:49:34.209588 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f98f1b2a-86c1-4a27-a79e-a2208cec61a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001592635C700>]}
[0m10:49:34.209588 [info ] [MainThread]: Found 2 models, 6 data tests, 1 source, 522 macros
[0m10:49:34.209588 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f98f1b2a-86c1-4a27-a79e-a2208cec61a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001592635C790>]}
[0m10:49:34.216013 [info ] [MainThread]: 
[0m10:49:34.216013 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m10:49:34.216013 [info ] [MainThread]: 
[0m10:49:34.216013 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m10:49:34.229111 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m10:49:34.241706 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m10:49:34.336377 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m10:49:34.336377 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m10:49:34.336377 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m10:49:34.336377 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m10:49:34.336377 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:49:34.336377 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:49:35.635422 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 1.296 seconds
[0m10:49:35.672722 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 1.334 seconds
[0m10:49:35.672722 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__processed'
[0m10:49:35.688519 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__staging'
[0m10:49:35.736238 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__processed"
[0m10:49:35.736238 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__staging"
[0m10:49:35.736238 [debug] [ThreadPool]: On list_INCREMENTALETL__processed: show objects in INCREMENTALETL._processed
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__processed"} */;
[0m10:49:35.736238 [debug] [ThreadPool]: On list_INCREMENTALETL__staging: show objects in INCREMENTALETL._staging
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__staging"} */;
[0m10:49:35.752027 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:49:35.752027 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:49:36.519696 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.781 seconds
[0m10:49:36.600259 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.851 seconds
[0m10:49:36.600259 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f98f1b2a-86c1-4a27-a79e-a2208cec61a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015924CCC160>]}
[0m10:49:36.616298 [debug] [Thread-2 (]: Began running node model.sales_pipelines.stg_sales
[0m10:49:36.616298 [info ] [Thread-2 (]: 1 of 2 START sql incremental model _staging.stg_sales .......................... [RUN]
[0m10:49:36.635430 [debug] [Thread-2 (]: Acquiring new snowflake connection 'model.sales_pipelines.stg_sales'
[0m10:49:36.639491 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.stg_sales
[0m10:49:36.664846 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.stg_sales"
[0m10:49:36.664846 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.stg_sales
[0m10:49:36.743225 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:49:36.743225 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: create or replace  temporary view INCREMENTALETL._staging.stg_sales__dbt_tmp
  
  
  
  
  as (
    

WITH raw_data AS (
    SELECT * 
    FROM INCREMENTALETL.LANDINGZONE.RAW_SALES
),

staged AS (
    SELECT
        -- IDs
        CAST(INVOICENO AS VARCHAR)    AS invoice_no,
        CAST(STOCKCODE AS VARCHAR)    AS stock_code,
        CAST(DESCRIPTION AS VARCHAR)  AS description,

        -- Quantities & Prices
        TRY_CAST(QUANTITY AS INT)      AS quantity,
        TRY_CAST(UNITPRICE AS FLOAT)   AS unit_price,
        TRY_CAST(DISCOUNT AS FLOAT)    AS discount,
        TRY_CAST(SHIPPINGCOST AS FLOAT) AS shipping_cost,
        TRY_CAST(CUSTOMERID AS FLOAT)  AS customer_id,

        -- Convert once
        TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS') AS invoice_date,

        TRIM(UPPER(COUNTRY))            AS country,
        TRIM(UPPER(PAYMENTMETHOD))      AS payment_method,
        TRIM(UPPER(CATEGORY))           AS category,
        TRIM(UPPER(SALESCHANNEL))       AS sales_channel,
        TRIM(UPPER(RETURNSTATUS))       AS return_status,
        TRIM(UPPER(SHIPMENTPROVIDER))   AS shipment_provider,
        TRIM(UPPER(WAREHOUSELOCATION))  AS warehouse_location,
        TRIM(UPPER(ORDERPRIORITY))      AS order_priority,

        CURRENT_TIMESTAMP()             AS stg_loaded_at

    FROM raw_data
),

validated AS (
    SELECT
        *,
        YEAR(invoice_date)  AS invoice_year,
        MONTH(invoice_date) AS invoice_month,

        CASE
            WHEN invoice_date IS NULL           THEN 'INVALID_DATE'
            WHEN quantity IS NULL               THEN 'INVALID_QUANTITY'
            WHEN unit_price IS NULL             THEN 'INVALID_PRICE'
            WHEN shipping_cost < 0              THEN 'NEGATIVE_SHIPPING'
            WHEN discount < 0 OR discount > 1   THEN 'INVALID_DISCOUNT'
            WHEN invoice_no IS NULL             THEN 'NULL_INVOICE'
            ELSE 'VALID'
        END AS data_quality_flag
    FROM staged
)

SELECT *
FROM validated


WHERE NOT EXISTS (
    SELECT 1
    FROM INCREMENTALETL._staging.stg_sales t
    WHERE t.invoice_year  = validated.invoice_year
      AND t.invoice_month = validated.invoice_month
)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:49:36.743225 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m10:49:37.774287 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 1.017 seconds
[0m10:49:37.806322 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:49:37.806322 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:49:37.983472 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.174 seconds
[0m10:49:37.999296 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:49:37.999296 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:49:38.212645 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.197 seconds
[0m10:49:38.260231 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:49:38.260231 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:49:38.481321 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.233 seconds
[0m10:49:38.608843 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:49:38.608843 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table "INCREMENTALETL"."_STAGING"."STG_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:49:39.221586 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.619 seconds
[0m10:49:39.269741 [debug] [Thread-2 (]: 
    In "INCREMENTALETL"."_STAGING"."STG_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m10:49:39.301621 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.stg_sales"
[0m10:49:39.301621 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:49:39.301621 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:49:39.504160 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.187 seconds
[0m10:49:39.504160 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:49:39.504160 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: merge into INCREMENTALETL._staging.stg_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._staging.stg_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","DATA_QUALITY_FLAG" = DBT_INTERNAL_SOURCE."DATA_QUALITY_FLAG"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "INVOICE_YEAR", "INVOICE_MONTH", "DATA_QUALITY_FLAG")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "INVOICE_YEAR", "INVOICE_MONTH", "DATA_QUALITY_FLAG")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:49:40.435518 [debug] [Thread-2 (]: SQL status: SUCCESS 744 in 0.931 seconds
[0m10:49:40.435518 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:49:40.446885 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:49:40.848855 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.400 seconds
[0m10:49:40.867763 [debug] [Thread-2 (]: Applying DROP to: INCREMENTALETL._staging.stg_sales__dbt_tmp
[0m10:49:40.883901 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:49:40.883901 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: drop view if exists INCREMENTALETL._staging.stg_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:49:41.080722 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.198 seconds
[0m10:49:41.141142 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f98f1b2a-86c1-4a27-a79e-a2208cec61a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000159118ABF40>]}
[0m10:49:41.141142 [info ] [Thread-2 (]: 1 of 2 OK created sql incremental model _staging.stg_sales ..................... [[32mSUCCESS 744[0m in 4.51s]
[0m10:49:41.141142 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.stg_sales
[0m10:49:41.141142 [debug] [Thread-4 (]: Began running node model.sales_pipelines.processed_sales
[0m10:49:41.141142 [info ] [Thread-4 (]: 2 of 2 START sql incremental model _processed.processed_sales .................. [RUN]
[0m10:49:41.141142 [debug] [Thread-4 (]: Acquiring new snowflake connection 'model.sales_pipelines.processed_sales'
[0m10:49:41.141142 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.processed_sales
[0m10:49:41.157032 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.processed_sales"
[0m10:49:41.157032 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.processed_sales
[0m10:49:41.172876 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:49:41.172876 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: create or replace  temporary view INCREMENTALETL._processed.processed_sales__dbt_tmp
  
  
  
  
  as (
    

WITH staging AS (
    SELECT * 
    FROM INCREMENTALETL._staging.stg_sales
),

deduped AS (
    SELECT *,
        ROW_NUMBER() OVER (
            PARTITION BY invoice_no, stock_code
            ORDER BY stg_loaded_at DESC
        ) AS row_num
    FROM staging
),

cleaned AS (
    SELECT
        invoice_no,
        stock_code,
        description,
        quantity,
        unit_price,
        discount,
        shipping_cost,
        customer_id,
        invoice_date,
        invoice_year,
        invoice_month,
        country,
        payment_method,
        category,
        sales_channel,
        return_status,
        shipment_provider,
        warehouse_location,
        order_priority,

        -- Derived metrics
        ROUND(quantity * unit_price, 2) AS gross_amount,
        ROUND(quantity * unit_price * (1 - discount), 2) AS net_amount,
        ROUND(
            quantity * unit_price * (1 - discount)
            + COALESCE(shipping_cost, 0),
        2) AS total_amount,

        CASE WHEN return_status = 'RETURNED' THEN TRUE ELSE FALSE END AS is_returned,
        CASE WHEN customer_id IS NULL THEN TRUE ELSE FALSE END AS is_guest_customer,

        stg_loaded_at,
        CURRENT_TIMESTAMP() AS processed_at

    FROM deduped
    WHERE
        row_num = 1
        AND data_quality_flag = 'VALID'
        AND invoice_date IS NOT NULL
)

SELECT *
FROM cleaned


WHERE NOT EXISTS (
    SELECT 1
    FROM INCREMENTALETL._processed.processed_sales t
    WHERE t.invoice_year  = cleaned.invoice_year
      AND t.invoice_month = cleaned.invoice_month
)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:49:41.172876 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m10:49:41.956155 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.789 seconds
[0m10:49:41.971967 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:49:41.971967 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:49:42.137653 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.163 seconds
[0m10:49:42.165385 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:49:42.165385 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:49:42.390001 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.220 seconds
[0m10:49:42.415986 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:49:42.417994 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:49:42.586296 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.168 seconds
[0m10:49:42.598989 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:49:42.598989 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:49:42.743895 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.132 seconds
[0m10:49:42.775725 [debug] [Thread-4 (]: 
    In "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m10:49:42.775725 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.processed_sales"
[0m10:49:42.775725 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:49:42.775725 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:49:43.004277 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.216 seconds
[0m10:49:43.004277 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:49:43.004277 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: merge into INCREMENTALETL._processed.processed_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._processed.processed_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","GROSS_AMOUNT" = DBT_INTERNAL_SOURCE."GROSS_AMOUNT","NET_AMOUNT" = DBT_INTERNAL_SOURCE."NET_AMOUNT","TOTAL_AMOUNT" = DBT_INTERNAL_SOURCE."TOTAL_AMOUNT","IS_RETURNED" = DBT_INTERNAL_SOURCE."IS_RETURNED","IS_GUEST_CUSTOMER" = DBT_INTERNAL_SOURCE."IS_GUEST_CUSTOMER","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","PROCESSED_AT" = DBT_INTERNAL_SOURCE."PROCESSED_AT"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:49:44.020283 [debug] [Thread-4 (]: SQL status: SUCCESS 728 in 1.011 seconds
[0m10:49:44.020283 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:49:44.020283 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:49:44.325775 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.300 seconds
[0m10:49:44.341801 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._processed.processed_sales__dbt_tmp
[0m10:49:44.341801 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:49:44.357696 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: drop view if exists INCREMENTALETL._processed.processed_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:49:44.534862 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.176 seconds
[0m10:49:44.534862 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f98f1b2a-86c1-4a27-a79e-a2208cec61a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015926BA0BE0>]}
[0m10:49:44.550736 [info ] [Thread-4 (]: 2 of 2 OK created sql incremental model _processed.processed_sales ............. [[32mSUCCESS 728[0m in 3.39s]
[0m10:49:44.550736 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.processed_sales
[0m10:49:44.561703 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:49:44.561703 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m10:49:44.566943 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m10:49:44.871917 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m10:49:44.871917 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m10:49:45.115634 [debug] [MainThread]: Connection 'list_INCREMENTALETL__processed' was left open.
[0m10:49:45.115634 [debug] [MainThread]: On list_INCREMENTALETL__processed: Close
[0m10:49:45.370048 [debug] [MainThread]: Connection 'list_INCREMENTALETL__staging' was left open.
[0m10:49:45.371070 [debug] [MainThread]: On list_INCREMENTALETL__staging: Close
[0m10:49:45.660804 [debug] [MainThread]: Connection 'model.sales_pipelines.stg_sales' was left open.
[0m10:49:45.660804 [debug] [MainThread]: On model.sales_pipelines.stg_sales: Close
[0m10:49:45.966619 [debug] [MainThread]: Connection 'model.sales_pipelines.processed_sales' was left open.
[0m10:49:45.966619 [debug] [MainThread]: On model.sales_pipelines.processed_sales: Close
[0m10:49:46.209535 [info ] [MainThread]: 
[0m10:49:46.223540 [info ] [MainThread]: Finished running 2 incremental models in 0 hours 0 minutes and 11.99 seconds (11.99s).
[0m10:49:46.223540 [debug] [MainThread]: Command end result
[0m10:49:46.287182 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m10:49:46.287182 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m10:49:46.303191 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\snowflake-incremental-pipeline\dbt_project\target\run_results.json
[0m10:49:46.303191 [info ] [MainThread]: 
[0m10:49:46.303191 [info ] [MainThread]: [32mCompleted successfully[0m
[0m10:49:46.303191 [info ] [MainThread]: 
[0m10:49:46.303191 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=2
[0m10:49:46.303191 [debug] [MainThread]: Command `dbt run` succeeded at 10:49:46.303191 after 14.59 seconds
[0m10:49:46.312787 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015912A27730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000159269ECA90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000159269ECB50>]}
[0m10:49:46.312787 [debug] [MainThread]: Flushing usage events
[0m10:49:47.428047 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:49:58.857296 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0B10576A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0B32D1000>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0B32D23B0>]}


============================== 10:49:58.860077 | ad477342-ad7e-47f2-9ea3-fba58d080e1e ==============================
[0m10:49:58.860077 [info ] [MainThread]: Running with dbt=1.11.6
[0m10:49:58.860077 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'profiles_dir': 'D:\\snowflake-incremental-pipeline\\dbt_project', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'use_colors': 'True', 'invocation_command': 'dbt run', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'D:\\snowflake-incremental-pipeline\\dbt_project\\logs'}
[0m10:49:59.816204 [debug] [MainThread]: Snowflake adapter: Setting snowflake.connector to ERROR (file logging only)
[0m10:49:59.816204 [debug] [MainThread]: Snowflake adapter: Setting botocore to ERROR (file logging only)
[0m10:49:59.830978 [debug] [MainThread]: Snowflake adapter: Setting boto3 to ERROR (file logging only)
[0m10:50:00.130192 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ad477342-ad7e-47f2-9ea3-fba58d080e1e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0B2AC12A0>]}
[0m10:50:00.208572 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ad477342-ad7e-47f2-9ea3-fba58d080e1e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0B336A890>]}
[0m10:50:00.208572 [info ] [MainThread]: Registered adapter: snowflake=1.11.2
[0m10:50:00.746872 [debug] [MainThread]: checksum: ec27062d18352e9cfca161c166fcc73d2201661a23a3cf3c89196a27f32cfcb8, vars: {}, profile: , target: , version: 1.11.6
[0m10:50:00.969191 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m10:50:00.969191 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m10:50:00.969191 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m10:50:01.048387 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ad477342-ad7e-47f2-9ea3-fba58d080e1e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0C59CC130>]}
[0m10:50:01.193962 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m10:50:01.193962 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m10:50:01.206937 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ad477342-ad7e-47f2-9ea3-fba58d080e1e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0C596CCD0>]}
[0m10:50:01.206937 [info ] [MainThread]: Found 2 models, 6 data tests, 1 source, 522 macros
[0m10:50:01.206937 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ad477342-ad7e-47f2-9ea3-fba58d080e1e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0C596CC70>]}
[0m10:50:01.206937 [info ] [MainThread]: 
[0m10:50:01.222862 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m10:50:01.222862 [info ] [MainThread]: 
[0m10:50:01.224616 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m10:50:01.229360 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m10:50:01.238902 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m10:50:01.350633 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m10:50:01.350633 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m10:50:01.350633 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m10:50:01.350633 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m10:50:01.350633 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:50:01.350633 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:50:02.218057 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 0.865 seconds
[0m10:50:02.234052 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 0.883 seconds
[0m10:50:02.249870 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__staging'
[0m10:50:02.265265 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__processed'
[0m10:50:02.297339 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__staging"
[0m10:50:02.313185 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__processed"
[0m10:50:02.313185 [debug] [ThreadPool]: On list_INCREMENTALETL__staging: show objects in INCREMENTALETL._staging
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__staging"} */;
[0m10:50:02.313185 [debug] [ThreadPool]: On list_INCREMENTALETL__processed: show objects in INCREMENTALETL._processed
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__processed"} */;
[0m10:50:02.313185 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:50:02.327767 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:50:03.097110 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.785 seconds
[0m10:50:03.139578 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.812 seconds
[0m10:50:03.151107 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ad477342-ad7e-47f2-9ea3-fba58d080e1e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0C47ABA60>]}
[0m10:50:03.162789 [debug] [Thread-2 (]: Began running node model.sales_pipelines.stg_sales
[0m10:50:03.162789 [info ] [Thread-2 (]: 1 of 2 START sql incremental model _staging.stg_sales .......................... [RUN]
[0m10:50:03.176863 [debug] [Thread-2 (]: Acquiring new snowflake connection 'model.sales_pipelines.stg_sales'
[0m10:50:03.178893 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.stg_sales
[0m10:50:03.218663 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.stg_sales"
[0m10:50:03.220671 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.stg_sales
[0m10:50:03.295600 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:50:03.297606 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: create or replace  temporary view INCREMENTALETL._staging.stg_sales__dbt_tmp
  
  
  
  
  as (
    

WITH raw_data AS (
    SELECT * 
    FROM INCREMENTALETL.LANDINGZONE.RAW_SALES
),

staged AS (
    SELECT
        -- IDs
        CAST(INVOICENO AS VARCHAR)    AS invoice_no,
        CAST(STOCKCODE AS VARCHAR)    AS stock_code,
        CAST(DESCRIPTION AS VARCHAR)  AS description,

        -- Quantities & Prices
        TRY_CAST(QUANTITY AS INT)      AS quantity,
        TRY_CAST(UNITPRICE AS FLOAT)   AS unit_price,
        TRY_CAST(DISCOUNT AS FLOAT)    AS discount,
        TRY_CAST(SHIPPINGCOST AS FLOAT) AS shipping_cost,
        TRY_CAST(CUSTOMERID AS FLOAT)  AS customer_id,

        -- Convert once
        TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS') AS invoice_date,

        TRIM(UPPER(COUNTRY))            AS country,
        TRIM(UPPER(PAYMENTMETHOD))      AS payment_method,
        TRIM(UPPER(CATEGORY))           AS category,
        TRIM(UPPER(SALESCHANNEL))       AS sales_channel,
        TRIM(UPPER(RETURNSTATUS))       AS return_status,
        TRIM(UPPER(SHIPMENTPROVIDER))   AS shipment_provider,
        TRIM(UPPER(WAREHOUSELOCATION))  AS warehouse_location,
        TRIM(UPPER(ORDERPRIORITY))      AS order_priority,

        CURRENT_TIMESTAMP()             AS stg_loaded_at

    FROM raw_data
),

validated AS (
    SELECT
        *,
        YEAR(invoice_date)  AS invoice_year,
        MONTH(invoice_date) AS invoice_month,

        CASE
            WHEN invoice_date IS NULL           THEN 'INVALID_DATE'
            WHEN quantity IS NULL               THEN 'INVALID_QUANTITY'
            WHEN unit_price IS NULL             THEN 'INVALID_PRICE'
            WHEN shipping_cost < 0              THEN 'NEGATIVE_SHIPPING'
            WHEN discount < 0 OR discount > 1   THEN 'INVALID_DISCOUNT'
            WHEN invoice_no IS NULL             THEN 'NULL_INVOICE'
            ELSE 'VALID'
        END AS data_quality_flag
    FROM staged
)

SELECT *
FROM validated


WHERE NOT EXISTS (
    SELECT 1
    FROM INCREMENTALETL._staging.stg_sales t
    WHERE t.invoice_year  = validated.invoice_year
      AND t.invoice_month = validated.invoice_month
)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:50:03.297606 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m10:50:04.311183 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 1.013 seconds
[0m10:50:04.345241 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:50:04.345241 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:50:04.514471 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.171 seconds
[0m10:50:04.541747 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:50:04.541747 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:50:04.714068 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.168 seconds
[0m10:50:04.759897 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:50:04.759897 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:50:04.917487 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.158 seconds
[0m10:50:05.020731 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:50:05.020731 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table "INCREMENTALETL"."_STAGING"."STG_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:50:05.322303 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.288 seconds
[0m10:50:05.365918 [debug] [Thread-2 (]: 
    In "INCREMENTALETL"."_STAGING"."STG_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m10:50:05.403524 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.stg_sales"
[0m10:50:05.407280 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:50:05.407280 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:50:05.616380 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.218 seconds
[0m10:50:05.616380 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:50:05.632263 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: merge into INCREMENTALETL._staging.stg_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._staging.stg_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","DATA_QUALITY_FLAG" = DBT_INTERNAL_SOURCE."DATA_QUALITY_FLAG"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "INVOICE_YEAR", "INVOICE_MONTH", "DATA_QUALITY_FLAG")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "INVOICE_YEAR", "INVOICE_MONTH", "DATA_QUALITY_FLAG")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:50:06.651174 [debug] [Thread-2 (]: SQL status: SUCCESS 720 in 1.014 seconds
[0m10:50:06.651174 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:50:06.651174 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:50:06.936472 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.282 seconds
[0m10:50:06.952494 [debug] [Thread-2 (]: Applying DROP to: INCREMENTALETL._staging.stg_sales__dbt_tmp
[0m10:50:06.968174 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:50:06.968174 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: drop view if exists INCREMENTALETL._staging.stg_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:50:07.191350 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.215 seconds
[0m10:50:07.223279 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ad477342-ad7e-47f2-9ea3-fba58d080e1e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0C4438C10>]}
[0m10:50:07.223279 [info ] [Thread-2 (]: 1 of 2 OK created sql incremental model _staging.stg_sales ..................... [[32mSUCCESS 720[0m in 4.06s]
[0m10:50:07.223279 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.stg_sales
[0m10:50:07.223279 [debug] [Thread-4 (]: Began running node model.sales_pipelines.processed_sales
[0m10:50:07.230175 [info ] [Thread-4 (]: 2 of 2 START sql incremental model _processed.processed_sales .................. [RUN]
[0m10:50:07.230175 [debug] [Thread-4 (]: Acquiring new snowflake connection 'model.sales_pipelines.processed_sales'
[0m10:50:07.230175 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.processed_sales
[0m10:50:07.239297 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.processed_sales"
[0m10:50:07.239297 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.processed_sales
[0m10:50:07.243340 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:50:07.243340 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: create or replace  temporary view INCREMENTALETL._processed.processed_sales__dbt_tmp
  
  
  
  
  as (
    

WITH staging AS (
    SELECT * 
    FROM INCREMENTALETL._staging.stg_sales
),

deduped AS (
    SELECT *,
        ROW_NUMBER() OVER (
            PARTITION BY invoice_no, stock_code
            ORDER BY stg_loaded_at DESC
        ) AS row_num
    FROM staging
),

cleaned AS (
    SELECT
        invoice_no,
        stock_code,
        description,
        quantity,
        unit_price,
        discount,
        shipping_cost,
        customer_id,
        invoice_date,
        invoice_year,
        invoice_month,
        country,
        payment_method,
        category,
        sales_channel,
        return_status,
        shipment_provider,
        warehouse_location,
        order_priority,

        -- Derived metrics
        ROUND(quantity * unit_price, 2) AS gross_amount,
        ROUND(quantity * unit_price * (1 - discount), 2) AS net_amount,
        ROUND(
            quantity * unit_price * (1 - discount)
            + COALESCE(shipping_cost, 0),
        2) AS total_amount,

        CASE WHEN return_status = 'RETURNED' THEN TRUE ELSE FALSE END AS is_returned,
        CASE WHEN customer_id IS NULL THEN TRUE ELSE FALSE END AS is_guest_customer,

        stg_loaded_at,
        CURRENT_TIMESTAMP() AS processed_at

    FROM deduped
    WHERE
        row_num = 1
        AND data_quality_flag = 'VALID'
        AND invoice_date IS NOT NULL
)

SELECT *
FROM cleaned


WHERE NOT EXISTS (
    SELECT 1
    FROM INCREMENTALETL._processed.processed_sales t
    WHERE t.invoice_year  = cleaned.invoice_year
      AND t.invoice_month = cleaned.invoice_month
)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:50:07.243340 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m10:50:08.012410 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.761 seconds
[0m10:50:08.012410 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:50:08.012410 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:50:08.174052 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.153 seconds
[0m10:50:08.185845 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:50:08.201712 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:50:08.491152 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.297 seconds
[0m10:50:08.522823 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:50:08.522823 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:50:08.675010 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.145 seconds
[0m10:50:08.703547 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:50:08.705561 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:50:08.905524 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.197 seconds
[0m10:50:08.935527 [debug] [Thread-4 (]: 
    In "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m10:50:08.943347 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.processed_sales"
[0m10:50:08.951391 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:50:08.951391 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:50:09.113080 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.163 seconds
[0m10:50:09.113080 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:50:09.113080 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: merge into INCREMENTALETL._processed.processed_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._processed.processed_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","GROSS_AMOUNT" = DBT_INTERNAL_SOURCE."GROSS_AMOUNT","NET_AMOUNT" = DBT_INTERNAL_SOURCE."NET_AMOUNT","TOTAL_AMOUNT" = DBT_INTERNAL_SOURCE."TOTAL_AMOUNT","IS_RETURNED" = DBT_INTERNAL_SOURCE."IS_RETURNED","IS_GUEST_CUSTOMER" = DBT_INTERNAL_SOURCE."IS_GUEST_CUSTOMER","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","PROCESSED_AT" = DBT_INTERNAL_SOURCE."PROCESSED_AT"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:50:10.225615 [debug] [Thread-4 (]: SQL status: SUCCESS 712 in 1.110 seconds
[0m10:50:10.225615 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:50:10.225615 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:50:10.536247 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.303 seconds
[0m10:50:10.548988 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._processed.processed_sales__dbt_tmp
[0m10:50:10.564845 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:50:10.564845 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: drop view if exists INCREMENTALETL._processed.processed_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:50:10.774162 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.202 seconds
[0m10:50:10.774162 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ad477342-ad7e-47f2-9ea3-fba58d080e1e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0C61B17B0>]}
[0m10:50:10.789946 [info ] [Thread-4 (]: 2 of 2 OK created sql incremental model _processed.processed_sales ............. [[32mSUCCESS 712[0m in 3.54s]
[0m10:50:10.789946 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.processed_sales
[0m10:50:10.801392 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:50:10.801392 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m10:50:10.807347 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m10:50:11.045848 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m10:50:11.048457 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m10:50:11.306143 [debug] [MainThread]: Connection 'list_INCREMENTALETL__staging' was left open.
[0m10:50:11.306143 [debug] [MainThread]: On list_INCREMENTALETL__staging: Close
[0m10:50:11.546974 [debug] [MainThread]: Connection 'list_INCREMENTALETL__processed' was left open.
[0m10:50:11.546974 [debug] [MainThread]: On list_INCREMENTALETL__processed: Close
[0m10:50:11.884174 [debug] [MainThread]: Connection 'model.sales_pipelines.stg_sales' was left open.
[0m10:50:11.886197 [debug] [MainThread]: On model.sales_pipelines.stg_sales: Close
[0m10:50:12.187167 [debug] [MainThread]: Connection 'model.sales_pipelines.processed_sales' was left open.
[0m10:50:12.187167 [debug] [MainThread]: On model.sales_pipelines.processed_sales: Close
[0m10:50:12.428169 [info ] [MainThread]: 
[0m10:50:12.444355 [info ] [MainThread]: Finished running 2 incremental models in 0 hours 0 minutes and 11.20 seconds (11.20s).
[0m10:50:12.444355 [debug] [MainThread]: Command end result
[0m10:50:12.511473 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m10:50:12.511473 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m10:50:12.528926 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\snowflake-incremental-pipeline\dbt_project\target\run_results.json
[0m10:50:12.528926 [info ] [MainThread]: 
[0m10:50:12.528926 [info ] [MainThread]: [32mCompleted successfully[0m
[0m10:50:12.528926 [info ] [MainThread]: 
[0m10:50:12.528926 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=2
[0m10:50:12.528926 [debug] [MainThread]: Command `dbt run` succeeded at 10:50:12.528926 after 13.80 seconds
[0m10:50:12.528926 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0B10576A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0B23EDC60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0C596D540>]}
[0m10:50:12.528926 [debug] [MainThread]: Flushing usage events
[0m10:50:13.637613 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:50:32.442599 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023F9FA27670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023FA1CA3340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023FA1CA29B0>]}


============================== 10:50:32.442599 | 7483d62c-96c4-44cd-8a58-c2cfbd244492 ==============================
[0m10:50:32.442599 [info ] [MainThread]: Running with dbt=1.11.6
[0m10:50:32.454176 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'D:\\snowflake-incremental-pipeline\\dbt_project', 'indirect_selection': 'eager', 'use_colors': 'True', 'invocation_command': 'dbt run', 'use_experimental_parser': 'False', 'log_path': 'D:\\snowflake-incremental-pipeline\\dbt_project\\logs'}
[0m10:50:33.476291 [debug] [MainThread]: Snowflake adapter: Setting snowflake.connector to ERROR (file logging only)
[0m10:50:33.476291 [debug] [MainThread]: Snowflake adapter: Setting botocore to ERROR (file logging only)
[0m10:50:33.476291 [debug] [MainThread]: Snowflake adapter: Setting boto3 to ERROR (file logging only)
[0m10:50:33.777152 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7483d62c-96c4-44cd-8a58-c2cfbd244492', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023FA1B693F0>]}
[0m10:50:33.856635 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7483d62c-96c4-44cd-8a58-c2cfbd244492', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023FA1CA3010>]}
[0m10:50:33.872420 [info ] [MainThread]: Registered adapter: snowflake=1.11.2
[0m10:50:34.380672 [debug] [MainThread]: checksum: ec27062d18352e9cfca161c166fcc73d2201661a23a3cf3c89196a27f32cfcb8, vars: {}, profile: , target: , version: 1.11.6
[0m10:50:34.618242 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m10:50:34.618242 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m10:50:34.618242 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m10:50:34.681959 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7483d62c-96c4-44cd-8a58-c2cfbd244492', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023FB43A0130>]}
[0m10:50:34.824496 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m10:50:34.824496 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m10:50:34.856609 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7483d62c-96c4-44cd-8a58-c2cfbd244492', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023FB433C940>]}
[0m10:50:34.856609 [info ] [MainThread]: Found 2 models, 6 data tests, 1 source, 522 macros
[0m10:50:34.861700 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7483d62c-96c4-44cd-8a58-c2cfbd244492', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023FB433CA00>]}
[0m10:50:34.861700 [info ] [MainThread]: 
[0m10:50:34.861700 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m10:50:34.861700 [info ] [MainThread]: 
[0m10:50:34.868124 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m10:50:34.872441 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m10:50:34.888163 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m10:50:34.966990 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m10:50:34.966990 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m10:50:34.966990 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m10:50:34.982745 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m10:50:34.983924 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:50:34.984301 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:50:36.967324 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 1.994 seconds
[0m10:50:36.967324 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 1.997 seconds
[0m10:50:36.983311 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__staging'
[0m10:50:36.999090 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__processed'
[0m10:50:37.020813 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__staging"
[0m10:50:37.028400 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__processed"
[0m10:50:37.028400 [debug] [ThreadPool]: On list_INCREMENTALETL__staging: show objects in INCREMENTALETL._staging
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__staging"} */;
[0m10:50:37.030743 [debug] [ThreadPool]: On list_INCREMENTALETL__processed: show objects in INCREMENTALETL._processed
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__processed"} */;
[0m10:50:37.030743 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:50:37.030743 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:50:37.815999 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.782 seconds
[0m10:50:37.825887 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.797 seconds
[0m10:50:37.831858 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7483d62c-96c4-44cd-8a58-c2cfbd244492', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023F9E0570A0>]}
[0m10:50:37.855962 [debug] [Thread-2 (]: Began running node model.sales_pipelines.stg_sales
[0m10:50:37.860972 [info ] [Thread-2 (]: 1 of 2 START sql incremental model _staging.stg_sales .......................... [RUN]
[0m10:50:37.865951 [debug] [Thread-2 (]: Acquiring new snowflake connection 'model.sales_pipelines.stg_sales'
[0m10:50:37.865951 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.stg_sales
[0m10:50:37.911042 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.stg_sales"
[0m10:50:37.911588 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.stg_sales
[0m10:50:37.974254 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:50:37.974254 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: create or replace  temporary view INCREMENTALETL._staging.stg_sales__dbt_tmp
  
  
  
  
  as (
    

WITH raw_data AS (
    SELECT * 
    FROM INCREMENTALETL.LANDINGZONE.RAW_SALES
),

staged AS (
    SELECT
        -- IDs
        CAST(INVOICENO AS VARCHAR)    AS invoice_no,
        CAST(STOCKCODE AS VARCHAR)    AS stock_code,
        CAST(DESCRIPTION AS VARCHAR)  AS description,

        -- Quantities & Prices
        TRY_CAST(QUANTITY AS INT)      AS quantity,
        TRY_CAST(UNITPRICE AS FLOAT)   AS unit_price,
        TRY_CAST(DISCOUNT AS FLOAT)    AS discount,
        TRY_CAST(SHIPPINGCOST AS FLOAT) AS shipping_cost,
        TRY_CAST(CUSTOMERID AS FLOAT)  AS customer_id,

        -- Convert once
        TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS') AS invoice_date,

        TRIM(UPPER(COUNTRY))            AS country,
        TRIM(UPPER(PAYMENTMETHOD))      AS payment_method,
        TRIM(UPPER(CATEGORY))           AS category,
        TRIM(UPPER(SALESCHANNEL))       AS sales_channel,
        TRIM(UPPER(RETURNSTATUS))       AS return_status,
        TRIM(UPPER(SHIPMENTPROVIDER))   AS shipment_provider,
        TRIM(UPPER(WAREHOUSELOCATION))  AS warehouse_location,
        TRIM(UPPER(ORDERPRIORITY))      AS order_priority,

        CURRENT_TIMESTAMP()             AS stg_loaded_at

    FROM raw_data
),

validated AS (
    SELECT
        *,
        YEAR(invoice_date)  AS invoice_year,
        MONTH(invoice_date) AS invoice_month,

        CASE
            WHEN invoice_date IS NULL           THEN 'INVALID_DATE'
            WHEN quantity IS NULL               THEN 'INVALID_QUANTITY'
            WHEN unit_price IS NULL             THEN 'INVALID_PRICE'
            WHEN shipping_cost < 0              THEN 'NEGATIVE_SHIPPING'
            WHEN discount < 0 OR discount > 1   THEN 'INVALID_DISCOUNT'
            WHEN invoice_no IS NULL             THEN 'NULL_INVOICE'
            ELSE 'VALID'
        END AS data_quality_flag
    FROM staged
)

SELECT *
FROM validated


WHERE NOT EXISTS (
    SELECT 1
    FROM INCREMENTALETL._staging.stg_sales t
    WHERE t.invoice_year  = validated.invoice_year
      AND t.invoice_month = validated.invoice_month
)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:50:37.990349 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m10:50:38.867402 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.877 seconds
[0m10:50:38.901455 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:50:38.901455 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:50:39.050710 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.149 seconds
[0m10:50:39.074309 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:50:39.074309 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:50:39.235128 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.156 seconds
[0m10:50:39.284734 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:50:39.284734 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:50:39.438576 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.157 seconds
[0m10:50:39.547324 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:50:39.547324 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table "INCREMENTALETL"."_STAGING"."STG_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:50:39.689116 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.132 seconds
[0m10:50:39.732986 [debug] [Thread-2 (]: 
    In "INCREMENTALETL"."_STAGING"."STG_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m10:50:39.772363 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.stg_sales"
[0m10:50:39.776380 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:50:39.776380 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:50:39.929687 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.163 seconds
[0m10:50:39.929687 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:50:39.945355 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: merge into INCREMENTALETL._staging.stg_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._staging.stg_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","DATA_QUALITY_FLAG" = DBT_INTERNAL_SOURCE."DATA_QUALITY_FLAG"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "INVOICE_YEAR", "INVOICE_MONTH", "DATA_QUALITY_FLAG")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "INVOICE_YEAR", "INVOICE_MONTH", "DATA_QUALITY_FLAG")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:50:40.873197 [debug] [Thread-2 (]: SQL status: SUCCESS 744 in 0.923 seconds
[0m10:50:40.873197 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:50:40.875204 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:50:41.173185 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.296 seconds
[0m10:50:41.197087 [debug] [Thread-2 (]: Applying DROP to: INCREMENTALETL._staging.stg_sales__dbt_tmp
[0m10:50:41.197087 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:50:41.210877 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: drop view if exists INCREMENTALETL._staging.stg_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:50:41.398235 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.186 seconds
[0m10:50:41.442439 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7483d62c-96c4-44cd-8a58-c2cfbd244492', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023FB2EA75E0>]}
[0m10:50:41.442439 [info ] [Thread-2 (]: 1 of 2 OK created sql incremental model _staging.stg_sales ..................... [[32mSUCCESS 744[0m in 3.58s]
[0m10:50:41.442439 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.stg_sales
[0m10:50:41.458132 [debug] [Thread-4 (]: Began running node model.sales_pipelines.processed_sales
[0m10:50:41.458132 [info ] [Thread-4 (]: 2 of 2 START sql incremental model _processed.processed_sales .................. [RUN]
[0m10:50:41.460236 [debug] [Thread-4 (]: Acquiring new snowflake connection 'model.sales_pipelines.processed_sales'
[0m10:50:41.460236 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.processed_sales
[0m10:50:41.460236 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.processed_sales"
[0m10:50:41.460236 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.processed_sales
[0m10:50:41.483267 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:50:41.483267 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: create or replace  temporary view INCREMENTALETL._processed.processed_sales__dbt_tmp
  
  
  
  
  as (
    

WITH staging AS (
    SELECT * 
    FROM INCREMENTALETL._staging.stg_sales
),

deduped AS (
    SELECT *,
        ROW_NUMBER() OVER (
            PARTITION BY invoice_no, stock_code
            ORDER BY stg_loaded_at DESC
        ) AS row_num
    FROM staging
),

cleaned AS (
    SELECT
        invoice_no,
        stock_code,
        description,
        quantity,
        unit_price,
        discount,
        shipping_cost,
        customer_id,
        invoice_date,
        invoice_year,
        invoice_month,
        country,
        payment_method,
        category,
        sales_channel,
        return_status,
        shipment_provider,
        warehouse_location,
        order_priority,

        -- Derived metrics
        ROUND(quantity * unit_price, 2) AS gross_amount,
        ROUND(quantity * unit_price * (1 - discount), 2) AS net_amount,
        ROUND(
            quantity * unit_price * (1 - discount)
            + COALESCE(shipping_cost, 0),
        2) AS total_amount,

        CASE WHEN return_status = 'RETURNED' THEN TRUE ELSE FALSE END AS is_returned,
        CASE WHEN customer_id IS NULL THEN TRUE ELSE FALSE END AS is_guest_customer,

        stg_loaded_at,
        CURRENT_TIMESTAMP() AS processed_at

    FROM deduped
    WHERE
        row_num = 1
        AND data_quality_flag = 'VALID'
        AND invoice_date IS NOT NULL
)

SELECT *
FROM cleaned


WHERE NOT EXISTS (
    SELECT 1
    FROM INCREMENTALETL._processed.processed_sales t
    WHERE t.invoice_year  = cleaned.invoice_year
      AND t.invoice_month = cleaned.invoice_month
)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:50:41.483267 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m10:50:42.698453 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 1.214 seconds
[0m10:50:42.709103 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:50:42.709103 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:50:42.902990 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.182 seconds
[0m10:50:42.918917 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:50:42.918917 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:50:43.112779 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.186 seconds
[0m10:50:43.130989 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:50:43.130989 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:50:43.314980 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.172 seconds
[0m10:50:43.338372 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:50:43.338372 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:50:43.560287 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.217 seconds
[0m10:50:43.580481 [debug] [Thread-4 (]: 
    In "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m10:50:43.596445 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.processed_sales"
[0m10:50:43.600160 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:50:43.600160 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:50:43.808664 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.217 seconds
[0m10:50:43.824794 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:50:43.824794 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: merge into INCREMENTALETL._processed.processed_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._processed.processed_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","GROSS_AMOUNT" = DBT_INTERNAL_SOURCE."GROSS_AMOUNT","NET_AMOUNT" = DBT_INTERNAL_SOURCE."NET_AMOUNT","TOTAL_AMOUNT" = DBT_INTERNAL_SOURCE."TOTAL_AMOUNT","IS_RETURNED" = DBT_INTERNAL_SOURCE."IS_RETURNED","IS_GUEST_CUSTOMER" = DBT_INTERNAL_SOURCE."IS_GUEST_CUSTOMER","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","PROCESSED_AT" = DBT_INTERNAL_SOURCE."PROCESSED_AT"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:50:44.903410 [debug] [Thread-4 (]: SQL status: SUCCESS 726 in 1.076 seconds
[0m10:50:44.903410 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:50:44.903410 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:50:45.180215 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.275 seconds
[0m10:50:45.214805 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._processed.processed_sales__dbt_tmp
[0m10:50:45.218845 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:50:45.220855 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: drop view if exists INCREMENTALETL._processed.processed_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:50:45.467259 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.252 seconds
[0m10:50:45.483265 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7483d62c-96c4-44cd-8a58-c2cfbd244492', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023F9F443C40>]}
[0m10:50:45.483265 [info ] [Thread-4 (]: 2 of 2 OK created sql incremental model _processed.processed_sales ............. [[32mSUCCESS 726[0m in 4.02s]
[0m10:50:45.493470 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.processed_sales
[0m10:50:45.498982 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:50:45.498982 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m10:50:45.498982 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m10:50:45.783883 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m10:50:45.785903 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m10:50:46.187338 [debug] [MainThread]: Connection 'list_INCREMENTALETL__staging' was left open.
[0m10:50:46.190882 [debug] [MainThread]: On list_INCREMENTALETL__staging: Close
[0m10:50:46.605828 [debug] [MainThread]: Connection 'list_INCREMENTALETL__processed' was left open.
[0m10:50:46.605828 [debug] [MainThread]: On list_INCREMENTALETL__processed: Close
[0m10:50:46.844948 [debug] [MainThread]: Connection 'model.sales_pipelines.stg_sales' was left open.
[0m10:50:46.844948 [debug] [MainThread]: On model.sales_pipelines.stg_sales: Close
[0m10:50:47.109868 [debug] [MainThread]: Connection 'model.sales_pipelines.processed_sales' was left open.
[0m10:50:47.113919 [debug] [MainThread]: On model.sales_pipelines.processed_sales: Close
[0m10:50:47.512331 [info ] [MainThread]: 
[0m10:50:47.514355 [info ] [MainThread]: Finished running 2 incremental models in 0 hours 0 minutes and 12.64 seconds (12.64s).
[0m10:50:47.522105 [debug] [MainThread]: Command end result
[0m10:50:47.581659 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m10:50:47.583666 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m10:50:47.593442 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\snowflake-incremental-pipeline\dbt_project\target\run_results.json
[0m10:50:47.595450 [info ] [MainThread]: 
[0m10:50:47.595450 [info ] [MainThread]: [32mCompleted successfully[0m
[0m10:50:47.595450 [info ] [MainThread]: 
[0m10:50:47.598431 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=2
[0m10:50:47.598431 [debug] [MainThread]: Command `dbt run` succeeded at 10:50:47.598431 after 15.30 seconds
[0m10:50:47.601794 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023F9FA27670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023FB2CAEE00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023FB2CAEE60>]}
[0m10:50:47.601794 [debug] [MainThread]: Flushing usage events
[0m10:50:48.833454 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:51:06.553366 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC4ECA76A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC4FF51000>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC4FF523B0>]}


============================== 10:51:06.553366 | 2fd2c949-83d3-4a3e-8c80-30ad08813cbe ==============================
[0m10:51:06.553366 [info ] [MainThread]: Running with dbt=1.11.6
[0m10:51:06.553366 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'profiles_dir': 'D:\\snowflake-incremental-pipeline\\dbt_project', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'indirect_selection': 'eager', 'use_colors': 'True', 'invocation_command': 'dbt run', 'use_experimental_parser': 'False', 'log_path': 'D:\\snowflake-incremental-pipeline\\dbt_project\\logs'}
[0m10:51:07.615324 [debug] [MainThread]: Snowflake adapter: Setting snowflake.connector to ERROR (file logging only)
[0m10:51:07.615324 [debug] [MainThread]: Snowflake adapter: Setting botocore to ERROR (file logging only)
[0m10:51:07.615324 [debug] [MainThread]: Snowflake adapter: Setting boto3 to ERROR (file logging only)
[0m10:51:07.931573 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2fd2c949-83d3-4a3e-8c80-30ad08813cbe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC4F7412A0>]}
[0m10:51:08.011198 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2fd2c949-83d3-4a3e-8c80-30ad08813cbe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC4FFEA890>]}
[0m10:51:08.011198 [info ] [MainThread]: Registered adapter: snowflake=1.11.2
[0m10:51:08.502416 [debug] [MainThread]: checksum: ec27062d18352e9cfca161c166fcc73d2201661a23a3cf3c89196a27f32cfcb8, vars: {}, profile: , target: , version: 1.11.6
[0m10:51:08.755926 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m10:51:08.755926 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m10:51:08.755926 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m10:51:08.835509 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2fd2c949-83d3-4a3e-8c80-30ad08813cbe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC6264C130>]}
[0m10:51:08.978104 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m10:51:08.978104 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m10:51:08.993951 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2fd2c949-83d3-4a3e-8c80-30ad08813cbe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC625ECCD0>]}
[0m10:51:09.009622 [info ] [MainThread]: Found 2 models, 6 data tests, 1 source, 522 macros
[0m10:51:09.009622 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2fd2c949-83d3-4a3e-8c80-30ad08813cbe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC625ECC70>]}
[0m10:51:09.012920 [info ] [MainThread]: 
[0m10:51:09.012920 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m10:51:09.012920 [info ] [MainThread]: 
[0m10:51:09.012920 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m10:51:09.025600 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m10:51:09.041382 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m10:51:09.152946 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m10:51:09.152946 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m10:51:09.152946 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m10:51:09.152946 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m10:51:09.152946 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:51:09.152946 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:51:10.476838 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 1.315 seconds
[0m10:51:10.852307 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 1.690 seconds
[0m10:51:10.852307 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__processed'
[0m10:51:10.866874 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__staging'
[0m10:51:10.897061 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__processed"
[0m10:51:10.913118 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__staging"
[0m10:51:10.913118 [debug] [ThreadPool]: On list_INCREMENTALETL__processed: show objects in INCREMENTALETL._processed
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__processed"} */;
[0m10:51:10.913118 [debug] [ThreadPool]: On list_INCREMENTALETL__staging: show objects in INCREMENTALETL._staging
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__staging"} */;
[0m10:51:10.913118 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:51:10.913118 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:51:11.848156 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.929 seconds
[0m10:51:12.261560 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 1.339 seconds
[0m10:51:12.267591 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2fd2c949-83d3-4a3e-8c80-30ad08813cbe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC626AF6A0>]}
[0m10:51:12.273397 [debug] [Thread-2 (]: Began running node model.sales_pipelines.stg_sales
[0m10:51:12.278746 [info ] [Thread-2 (]: 1 of 2 START sql incremental model _staging.stg_sales .......................... [RUN]
[0m10:51:12.281862 [debug] [Thread-2 (]: Acquiring new snowflake connection 'model.sales_pipelines.stg_sales'
[0m10:51:12.281862 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.stg_sales
[0m10:51:12.314593 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.stg_sales"
[0m10:51:12.314593 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.stg_sales
[0m10:51:12.407425 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:51:12.407425 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: create or replace  temporary view INCREMENTALETL._staging.stg_sales__dbt_tmp
  
  
  
  
  as (
    

WITH raw_data AS (
    SELECT * 
    FROM INCREMENTALETL.LANDINGZONE.RAW_SALES
),

staged AS (
    SELECT
        -- IDs
        CAST(INVOICENO AS VARCHAR)    AS invoice_no,
        CAST(STOCKCODE AS VARCHAR)    AS stock_code,
        CAST(DESCRIPTION AS VARCHAR)  AS description,

        -- Quantities & Prices
        TRY_CAST(QUANTITY AS INT)      AS quantity,
        TRY_CAST(UNITPRICE AS FLOAT)   AS unit_price,
        TRY_CAST(DISCOUNT AS FLOAT)    AS discount,
        TRY_CAST(SHIPPINGCOST AS FLOAT) AS shipping_cost,
        TRY_CAST(CUSTOMERID AS FLOAT)  AS customer_id,

        -- Convert once
        TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS') AS invoice_date,

        TRIM(UPPER(COUNTRY))            AS country,
        TRIM(UPPER(PAYMENTMETHOD))      AS payment_method,
        TRIM(UPPER(CATEGORY))           AS category,
        TRIM(UPPER(SALESCHANNEL))       AS sales_channel,
        TRIM(UPPER(RETURNSTATUS))       AS return_status,
        TRIM(UPPER(SHIPMENTPROVIDER))   AS shipment_provider,
        TRIM(UPPER(WAREHOUSELOCATION))  AS warehouse_location,
        TRIM(UPPER(ORDERPRIORITY))      AS order_priority,

        CURRENT_TIMESTAMP()             AS stg_loaded_at

    FROM raw_data
),

validated AS (
    SELECT
        *,
        YEAR(invoice_date)  AS invoice_year,
        MONTH(invoice_date) AS invoice_month,

        CASE
            WHEN invoice_date IS NULL           THEN 'INVALID_DATE'
            WHEN quantity IS NULL               THEN 'INVALID_QUANTITY'
            WHEN unit_price IS NULL             THEN 'INVALID_PRICE'
            WHEN shipping_cost < 0              THEN 'NEGATIVE_SHIPPING'
            WHEN discount < 0 OR discount > 1   THEN 'INVALID_DISCOUNT'
            WHEN invoice_no IS NULL             THEN 'NULL_INVOICE'
            ELSE 'VALID'
        END AS data_quality_flag
    FROM staged
)

SELECT *
FROM validated


WHERE NOT EXISTS (
    SELECT 1
    FROM INCREMENTALETL._staging.stg_sales t
    WHERE t.invoice_year  = validated.invoice_year
      AND t.invoice_month = validated.invoice_month
)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:51:12.409472 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m10:51:13.346120 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.939 seconds
[0m10:51:13.370471 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:51:13.370471 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:51:13.565346 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.189 seconds
[0m10:51:13.581595 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:51:13.581595 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:51:13.772447 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.188 seconds
[0m10:51:13.820539 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:51:13.822550 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:51:13.968707 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.145 seconds
[0m10:51:14.060121 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:51:14.075291 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table "INCREMENTALETL"."_STAGING"."STG_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:51:14.666128 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.591 seconds
[0m10:51:14.705838 [debug] [Thread-2 (]: 
    In "INCREMENTALETL"."_STAGING"."STG_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m10:51:14.751789 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.stg_sales"
[0m10:51:14.757707 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:51:14.757707 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:51:14.936817 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.192 seconds
[0m10:51:14.952631 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:51:14.952631 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: merge into INCREMENTALETL._staging.stg_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._staging.stg_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","DATA_QUALITY_FLAG" = DBT_INTERNAL_SOURCE."DATA_QUALITY_FLAG"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "INVOICE_YEAR", "INVOICE_MONTH", "DATA_QUALITY_FLAG")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "INVOICE_YEAR", "INVOICE_MONTH", "DATA_QUALITY_FLAG")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:51:15.826059 [debug] [Thread-2 (]: SQL status: SUCCESS 720 in 0.875 seconds
[0m10:51:15.826059 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:51:15.826059 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:51:16.112705 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.285 seconds
[0m10:51:16.136011 [debug] [Thread-2 (]: Applying DROP to: INCREMENTALETL._staging.stg_sales__dbt_tmp
[0m10:51:16.144385 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:51:16.146652 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: drop view if exists INCREMENTALETL._staging.stg_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:51:16.337273 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.193 seconds
[0m10:51:16.385215 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2fd2c949-83d3-4a3e-8c80-30ad08813cbe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC4FFE8EE0>]}
[0m10:51:16.401248 [info ] [Thread-2 (]: 1 of 2 OK created sql incremental model _staging.stg_sales ..................... [[32mSUCCESS 720[0m in 4.10s]
[0m10:51:16.401248 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.stg_sales
[0m10:51:16.403634 [debug] [Thread-4 (]: Began running node model.sales_pipelines.processed_sales
[0m10:51:16.403634 [info ] [Thread-4 (]: 2 of 2 START sql incremental model _processed.processed_sales .................. [RUN]
[0m10:51:16.403634 [debug] [Thread-4 (]: Acquiring new snowflake connection 'model.sales_pipelines.processed_sales'
[0m10:51:16.408205 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.processed_sales
[0m10:51:16.408205 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.processed_sales"
[0m10:51:16.417126 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.processed_sales
[0m10:51:16.417126 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:51:16.417126 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: create or replace  temporary view INCREMENTALETL._processed.processed_sales__dbt_tmp
  
  
  
  
  as (
    

WITH staging AS (
    SELECT * 
    FROM INCREMENTALETL._staging.stg_sales
),

deduped AS (
    SELECT *,
        ROW_NUMBER() OVER (
            PARTITION BY invoice_no, stock_code
            ORDER BY stg_loaded_at DESC
        ) AS row_num
    FROM staging
),

cleaned AS (
    SELECT
        invoice_no,
        stock_code,
        description,
        quantity,
        unit_price,
        discount,
        shipping_cost,
        customer_id,
        invoice_date,
        invoice_year,
        invoice_month,
        country,
        payment_method,
        category,
        sales_channel,
        return_status,
        shipment_provider,
        warehouse_location,
        order_priority,

        -- Derived metrics
        ROUND(quantity * unit_price, 2) AS gross_amount,
        ROUND(quantity * unit_price * (1 - discount), 2) AS net_amount,
        ROUND(
            quantity * unit_price * (1 - discount)
            + COALESCE(shipping_cost, 0),
        2) AS total_amount,

        CASE WHEN return_status = 'RETURNED' THEN TRUE ELSE FALSE END AS is_returned,
        CASE WHEN customer_id IS NULL THEN TRUE ELSE FALSE END AS is_guest_customer,

        stg_loaded_at,
        CURRENT_TIMESTAMP() AS processed_at

    FROM deduped
    WHERE
        row_num = 1
        AND data_quality_flag = 'VALID'
        AND invoice_date IS NOT NULL
)

SELECT *
FROM cleaned


WHERE NOT EXISTS (
    SELECT 1
    FROM INCREMENTALETL._processed.processed_sales t
    WHERE t.invoice_year  = cleaned.invoice_year
      AND t.invoice_month = cleaned.invoice_month
)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:51:16.417126 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m10:51:17.315100 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.887 seconds
[0m10:51:17.316702 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:51:17.332837 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:51:17.484811 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.149 seconds
[0m10:51:17.509572 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:51:17.513338 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:51:17.657508 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.151 seconds
[0m10:51:17.689483 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:51:17.689483 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:51:17.834451 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.141 seconds
[0m10:51:17.850364 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:51:17.850364 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:51:17.991826 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.133 seconds
[0m10:51:18.023574 [debug] [Thread-4 (]: 
    In "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m10:51:18.023574 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.processed_sales"
[0m10:51:18.023574 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:51:18.023574 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:51:18.216341 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.192 seconds
[0m10:51:18.216341 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:51:18.232406 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: merge into INCREMENTALETL._processed.processed_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._processed.processed_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","GROSS_AMOUNT" = DBT_INTERNAL_SOURCE."GROSS_AMOUNT","NET_AMOUNT" = DBT_INTERNAL_SOURCE."NET_AMOUNT","TOTAL_AMOUNT" = DBT_INTERNAL_SOURCE."TOTAL_AMOUNT","IS_RETURNED" = DBT_INTERNAL_SOURCE."IS_RETURNED","IS_GUEST_CUSTOMER" = DBT_INTERNAL_SOURCE."IS_GUEST_CUSTOMER","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","PROCESSED_AT" = DBT_INTERNAL_SOURCE."PROCESSED_AT"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:51:19.355589 [debug] [Thread-4 (]: SQL status: SUCCESS 701 in 1.135 seconds
[0m10:51:19.355589 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:51:19.371608 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:51:19.634166 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.260 seconds
[0m10:51:19.654378 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._processed.processed_sales__dbt_tmp
[0m10:51:19.660194 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:51:19.662212 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: drop view if exists INCREMENTALETL._processed.processed_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:51:19.842209 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.181 seconds
[0m10:51:19.856323 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2fd2c949-83d3-4a3e-8c80-30ad08813cbe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC62E08070>]}
[0m10:51:19.856323 [info ] [Thread-4 (]: 2 of 2 OK created sql incremental model _processed.processed_sales ............. [[32mSUCCESS 701[0m in 3.45s]
[0m10:51:19.863894 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.processed_sales
[0m10:51:19.870510 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:51:19.870510 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m10:51:19.870510 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m10:51:20.158648 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m10:51:20.158648 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m10:51:20.488844 [debug] [MainThread]: Connection 'list_INCREMENTALETL__processed' was left open.
[0m10:51:20.490870 [debug] [MainThread]: On list_INCREMENTALETL__processed: Close
[0m10:51:23.653142 [debug] [MainThread]: Connection 'list_INCREMENTALETL__staging' was left open.
[0m10:51:23.655162 [debug] [MainThread]: On list_INCREMENTALETL__staging: Close
[0m10:51:23.949850 [debug] [MainThread]: Connection 'model.sales_pipelines.stg_sales' was left open.
[0m10:51:23.951871 [debug] [MainThread]: On model.sales_pipelines.stg_sales: Close
[0m10:51:24.635711 [debug] [MainThread]: Connection 'model.sales_pipelines.processed_sales' was left open.
[0m10:51:24.635711 [debug] [MainThread]: On model.sales_pipelines.processed_sales: Close
[0m10:51:24.910519 [info ] [MainThread]: 
[0m10:51:24.910519 [info ] [MainThread]: Finished running 2 incremental models in 0 hours 0 minutes and 15.90 seconds (15.90s).
[0m10:51:24.910519 [debug] [MainThread]: Command end result
[0m10:51:24.974296 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m10:51:24.974296 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m10:51:24.990172 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\snowflake-incremental-pipeline\dbt_project\target\run_results.json
[0m10:51:24.990172 [info ] [MainThread]: 
[0m10:51:24.990172 [info ] [MainThread]: [32mCompleted successfully[0m
[0m10:51:24.990172 [info ] [MainThread]: 
[0m10:51:24.990172 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=2
[0m10:51:24.990172 [debug] [MainThread]: Command `dbt run` succeeded at 10:51:24.990172 after 18.57 seconds
[0m10:51:24.990172 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC4ECA76A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC4C06D360>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC60F5F940>]}
[0m10:51:24.990172 [debug] [MainThread]: Flushing usage events
[0m10:51:25.980289 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:51:40.585031 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001580AF17610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001580C1C2020>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001580C1C3790>]}


============================== 10:51:40.585031 | 15290ead-3dfb-4908-8af3-adb11ac18979 ==============================
[0m10:51:40.585031 [info ] [MainThread]: Running with dbt=1.11.6
[0m10:51:40.597102 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'profiles_dir': 'D:\\snowflake-incremental-pipeline\\dbt_project', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'use_colors': 'True', 'invocation_command': 'dbt run', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'D:\\snowflake-incremental-pipeline\\dbt_project\\logs'}
[0m10:51:41.543678 [debug] [MainThread]: Snowflake adapter: Setting snowflake.connector to ERROR (file logging only)
[0m10:51:41.543678 [debug] [MainThread]: Snowflake adapter: Setting botocore to ERROR (file logging only)
[0m10:51:41.543678 [debug] [MainThread]: Snowflake adapter: Setting boto3 to ERROR (file logging only)
[0m10:51:41.838311 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '15290ead-3dfb-4908-8af3-adb11ac18979', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001580C201210>]}
[0m10:51:41.917184 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '15290ead-3dfb-4908-8af3-adb11ac18979', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001581D249540>]}
[0m10:51:41.917184 [info ] [MainThread]: Registered adapter: snowflake=1.11.2
[0m10:51:42.423946 [debug] [MainThread]: checksum: ec27062d18352e9cfca161c166fcc73d2201661a23a3cf3c89196a27f32cfcb8, vars: {}, profile: , target: , version: 1.11.6
[0m10:51:42.645044 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m10:51:42.645044 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m10:51:42.645044 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m10:51:42.741372 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '15290ead-3dfb-4908-8af3-adb11ac18979', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001581E8BC130>]}
[0m10:51:42.883242 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m10:51:42.883242 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m10:51:42.914781 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '15290ead-3dfb-4908-8af3-adb11ac18979', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001581E85E8F0>]}
[0m10:51:42.914781 [info ] [MainThread]: Found 2 models, 6 data tests, 1 source, 522 macros
[0m10:51:42.914781 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '15290ead-3dfb-4908-8af3-adb11ac18979', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001581E85EA10>]}
[0m10:51:42.920359 [info ] [MainThread]: 
[0m10:51:42.920359 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m10:51:42.920359 [info ] [MainThread]: 
[0m10:51:42.920359 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m10:51:42.930911 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m10:51:42.946604 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m10:51:43.027483 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m10:51:43.027483 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m10:51:43.041997 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m10:51:43.043166 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m10:51:43.043166 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:51:43.044337 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:51:44.787805 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 1.749 seconds
[0m10:51:44.813904 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 1.775 seconds
[0m10:51:44.829164 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__processed'
[0m10:51:44.829164 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__staging'
[0m10:51:44.860878 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__processed"
[0m10:51:44.880818 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__staging"
[0m10:51:44.880818 [debug] [ThreadPool]: On list_INCREMENTALETL__processed: show objects in INCREMENTALETL._processed
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__processed"} */;
[0m10:51:44.883897 [debug] [ThreadPool]: On list_INCREMENTALETL__staging: show objects in INCREMENTALETL._staging
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__staging"} */;
[0m10:51:44.883897 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:51:44.883897 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:51:45.775227 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.891 seconds
[0m10:51:45.777251 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 0.896 seconds
[0m10:51:45.788869 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '15290ead-3dfb-4908-8af3-adb11ac18979', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001581D1CCD30>]}
[0m10:51:45.809479 [debug] [Thread-2 (]: Began running node model.sales_pipelines.stg_sales
[0m10:51:45.820455 [info ] [Thread-2 (]: 1 of 2 START sql incremental model _staging.stg_sales .......................... [RUN]
[0m10:51:45.822663 [debug] [Thread-2 (]: Acquiring new snowflake connection 'model.sales_pipelines.stg_sales'
[0m10:51:45.822663 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.stg_sales
[0m10:51:45.853705 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.stg_sales"
[0m10:51:45.853705 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.stg_sales
[0m10:51:45.931423 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:51:45.931423 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: create or replace  temporary view INCREMENTALETL._staging.stg_sales__dbt_tmp
  
  
  
  
  as (
    

WITH raw_data AS (
    SELECT * 
    FROM INCREMENTALETL.LANDINGZONE.RAW_SALES
),

staged AS (
    SELECT
        -- IDs
        CAST(INVOICENO AS VARCHAR)    AS invoice_no,
        CAST(STOCKCODE AS VARCHAR)    AS stock_code,
        CAST(DESCRIPTION AS VARCHAR)  AS description,

        -- Quantities & Prices
        TRY_CAST(QUANTITY AS INT)      AS quantity,
        TRY_CAST(UNITPRICE AS FLOAT)   AS unit_price,
        TRY_CAST(DISCOUNT AS FLOAT)    AS discount,
        TRY_CAST(SHIPPINGCOST AS FLOAT) AS shipping_cost,
        TRY_CAST(CUSTOMERID AS FLOAT)  AS customer_id,

        -- Convert once
        TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS') AS invoice_date,

        TRIM(UPPER(COUNTRY))            AS country,
        TRIM(UPPER(PAYMENTMETHOD))      AS payment_method,
        TRIM(UPPER(CATEGORY))           AS category,
        TRIM(UPPER(SALESCHANNEL))       AS sales_channel,
        TRIM(UPPER(RETURNSTATUS))       AS return_status,
        TRIM(UPPER(SHIPMENTPROVIDER))   AS shipment_provider,
        TRIM(UPPER(WAREHOUSELOCATION))  AS warehouse_location,
        TRIM(UPPER(ORDERPRIORITY))      AS order_priority,

        CURRENT_TIMESTAMP()             AS stg_loaded_at

    FROM raw_data
),

validated AS (
    SELECT
        *,
        YEAR(invoice_date)  AS invoice_year,
        MONTH(invoice_date) AS invoice_month,

        CASE
            WHEN invoice_date IS NULL           THEN 'INVALID_DATE'
            WHEN quantity IS NULL               THEN 'INVALID_QUANTITY'
            WHEN unit_price IS NULL             THEN 'INVALID_PRICE'
            WHEN shipping_cost < 0              THEN 'NEGATIVE_SHIPPING'
            WHEN discount < 0 OR discount > 1   THEN 'INVALID_DISCOUNT'
            WHEN invoice_no IS NULL             THEN 'NULL_INVOICE'
            ELSE 'VALID'
        END AS data_quality_flag
    FROM staged
)

SELECT *
FROM validated


WHERE NOT EXISTS (
    SELECT 1
    FROM INCREMENTALETL._staging.stg_sales t
    WHERE t.invoice_year  = validated.invoice_year
      AND t.invoice_month = validated.invoice_month
)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:51:45.931423 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m10:51:46.862828 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.937 seconds
[0m10:51:46.894339 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:51:46.894339 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:51:47.069647 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.168 seconds
[0m10:51:47.069647 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:51:47.069647 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:51:47.294244 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.209 seconds
[0m10:51:47.343362 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:51:47.343362 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:51:47.475448 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.138 seconds
[0m10:51:47.584115 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:51:47.584115 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table "INCREMENTALETL"."_STAGING"."STG_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:51:47.744542 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.163 seconds
[0m10:51:47.792300 [debug] [Thread-2 (]: 
    In "INCREMENTALETL"."_STAGING"."STG_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m10:51:47.828463 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.stg_sales"
[0m10:51:47.828463 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:51:47.828463 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:51:47.999478 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.161 seconds
[0m10:51:48.001500 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:51:48.005294 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: merge into INCREMENTALETL._staging.stg_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._staging.stg_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","DATA_QUALITY_FLAG" = DBT_INTERNAL_SOURCE."DATA_QUALITY_FLAG"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "INVOICE_YEAR", "INVOICE_MONTH", "DATA_QUALITY_FLAG")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "INVOICE_YEAR", "INVOICE_MONTH", "DATA_QUALITY_FLAG")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:51:48.931911 [debug] [Thread-2 (]: SQL status: SUCCESS 744 in 0.923 seconds
[0m10:51:48.931911 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:51:48.931911 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:51:49.347772 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.420 seconds
[0m10:51:49.381479 [debug] [Thread-2 (]: Applying DROP to: INCREMENTALETL._staging.stg_sales__dbt_tmp
[0m10:51:49.395568 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:51:49.395568 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: drop view if exists INCREMENTALETL._staging.stg_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:51:49.572362 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.175 seconds
[0m10:51:49.604007 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '15290ead-3dfb-4908-8af3-adb11ac18979', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001580C258E20>]}
[0m10:51:49.619911 [info ] [Thread-2 (]: 1 of 2 OK created sql incremental model _staging.stg_sales ..................... [[32mSUCCESS 744[0m in 3.78s]
[0m10:51:49.619911 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.stg_sales
[0m10:51:49.619911 [debug] [Thread-4 (]: Began running node model.sales_pipelines.processed_sales
[0m10:51:49.619911 [info ] [Thread-4 (]: 2 of 2 START sql incremental model _processed.processed_sales .................. [RUN]
[0m10:51:49.619911 [debug] [Thread-4 (]: Acquiring new snowflake connection 'model.sales_pipelines.processed_sales'
[0m10:51:49.619911 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.processed_sales
[0m10:51:49.635973 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.processed_sales"
[0m10:51:49.635973 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.processed_sales
[0m10:51:49.651983 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:51:49.651983 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: create or replace  temporary view INCREMENTALETL._processed.processed_sales__dbt_tmp
  
  
  
  
  as (
    

WITH staging AS (
    SELECT * 
    FROM INCREMENTALETL._staging.stg_sales
),

deduped AS (
    SELECT *,
        ROW_NUMBER() OVER (
            PARTITION BY invoice_no, stock_code
            ORDER BY stg_loaded_at DESC
        ) AS row_num
    FROM staging
),

cleaned AS (
    SELECT
        invoice_no,
        stock_code,
        description,
        quantity,
        unit_price,
        discount,
        shipping_cost,
        customer_id,
        invoice_date,
        invoice_year,
        invoice_month,
        country,
        payment_method,
        category,
        sales_channel,
        return_status,
        shipment_provider,
        warehouse_location,
        order_priority,

        -- Derived metrics
        ROUND(quantity * unit_price, 2) AS gross_amount,
        ROUND(quantity * unit_price * (1 - discount), 2) AS net_amount,
        ROUND(
            quantity * unit_price * (1 - discount)
            + COALESCE(shipping_cost, 0),
        2) AS total_amount,

        CASE WHEN return_status = 'RETURNED' THEN TRUE ELSE FALSE END AS is_returned,
        CASE WHEN customer_id IS NULL THEN TRUE ELSE FALSE END AS is_guest_customer,

        stg_loaded_at,
        CURRENT_TIMESTAMP() AS processed_at

    FROM deduped
    WHERE
        row_num = 1
        AND data_quality_flag = 'VALID'
        AND invoice_date IS NOT NULL
)

SELECT *
FROM cleaned


WHERE NOT EXISTS (
    SELECT 1
    FROM INCREMENTALETL._processed.processed_sales t
    WHERE t.invoice_year  = cleaned.invoice_year
      AND t.invoice_month = cleaned.invoice_month
)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:51:49.651983 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m10:51:50.551955 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.892 seconds
[0m10:51:50.565111 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:51:50.569155 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:51:50.735083 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.167 seconds
[0m10:51:50.749066 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:51:50.749066 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:51:50.901820 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.138 seconds
[0m10:51:50.929016 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:51:50.929016 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:51:51.073027 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.140 seconds
[0m10:51:51.086135 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:51:51.102078 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:51:51.256684 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.153 seconds
[0m10:51:51.278340 [debug] [Thread-4 (]: 
    In "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m10:51:51.278340 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.processed_sales"
[0m10:51:51.294348 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:51:51.294348 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:51:51.508265 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.214 seconds
[0m10:51:51.508265 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:51:51.508265 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: merge into INCREMENTALETL._processed.processed_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._processed.processed_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","GROSS_AMOUNT" = DBT_INTERNAL_SOURCE."GROSS_AMOUNT","NET_AMOUNT" = DBT_INTERNAL_SOURCE."NET_AMOUNT","TOTAL_AMOUNT" = DBT_INTERNAL_SOURCE."TOTAL_AMOUNT","IS_RETURNED" = DBT_INTERNAL_SOURCE."IS_RETURNED","IS_GUEST_CUSTOMER" = DBT_INTERNAL_SOURCE."IS_GUEST_CUSTOMER","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","PROCESSED_AT" = DBT_INTERNAL_SOURCE."PROCESSED_AT"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:51:52.592842 [debug] [Thread-4 (]: SQL status: SUCCESS 732 in 1.072 seconds
[0m10:51:52.592842 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:51:52.592842 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:51:52.891923 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.300 seconds
[0m10:51:52.908092 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._processed.processed_sales__dbt_tmp
[0m10:51:52.924105 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:51:52.924105 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: drop view if exists INCREMENTALETL._processed.processed_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:51:53.103065 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.179 seconds
[0m10:51:53.103065 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '15290ead-3dfb-4908-8af3-adb11ac18979', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001581F087A30>]}
[0m10:51:53.103065 [info ] [Thread-4 (]: 2 of 2 OK created sql incremental model _processed.processed_sales ............. [[32mSUCCESS 732[0m in 3.48s]
[0m10:51:53.103065 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.processed_sales
[0m10:51:53.119202 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:51:53.119202 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m10:51:53.119202 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m10:51:53.454291 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m10:51:53.454291 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m10:51:53.760623 [debug] [MainThread]: Connection 'list_INCREMENTALETL__processed' was left open.
[0m10:51:53.760623 [debug] [MainThread]: On list_INCREMENTALETL__processed: Close
[0m10:51:54.195810 [debug] [MainThread]: Connection 'list_INCREMENTALETL__staging' was left open.
[0m10:51:54.195810 [debug] [MainThread]: On list_INCREMENTALETL__staging: Close
[0m10:51:54.584481 [debug] [MainThread]: Connection 'model.sales_pipelines.stg_sales' was left open.
[0m10:51:54.586003 [debug] [MainThread]: On model.sales_pipelines.stg_sales: Close
[0m10:51:54.876733 [debug] [MainThread]: Connection 'model.sales_pipelines.processed_sales' was left open.
[0m10:51:54.892676 [debug] [MainThread]: On model.sales_pipelines.processed_sales: Close
[0m10:51:55.207394 [info ] [MainThread]: 
[0m10:51:55.211443 [info ] [MainThread]: Finished running 2 incremental models in 0 hours 0 minutes and 12.29 seconds (12.29s).
[0m10:51:55.220540 [debug] [MainThread]: Command end result
[0m10:51:55.261435 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m10:51:55.261435 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m10:51:55.277217 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\snowflake-incremental-pipeline\dbt_project\target\run_results.json
[0m10:51:55.277217 [info ] [MainThread]: 
[0m10:51:55.277217 [info ] [MainThread]: [32mCompleted successfully[0m
[0m10:51:55.277217 [info ] [MainThread]: 
[0m10:51:55.277217 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=2
[0m10:51:55.277217 [debug] [MainThread]: Command `dbt run` succeeded at 10:51:55.277217 after 14.84 seconds
[0m10:51:55.289348 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001580AF17610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001580BA4A650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001581E90D510>]}
[0m10:51:55.289348 [debug] [MainThread]: Flushing usage events
[0m10:51:57.024921 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:52:22.432617 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000280193F7700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002801A6A2230>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002801A6A0AF0>]}


============================== 10:52:22.436137 | cd956227-bbf0-4854-b570-64eb315f217d ==============================
[0m10:52:22.436137 [info ] [MainThread]: Running with dbt=1.11.6
[0m10:52:22.436137 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'D:\\snowflake-incremental-pipeline\\dbt_project', 'invocation_command': 'dbt run', 'use_colors': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'D:\\snowflake-incremental-pipeline\\dbt_project\\logs'}
[0m10:52:23.355187 [debug] [MainThread]: Snowflake adapter: Setting snowflake.connector to ERROR (file logging only)
[0m10:52:23.355187 [debug] [MainThread]: Snowflake adapter: Setting botocore to ERROR (file logging only)
[0m10:52:23.355187 [debug] [MainThread]: Snowflake adapter: Setting boto3 to ERROR (file logging only)
[0m10:52:23.688355 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'cd956227-bbf0-4854-b570-64eb315f217d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002801A673370>]}
[0m10:52:23.767814 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'cd956227-bbf0-4854-b570-64eb315f217d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028019E679D0>]}
[0m10:52:23.767814 [info ] [MainThread]: Registered adapter: snowflake=1.11.2
[0m10:52:24.293636 [debug] [MainThread]: checksum: ec27062d18352e9cfca161c166fcc73d2201661a23a3cf3c89196a27f32cfcb8, vars: {}, profile: , target: , version: 1.11.6
[0m10:52:24.531080 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m10:52:24.532321 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m10:52:24.532321 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m10:52:24.625675 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'cd956227-bbf0-4854-b570-64eb315f217d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002802CD9C130>]}
[0m10:52:24.799786 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m10:52:24.799786 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m10:52:24.815452 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'cd956227-bbf0-4854-b570-64eb315f217d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002802CD3C880>]}
[0m10:52:24.815452 [info ] [MainThread]: Found 2 models, 6 data tests, 1 source, 522 macros
[0m10:52:24.831444 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'cd956227-bbf0-4854-b570-64eb315f217d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002802CD3C970>]}
[0m10:52:24.831444 [info ] [MainThread]: 
[0m10:52:24.835541 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m10:52:24.835541 [info ] [MainThread]: 
[0m10:52:24.835541 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m10:52:24.847087 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m10:52:24.862449 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL'
[0m10:52:24.957860 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m10:52:24.957860 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL"
[0m10:52:24.957860 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m10:52:24.957860 [debug] [ThreadPool]: On list_INCREMENTALETL: show terse schemas in database INCREMENTALETL
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL"} */
[0m10:52:24.973565 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:52:24.973565 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:52:26.214206 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 1.241 seconds
[0m10:52:26.258859 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 1.285 seconds
[0m10:52:26.265202 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__staging'
[0m10:52:26.265202 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_INCREMENTALETL__processed'
[0m10:52:26.286536 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__staging"
[0m10:52:26.292129 [debug] [ThreadPool]: Using snowflake connection "list_INCREMENTALETL__processed"
[0m10:52:26.292129 [debug] [ThreadPool]: On list_INCREMENTALETL__staging: show objects in INCREMENTALETL._staging
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__staging"} */;
[0m10:52:26.292129 [debug] [ThreadPool]: On list_INCREMENTALETL__processed: show objects in INCREMENTALETL._processed
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "connection_name": "list_INCREMENTALETL__processed"} */;
[0m10:52:26.294136 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:52:26.294136 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:52:27.369547 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 1.077 seconds
[0m10:52:27.376658 [debug] [ThreadPool]: SQL status: SUCCESS 1 in 1.082 seconds
[0m10:52:27.390498 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'cd956227-bbf0-4854-b570-64eb315f217d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002802B6AED10>]}
[0m10:52:27.406169 [debug] [Thread-2 (]: Began running node model.sales_pipelines.stg_sales
[0m10:52:27.406169 [info ] [Thread-2 (]: 1 of 2 START sql incremental model _staging.stg_sales .......................... [RUN]
[0m10:52:27.418078 [debug] [Thread-2 (]: Acquiring new snowflake connection 'model.sales_pipelines.stg_sales'
[0m10:52:27.418078 [debug] [Thread-2 (]: Began compiling node model.sales_pipelines.stg_sales
[0m10:52:27.439388 [debug] [Thread-2 (]: Writing injected SQL for node "model.sales_pipelines.stg_sales"
[0m10:52:27.439388 [debug] [Thread-2 (]: Began executing node model.sales_pipelines.stg_sales
[0m10:52:27.533282 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:52:27.533282 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: create or replace  temporary view INCREMENTALETL._staging.stg_sales__dbt_tmp
  
  
  
  
  as (
    

WITH raw_data AS (
    SELECT * 
    FROM INCREMENTALETL.LANDINGZONE.RAW_SALES
),

staged AS (
    SELECT
        -- IDs
        CAST(INVOICENO AS VARCHAR)    AS invoice_no,
        CAST(STOCKCODE AS VARCHAR)    AS stock_code,
        CAST(DESCRIPTION AS VARCHAR)  AS description,

        -- Quantities & Prices
        TRY_CAST(QUANTITY AS INT)      AS quantity,
        TRY_CAST(UNITPRICE AS FLOAT)   AS unit_price,
        TRY_CAST(DISCOUNT AS FLOAT)    AS discount,
        TRY_CAST(SHIPPINGCOST AS FLOAT) AS shipping_cost,
        TRY_CAST(CUSTOMERID AS FLOAT)  AS customer_id,

        -- Convert once
        TRY_TO_TIMESTAMP(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS') AS invoice_date,

        TRIM(UPPER(COUNTRY))            AS country,
        TRIM(UPPER(PAYMENTMETHOD))      AS payment_method,
        TRIM(UPPER(CATEGORY))           AS category,
        TRIM(UPPER(SALESCHANNEL))       AS sales_channel,
        TRIM(UPPER(RETURNSTATUS))       AS return_status,
        TRIM(UPPER(SHIPMENTPROVIDER))   AS shipment_provider,
        TRIM(UPPER(WAREHOUSELOCATION))  AS warehouse_location,
        TRIM(UPPER(ORDERPRIORITY))      AS order_priority,

        CURRENT_TIMESTAMP()             AS stg_loaded_at

    FROM raw_data
),

validated AS (
    SELECT
        *,
        YEAR(invoice_date)  AS invoice_year,
        MONTH(invoice_date) AS invoice_month,

        CASE
            WHEN invoice_date IS NULL           THEN 'INVALID_DATE'
            WHEN quantity IS NULL               THEN 'INVALID_QUANTITY'
            WHEN unit_price IS NULL             THEN 'INVALID_PRICE'
            WHEN shipping_cost < 0              THEN 'NEGATIVE_SHIPPING'
            WHEN discount < 0 OR discount > 1   THEN 'INVALID_DISCOUNT'
            WHEN invoice_no IS NULL             THEN 'NULL_INVOICE'
            ELSE 'VALID'
        END AS data_quality_flag
    FROM staged
)

SELECT *
FROM validated


WHERE NOT EXISTS (
    SELECT 1
    FROM INCREMENTALETL._staging.stg_sales t
    WHERE t.invoice_year  = validated.invoice_year
      AND t.invoice_month = validated.invoice_month
)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:52:27.533282 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m10:52:28.630675 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 1.097 seconds
[0m10:52:28.660607 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:52:28.660607 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:52:28.820748 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.153 seconds
[0m10:52:28.836803 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:52:28.836803 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:52:28.991073 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.142 seconds
[0m10:52:29.028633 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:52:29.028633 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table INCREMENTALETL._staging.stg_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:52:29.216600 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.176 seconds
[0m10:52:29.332690 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:52:29.332690 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: describe table "INCREMENTALETL"."_STAGING"."STG_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:52:29.468882 [debug] [Thread-2 (]: SQL status: SUCCESS 21 in 0.138 seconds
[0m10:52:29.508267 [debug] [Thread-2 (]: 
    In "INCREMENTALETL"."_STAGING"."STG_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m10:52:29.539812 [debug] [Thread-2 (]: Writing runtime sql for node "model.sales_pipelines.stg_sales"
[0m10:52:29.555518 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:52:29.555518 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:52:29.699586 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.155 seconds
[0m10:52:29.715754 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:52:29.715754 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: merge into INCREMENTALETL._staging.stg_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._staging.stg_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","DATA_QUALITY_FLAG" = DBT_INTERNAL_SOURCE."DATA_QUALITY_FLAG"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "INVOICE_YEAR", "INVOICE_MONTH", "DATA_QUALITY_FLAG")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "STG_LOADED_AT", "INVOICE_YEAR", "INVOICE_MONTH", "DATA_QUALITY_FLAG")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */;
[0m10:52:30.625154 [debug] [Thread-2 (]: SQL status: SUCCESS 0 in 0.903 seconds
[0m10:52:30.625154 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:52:30.625154 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:52:30.877007 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.250 seconds
[0m10:52:30.908736 [debug] [Thread-2 (]: Applying DROP to: INCREMENTALETL._staging.stg_sales__dbt_tmp
[0m10:52:30.929484 [debug] [Thread-2 (]: Using snowflake connection "model.sales_pipelines.stg_sales"
[0m10:52:30.929484 [debug] [Thread-2 (]: On model.sales_pipelines.stg_sales: drop view if exists INCREMENTALETL._staging.stg_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.stg_sales"} */
[0m10:52:31.134933 [debug] [Thread-2 (]: SQL status: SUCCESS 1 in 0.206 seconds
[0m10:52:31.181138 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cd956227-bbf0-4854-b570-64eb315f217d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002801A734F40>]}
[0m10:52:31.181138 [info ] [Thread-2 (]: 1 of 2 OK created sql incremental model _staging.stg_sales ..................... [[32mSUCCESS 0[0m in 3.77s]
[0m10:52:31.181138 [debug] [Thread-2 (]: Finished running node model.sales_pipelines.stg_sales
[0m10:52:31.196944 [debug] [Thread-4 (]: Began running node model.sales_pipelines.processed_sales
[0m10:52:31.198133 [info ] [Thread-4 (]: 2 of 2 START sql incremental model _processed.processed_sales .................. [RUN]
[0m10:52:31.198133 [debug] [Thread-4 (]: Acquiring new snowflake connection 'model.sales_pipelines.processed_sales'
[0m10:52:31.200254 [debug] [Thread-4 (]: Began compiling node model.sales_pipelines.processed_sales
[0m10:52:31.200254 [debug] [Thread-4 (]: Writing injected SQL for node "model.sales_pipelines.processed_sales"
[0m10:52:31.200254 [debug] [Thread-4 (]: Began executing node model.sales_pipelines.processed_sales
[0m10:52:31.215163 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:52:31.215163 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: create or replace  temporary view INCREMENTALETL._processed.processed_sales__dbt_tmp
  
  
  
  
  as (
    

WITH staging AS (
    SELECT * 
    FROM INCREMENTALETL._staging.stg_sales
),

deduped AS (
    SELECT *,
        ROW_NUMBER() OVER (
            PARTITION BY invoice_no, stock_code
            ORDER BY stg_loaded_at DESC
        ) AS row_num
    FROM staging
),

cleaned AS (
    SELECT
        invoice_no,
        stock_code,
        description,
        quantity,
        unit_price,
        discount,
        shipping_cost,
        customer_id,
        invoice_date,
        invoice_year,
        invoice_month,
        country,
        payment_method,
        category,
        sales_channel,
        return_status,
        shipment_provider,
        warehouse_location,
        order_priority,

        -- Derived metrics
        ROUND(quantity * unit_price, 2) AS gross_amount,
        ROUND(quantity * unit_price * (1 - discount), 2) AS net_amount,
        ROUND(
            quantity * unit_price * (1 - discount)
            + COALESCE(shipping_cost, 0),
        2) AS total_amount,

        CASE WHEN return_status = 'RETURNED' THEN TRUE ELSE FALSE END AS is_returned,
        CASE WHEN customer_id IS NULL THEN TRUE ELSE FALSE END AS is_guest_customer,

        stg_loaded_at,
        CURRENT_TIMESTAMP() AS processed_at

    FROM deduped
    WHERE
        row_num = 1
        AND data_quality_flag = 'VALID'
        AND invoice_date IS NOT NULL
)

SELECT *
FROM cleaned


WHERE NOT EXISTS (
    SELECT 1
    FROM INCREMENTALETL._processed.processed_sales t
    WHERE t.invoice_year  = cleaned.invoice_year
      AND t.invoice_month = cleaned.invoice_month
)

  )
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:52:31.215163 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m10:52:32.290121 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 1.070 seconds
[0m10:52:32.290121 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:52:32.290121 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:52:32.476958 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.184 seconds
[0m10:52:32.497271 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:52:32.497271 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:52:32.881578 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.373 seconds
[0m10:52:32.897634 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:52:32.897634 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table INCREMENTALETL._processed.processed_sales__dbt_tmp
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:52:33.107402 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.198 seconds
[0m10:52:33.121583 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:52:33.121583 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: describe table "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES"
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:52:33.309442 [debug] [Thread-4 (]: SQL status: SUCCESS 26 in 0.176 seconds
[0m10:52:33.330967 [debug] [Thread-4 (]: 
    In "INCREMENTALETL"."_PROCESSED"."PROCESSED_SALES":
        Schema changed: False
        Source columns not in target: []
        Target columns not in source: []
        New column types: []
  
[0m10:52:33.347094 [debug] [Thread-4 (]: Writing runtime sql for node "model.sales_pipelines.processed_sales"
[0m10:52:33.347094 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:52:33.347094 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: -- back compat for old kwarg name
  
  begin
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:52:33.559300 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.206 seconds
[0m10:52:33.559300 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:52:33.559300 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: merge into INCREMENTALETL._processed.processed_sales as DBT_INTERNAL_DEST
        using INCREMENTALETL._processed.processed_sales__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                    DBT_INTERNAL_SOURCE.invoice_no = DBT_INTERNAL_DEST.invoice_no
                ) and (
                    DBT_INTERNAL_SOURCE.stock_code = DBT_INTERNAL_DEST.stock_code
                )

    
    when matched then update set
        "INVOICE_NO" = DBT_INTERNAL_SOURCE."INVOICE_NO","STOCK_CODE" = DBT_INTERNAL_SOURCE."STOCK_CODE","DESCRIPTION" = DBT_INTERNAL_SOURCE."DESCRIPTION","QUANTITY" = DBT_INTERNAL_SOURCE."QUANTITY","UNIT_PRICE" = DBT_INTERNAL_SOURCE."UNIT_PRICE","DISCOUNT" = DBT_INTERNAL_SOURCE."DISCOUNT","SHIPPING_COST" = DBT_INTERNAL_SOURCE."SHIPPING_COST","CUSTOMER_ID" = DBT_INTERNAL_SOURCE."CUSTOMER_ID","INVOICE_DATE" = DBT_INTERNAL_SOURCE."INVOICE_DATE","INVOICE_YEAR" = DBT_INTERNAL_SOURCE."INVOICE_YEAR","INVOICE_MONTH" = DBT_INTERNAL_SOURCE."INVOICE_MONTH","COUNTRY" = DBT_INTERNAL_SOURCE."COUNTRY","PAYMENT_METHOD" = DBT_INTERNAL_SOURCE."PAYMENT_METHOD","CATEGORY" = DBT_INTERNAL_SOURCE."CATEGORY","SALES_CHANNEL" = DBT_INTERNAL_SOURCE."SALES_CHANNEL","RETURN_STATUS" = DBT_INTERNAL_SOURCE."RETURN_STATUS","SHIPMENT_PROVIDER" = DBT_INTERNAL_SOURCE."SHIPMENT_PROVIDER","WAREHOUSE_LOCATION" = DBT_INTERNAL_SOURCE."WAREHOUSE_LOCATION","ORDER_PRIORITY" = DBT_INTERNAL_SOURCE."ORDER_PRIORITY","GROSS_AMOUNT" = DBT_INTERNAL_SOURCE."GROSS_AMOUNT","NET_AMOUNT" = DBT_INTERNAL_SOURCE."NET_AMOUNT","TOTAL_AMOUNT" = DBT_INTERNAL_SOURCE."TOTAL_AMOUNT","IS_RETURNED" = DBT_INTERNAL_SOURCE."IS_RETURNED","IS_GUEST_CUSTOMER" = DBT_INTERNAL_SOURCE."IS_GUEST_CUSTOMER","STG_LOADED_AT" = DBT_INTERNAL_SOURCE."STG_LOADED_AT","PROCESSED_AT" = DBT_INTERNAL_SOURCE."PROCESSED_AT"
    

    when not matched then insert
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")
    values
        ("INVOICE_NO", "STOCK_CODE", "DESCRIPTION", "QUANTITY", "UNIT_PRICE", "DISCOUNT", "SHIPPING_COST", "CUSTOMER_ID", "INVOICE_DATE", "INVOICE_YEAR", "INVOICE_MONTH", "COUNTRY", "PAYMENT_METHOD", "CATEGORY", "SALES_CHANNEL", "RETURN_STATUS", "SHIPMENT_PROVIDER", "WAREHOUSE_LOCATION", "ORDER_PRIORITY", "GROSS_AMOUNT", "NET_AMOUNT", "TOTAL_AMOUNT", "IS_RETURNED", "IS_GUEST_CUSTOMER", "STG_LOADED_AT", "PROCESSED_AT")


/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */;
[0m10:52:34.543104 [debug] [Thread-4 (]: SQL status: SUCCESS 0 in 0.975 seconds
[0m10:52:34.543104 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:52:34.543104 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: COMMIT
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:52:35.433731 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.896 seconds
[0m10:52:35.449801 [debug] [Thread-4 (]: Applying DROP to: INCREMENTALETL._processed.processed_sales__dbt_tmp
[0m10:52:35.449801 [debug] [Thread-4 (]: Using snowflake connection "model.sales_pipelines.processed_sales"
[0m10:52:35.449801 [debug] [Thread-4 (]: On model.sales_pipelines.processed_sales: drop view if exists INCREMENTALETL._processed.processed_sales__dbt_tmp cascade
/* {"app": "dbt", "dbt_version": "1.11.6", "profile_name": "sales_pipelines", "target_name": "dev", "node_id": "model.sales_pipelines.processed_sales"} */
[0m10:52:35.840573 [debug] [Thread-4 (]: SQL status: SUCCESS 1 in 0.389 seconds
[0m10:52:35.856447 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cd956227-bbf0-4854-b570-64eb315f217d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002802D5CAC50>]}
[0m10:52:35.856447 [info ] [Thread-4 (]: 2 of 2 OK created sql incremental model _processed.processed_sales ............. [[32mSUCCESS 0[0m in 4.66s]
[0m10:52:35.856447 [debug] [Thread-4 (]: Finished running node model.sales_pipelines.processed_sales
[0m10:52:35.877006 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:52:35.877006 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m10:52:35.877006 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m10:52:36.243344 [debug] [MainThread]: Connection 'list_INCREMENTALETL' was left open.
[0m10:52:36.243344 [debug] [MainThread]: On list_INCREMENTALETL: Close
[0m10:52:36.566050 [debug] [MainThread]: Connection 'list_INCREMENTALETL__staging' was left open.
[0m10:52:36.566050 [debug] [MainThread]: On list_INCREMENTALETL__staging: Close
[0m10:52:36.855516 [debug] [MainThread]: Connection 'list_INCREMENTALETL__processed' was left open.
[0m10:52:36.855516 [debug] [MainThread]: On list_INCREMENTALETL__processed: Close
[0m10:52:37.271951 [debug] [MainThread]: Connection 'model.sales_pipelines.stg_sales' was left open.
[0m10:52:37.288118 [debug] [MainThread]: On model.sales_pipelines.stg_sales: Close
[0m10:52:37.543695 [debug] [MainThread]: Connection 'model.sales_pipelines.processed_sales' was left open.
[0m10:52:37.543695 [debug] [MainThread]: On model.sales_pipelines.processed_sales: Close
[0m10:52:38.103968 [info ] [MainThread]: 
[0m10:52:38.103968 [info ] [MainThread]: Finished running 2 incremental models in 0 hours 0 minutes and 13.27 seconds (13.27s).
[0m10:52:38.120088 [debug] [MainThread]: Command end result
[0m10:52:38.185429 [debug] [MainThread]: Wrote artifact WritableManifest to D:\snowflake-incremental-pipeline\dbt_project\target\manifest.json
[0m10:52:38.187438 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\snowflake-incremental-pipeline\dbt_project\target\semantic_manifest.json
[0m10:52:38.197473 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\snowflake-incremental-pipeline\dbt_project\target\run_results.json
[0m10:52:38.199223 [info ] [MainThread]: 
[0m10:52:38.199223 [info ] [MainThread]: [32mCompleted successfully[0m
[0m10:52:38.199223 [info ] [MainThread]: 
[0m10:52:38.202417 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=2
[0m10:52:38.202417 [debug] [MainThread]: Command `dbt run` succeeded at 10:52:38.202417 after 15.90 seconds
[0m10:52:38.202417 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000280193F7700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028017E13730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028019EB9780>]}
[0m10:52:38.202417 [debug] [MainThread]: Flushing usage events
[0m10:52:39.530533 [debug] [MainThread]: An error was encountered while trying to flush usage events
